[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Quantitative Methods for Linguistic Data",
    "section": "",
    "text": "Preface\nThis e-book is a study guide on quantitative methods for linguistic data, beyond the (2025) standard of frequentist linear and logistic (mixed-effects) regression modeling. Each chapter assumes that you have done readings indicated at the beginning of the chapter, each of which corresponds roughly to a week of a semester-long course (see “Context”).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "1  Bayesian basics 1",
    "section": "",
    "text": "1.1 Preliminaries\nLoad libraries we will need:\nlibrary(arm) # for the invlogit function\nlibrary(tidyverse)\nlibrary(brms)\nLoad the diatones dataset from Regression Modeling for Linguistic Data:\ndiatones &lt;- read.csv(\"https://osf.io/tqjm8/download\")\nThis dataset is described in Sec. 6.1.1 of Sonderegger (2023),1 where you can learn more as needed. For the moment, all we need to know is:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian basics 1</span>"
    ]
  },
  {
    "objectID": "week2.html#sec-bb1-prelim",
    "href": "week2.html#sec-bb1-prelim",
    "title": "1  Bayesian basics 1",
    "section": "",
    "text": "Practical note\n\n\n\nIf you have loaded rethinking, you need to detach it before using brms. See Sec 4.3.1 of Kurz.\n\n\n\n\n\n\nThere are 130 observations: English noun/verb pairs (or “words”) like “research”, “combat”.\nAll had noun and verb forms pronounced with final stress (“proTEST”) in 1700.\nstress_shifted indicates whether the noun form of the verb shifted stress between 1700 and 2005—e.g. “protest” and “debate” have stress_shifted=1, 0.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian basics 1</span>"
    ]
  },
  {
    "objectID": "week2.html#bayes-theorem",
    "href": "week2.html#bayes-theorem",
    "title": "1  Bayesian basics 1",
    "section": "1.2 Bayes’ theorem",
    "text": "1.2 Bayes’ theorem\nBayes’ theorem (or “rule”) says that for any events \\(A\\) and \\(B\\):\n\\[\nP(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n\\]\nHere, \\(P(A|B)\\) and \\(P(B|A)\\) are conditional probabilities, and Bayes’ theorem follows almost directly from the definition of conditional probabilities. We can think of \\(A\\) and \\(B\\) above as discrete random variables, and the same rule holds for continuous random variables (that is, their probability density functions).\n\nNote: a method which uses Bayes rule is not (automatically) “Bayesian” (McElreath pp. 38-39 box).\n\nIt is customary to introduce Bayes’ theorem with an example where \\(A\\) and \\(B\\) are discrete, like medical diagnostics, to develop intuition for how it works and can give unexpected results, even in a simple case. An exercise is below (to skip in class).\n\nExercise 1.1 Open Chapter 5 of Seeing Theory and go to “Bayes Theorem”. Consider a Covid-19 testing example, characteristic of the early days of the pandemic:\n\n\\(P(D)\\) = 0.05, \\(P(H)\\) = 0.95\n\\(P(+|D) = 0.99\\), \\(P(-|H)=0.95\\)\n\nThat is, the test detects Covid near-perfectly in people who have the disease (“very high sensitivity”), and gives a few false positives for people who don’t (“high specificity”). We assume that the base rate—the percentage of people who have Covid—is low (5%).\n\nWhat is the posterior probability \\(P(D|+)\\)—that is, the probability that someone has Covid if they test positive?\nWhy might your answer seem surprising?\nNow suppose that the base rate is 0.25 (25% of the population is infected). What is \\(P(D|+)\\) now?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian basics 1</span>"
    ]
  },
  {
    "objectID": "week2.html#prior-likelihood-posterior",
    "href": "week2.html#prior-likelihood-posterior",
    "title": "1  Bayesian basics 1",
    "section": "1.3 Prior, likelihood, posterior",
    "text": "1.3 Prior, likelihood, posterior\nMcElreath (Sec. 2.1-2.2) introduces key concepts of Bayesian inference using marble-counting and “globe tossing” examples. We will instead use a simple linguistic example, after first introducing notation:\n\nWe are interested in a parameter, \\(p\\).2\nWe observe data \\(D\\) to estimate the parameters.\nWe have some knowledge about how likely different parameter values are, the prior (probability): \\(P(p)\\)\nThe likelihood is the probability of the data for a particular parameter value: \\(P(D|p)\\).\nThe posterior (probability) is the plausibility of any parameter value (\\(p\\)) after seeing the data: \\(P(p|D)\\).\n\nBayes’ rule gives:\n\\[\nP(p | D ) = \\frac{P(D|p) P(p)}{P(D)}\n\\]\n\\(P(D)\\) is called the average probability or marginal likelihood, or other names.\nConsider the diatones data:\n\n\\(p_{\\text{shift}}\\) is the probability that a word of English shifts stress between 1700 and 2005.\nThe data is a sequence of 0 or 1 values.\n\nTechnically: each is a measurement of a random variable \\(y\\). The \\(i\\)th observation is \\(y_i\\). In this data \\(y_1 = 1\\), \\(y_2 = 0\\), and so on.\n\n\n\ndiatones %&gt;% select(word, stress_shifted) %&gt;% head()\n##      word stress_shifted\n## 1 proceed              1\n## 2 protest              1\n## 3  debate              0\n## 4 debauch              0\n## 5   decay              0\n## 6 decease              1\n\n\nxtabs(~stress_shifted, data=diatones)\n## stress_shifted\n##   0   1 \n## 109  21\n\nTo build a Bayesian model we must specify the exact data, then fit the model to the data. It is useful to always first have a data story (McElreath 2.2), describing the generative process behind the data. This helps to write down the model.\nIn our case, the story is just a description of how the process leading to this sample:\n\nWe have no prior knowledge about how likely English N/V pairs (“words”) are to shift stress.\nThe true proportion of words which shift stress is \\(p\\).\nEvery time we look up a new English word in the 1700 and 2005 dictionaries, we get a new 0 (stress didn’t shift) or 1 (stress shifted) observation, \\(y\\).\nEach word lookup is independent of the others.\n\n(I have added a Step 0 for the prior, which McElreath doesn’t include.)\nWe observe \\(N\\) words, of which \\(k\\) have shifted stress.\nThis lets us write down a probability model:\n\n\\(p \\sim \\text{Uniform}(0,1)\\)\n\\(k \\sim \\text{Binomial}(N, p)\\)\n\nSee here what the binomial distribution looks like for different \\(N\\) and \\(p\\) values, if you’re rusty.\nThis probability model means the pieces \\(P(p)\\), \\(P(D|p)\\), and \\(P(D)\\) of the equation above are all defined: once we know the observed data (\\(D\\)), the posterior distribution automatically results (left-hand side of Bayes’ rule above).\nThis is Bayesian update, which can be thought of either as:\n\nAn incremental process—update the posterior after \\(y_i\\); this is the prior when we see \\(y_{i+1}\\).\nSeeing all \\(N\\) observations at once—update the posterior once.\n\nWe can substitute the formula for binomial likelihood in to Bayes’ rule in this case to give the mathematical formula for the posterior:\n\\[\nP(p | k) = \\frac{{n\\choose{k}} p^{k}(1-p)^{n-k}}{P(k)}  \\quad \\text{for } 0 \\le p \\le 1\n\\]\n(The prior here, \\(P(p)\\), is flat, a.k.a. uniform: it is 1 when \\(0 \\le p \\le 1\\) and 0 otherwise.)\nIt is common to just write this kind of Bayesian update without the denominator, which is only there to make sure the posterior distribution is a probability distribution (it sums to 1 over all possible values of \\(p\\)). This is written with the “proportional to” symbol:\n\\[\nP(p | k) \\propto {n\\choose{k}} p^{k}(1-p)^{n-k}  \\quad \\text{for } 0 \\le p \\le 1\n\\]\nIf you find the analytic derivation or more details for this case helpful, see Sec Sec. 2.2 of Nicenboim, Schad, and Vaishth (2024). (If you find the math confusing, don’t worry.)\n\nExercise 1.2 We will instead get a feel for Bayesian update visually, by playing with the Seeing Theory app here.\nThis app shows a beta prior on \\(p\\), which has parameters \\(\\alpha\\) and \\(\\beta\\). When \\(\\alpha=\\beta=1\\), this is a uniform distribution (over \\(p \\in [0,1]\\)).\nLet \\(p=0.2\\) for this example—meaning that in reality, 1/5 of words shifted stress.\nWith default settings, change \\(p=0.2\\). Click “flip the coin” 5 times, and watch what happens. Do this again (resetting first) a few times.\n\nWhat is the same every time? (Does the posterior narrow? Shift right or left?)\nWhat is different every time?\n\n\n\nExercise 1.3 Now do the same exercise, with 50 flips (with \\(p=0.2\\))—use “Flip 10 times”x5.\n\n\nExercise 1.4 Play around with \\(\\alpha\\) and \\(\\beta\\) to get priors which captures the intuition that:\n\nValues of \\(p\\) further from 0 are less likely\nValues of \\(p\\) further from 1 are less likely\nValues of \\(p\\) which are below 0.05 or above 0.95 are very unlikely\n\n\n\n\n\n\n\n\nLinguistics note\n\n\n\nEach of these priors could be reasonable when studying different kinds of language variation and change.\nFor example, prior (b) could be applied for data from a completed sound change, to capture the intuition that most sound changes eventually apply to all possible words (the “Neogrammarian hypothesis”). Prior (c) could be applied to data from a sound change in progress.\n\n\nWe will discuss soon how to actually choose priors. The goal for now is just to get comfortable thinking about the concept, and the effect they have on the final model (the posterior).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian basics 1</span>"
    ]
  },
  {
    "objectID": "week2.html#prior-vs.-posterior-and-sample-size",
    "href": "week2.html#prior-vs.-posterior-and-sample-size",
    "title": "1  Bayesian basics 1",
    "section": "1.4 Prior vs. posterior and sample size",
    "text": "1.4 Prior vs. posterior and sample size\nNow let’s try manually computing the posterior distribution, starting from a uniform prior for \\(p\\), for our diatones example. (This is the case being simulated in the Seeing Theory app.)\nThe plot below shows the prior and the posterior, assuming we have just seen the first:\n\n5 observations\n10 observations\n65 observations (50% of the data)\n130 observations (all data)\n\nWe can do this without simulation, in this case, because the beta distribution is the conjugate prior for a binomial distribution—meaning that the posterior is also a beta distribution. You don’t need to understand this code.\n\n# Beta is conjugate prior for binomial means:\n# If you start with a Beta(\\alpha, \\beta) prior, \n# then observe $k$ $y=1$ observations of $N$ total, the posterior is:\n# Beta(k+\\alpha, N-k + \\beta)\n\n# For this case: \n\n#The numbers of observations with `stress_shifted`=1 in the first 5/10/65/130 observations are:\nk_5 &lt;- sum(diatones$stress_shifted[1:5])\nk_10 &lt;- sum(diatones$stress_shifted[1:10])\nk_65 &lt;- sum(diatones$stress_shifted[1:65])\nk_130 &lt;- sum(diatones$stress_shifted[1:130])\n\n#set up a vector of p=0...1\np_vec &lt;- seq(0,1, by=0.01)\n\n# beta dist parameters for the prior\na &lt;- 1\nb &lt;- 1\n\n## ugly generation of posterior for each case -- should use a map function!\nbind_rows(\n  data.frame(p=p_vec, post=dbeta(p_vec, a,b), n=0),\n  data.frame(p=p_vec, post=dbeta(p_vec, a+k_5,b+5-k_5), n=5),\n  data.frame(p=p_vec, post=dbeta(p_vec, a+k_10,b+10-k_10), n=10),\n  data.frame(p=p_vec, post=dbeta(p_vec, a+k_65,b+65-k_65), n=65),\n    data.frame(p=p_vec, post=dbeta(p_vec, a+k_130,b+130-k_130), n=130)\n\n) %&gt;% mutate(n=factor(n)) %&gt;% ## make the plot\n  ggplot(aes(x=p, y=post)) + geom_line(aes(color=n)) +\n  ylab(\"Posterior prob\") \n\n\n\n\n\n\n\n\nNote how the posterior shifts a lot more in the first 10 observations than in the last 50% of observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis example demonstrates how:\n\nThe posterior is a compromise between the prior and the likelihood\nFor smaller samples, the prior matters more\nFor large enough samples, the likelihood dominates\n\n\nAgain, Sec. 2.2 of BDA for CogSci shows a similar example in much more detail (and discusses what “conjugate prior” means).\nIn practice you rarely have to do Bayesian analyses “analytically”, by manually calculating the posterior distribution, and it is impossible for more realistic models. It is worth seeing this one example to emphasize that at a low level, every Bayesian model is just doing this process: given a prior and a probability model, calculate the likelihood of the observed data, use it to compute the posterior.\nThis is different from frequentist methods: even for a linear regression, much less a mixed-effects model, it’s often harder to conceptualize what fitting the model is “doing”. The black magic part of Bayesian modeling is the MCMC methods used to actually fit the models (compute the posterior)—but this is conceptually separate.\n\nExercise 1.5  \n\nRedo the plot above, now using a different prior: the one you chose for “Values of \\(p\\) which are below 0.05 or above 0.95 are very unlikely” above. This just requires changing a and b in the code.\nRe-generate the plot with these a and b. What do you notice, comparing the two plots for lower n (5, 10) and higher n (65, 130)?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian basics 1</span>"
    ]
  },
  {
    "objectID": "week2.html#computing-the-posterior",
    "href": "week2.html#computing-the-posterior",
    "title": "1  Bayesian basics 1",
    "section": "1.5 Computing the posterior",
    "text": "1.5 Computing the posterior\nFor most realistic models it is not possible to manually compute the posterior, so an approximation method is used. McElreath makes extensive use in the first chapters of two methods:\n\nGrid approximation\nQuadratic approximation, closely related to the MAP estimate (“maximum a -posteriori”)\n\nWe will not be using these, but they are worth knowing about—especially MAP, which often gives similar results to frequentist methods.\n\n\n\n\n\n\nPractical note\n\n\n\nAn important practical example is the blme package for fitting lme4-style mixed-effects models. This package:\n\nRedefines these models in a Bayesian framework\nUses weakly-informative priors (more on this later)\nReturns MAP estimates of model parameters (fixed, random-effect coefficients).\n\nThis alone is enough to eliminate many model convergence woes with lme4 models, and fitting a blme model is often faster than the equivalent brms model. It will be less accurate (as the MAP approximation is just that).\n\n\n\n1.5.1 MCMC\nInstead, we will follow Kurz by immediately turning to a third method—Markov Chain Monte Carlo (MCMC)—which is more generally applicable.\nFor now we will just introduce MCMC at a high level, and will return for more discussion in a future week. (hopefully) return in more detail in a later week. There are various introductions online (post one you like!) if you’d like to go further, at varying levels of technical detail. A relevant McElreath blog post is here.\nThe basic idea behind MCMC is that it is often much easier to simulate from a posterior distribution than to actually compute it. MCMC algorithms “walk around” the posterior distribution in a stochastic way, taking a sample of the parameters (here, \\(p\\)) at each step. “walk around” is defined such that the sample approximates the posterior distribution—once you wait long enough.\nThe simplest MCMC algorithm is called Metropolis-Hastings. MCMC is “easier to see than to understand” (McElreath). Open this demo, by Chi Feng, then:\n\nClick “Random Walk Metropolis Hastings”\nSet “target distribution” to “standard”\n\nThe underlying distribution here is a 2D Gaussian—a Gaussian hill. (If this is confusing, you can just focus on one dimension—its a Gaussian.) Note how the marginal distributions (on the \\(x\\) and \\(y\\) axes) get closer to normal distributions the longer you let the simulation run.\nWe will discuss some in class.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian basics 1</span>"
    ]
  },
  {
    "objectID": "week2.html#brms-a-first-look",
    "href": "week2.html#brms-a-first-look",
    "title": "1  Bayesian basics 1",
    "section": "1.6 brms: a first look",
    "text": "1.6 brms: a first look\nTo estimate \\(p\\) in a frequentist framework, we would fit a logistic regression with an intercept and no predictors:\n\nm1_glm &lt;- glm(stress_shifted ~ 1, data=diatones, family=\n                'binomial')\n\nThe estimated intercept, and its 95% confidence intervals are (in log-odds):\n\ncoefficients(m1_glm)\n## (Intercept) \n##   -1.646825\nconfint(m1_glm)\n## Waiting for profiling to be done...\n##     2.5 %    97.5 % \n## -2.141121 -1.202033\n\nThese correspond in probability to:\n\ninvlogit(coefficients(m1_glm))\n## (Intercept) \n##   0.1615385\ninvlogit(confint(m1_glm))\n## Waiting for profiling to be done...\n##     2.5 %    97.5 % \n## 0.1051638 0.2311138\n\nSo the estimate is \\(\\hat{p} = 0.12\\) (95% CI: 0.11-0.23).\nThe equivalent model is fitted in brms, with a uniform prior on \\(p\\), as follows:\n\ndiatones_m1 &lt;-\n  brm(data = diatones,\n      family = binomial(link = \"identity\"),\n      stress_shifted | trials(1) ~ 0 + Intercept,\n      prior(beta(1, 1), class = b, lb = 0, ub = 1)\n  )\n## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n## Compiling Stan program...\n## Trying to compile a simple C file\n## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\n## using C compiler: ‘Apple clang version 15.0.0 (clang-1500.1.0.2.5)’\n## using SDK: ‘MacOSX14.2.sdk’\n## clang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\n## In file included from &lt;built-in&gt;:1:\n## In file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\n## In file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\n## In file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n## /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n## #include &lt;cmath&gt;\n##          ^~~~~~~\n## 1 error generated.\n## make: *** [foo.o] Error 1\n## Start sampling\n## \n## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 2.8e-05 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.013 seconds (Warm-up)\n## Chain 1:                0.012 seconds (Sampling)\n## Chain 1:                0.025 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 6e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.013 seconds (Warm-up)\n## Chain 2:                0.012 seconds (Sampling)\n## Chain 2:                0.025 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 4e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.013 seconds (Warm-up)\n## Chain 3:                0.012 seconds (Sampling)\n## Chain 3:                0.025 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 4e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.013 seconds (Warm-up)\n## Chain 4:                0.015 seconds (Sampling)\n## Chain 4:                0.028 seconds (Total)\n## Chain 4:\n\nbrm() is the workhorse function for fitting bayesian regression models. The family argument and the model formula together express the likelihood for \\(p\\): this is a logistic regression on data where each \\(y\\) is 0 or 1. The prior argument captures the uniform (\\(\\text{Beta}(1,1)\\)) prior. (This particular model needs an unusual structure to make the prior work—when we do logistic regression we’ll proceed differently.)\n\nsummary(diatones_m1)\n##  Family: binomial \n##   Links: mu = identity \n## Formula: stress_shifted | trials(1) ~ 0 + Intercept \n##    Data: diatones (Number of observations: 130) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.17      0.03     0.11     0.24 1.01     1251     1540\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nWe will discuss the arguments, the messages during fitting, and the model output more soon. For now we focus on the posterior for \\(p\\). A numeric summary is:\n\nposterior_summary(diatones_m1)\n##                Estimate  Est.Error        Q2.5       Q97.5\n## b_Intercept   0.1664336 0.03187322   0.1080541   0.2352411\n## lprior        0.0000000 0.00000000   0.0000000   0.0000000\n## lp__        -59.9652369 0.70648371 -62.0371273 -59.4744656\n\n(Ignore the second row for now.) This shows the mean, SD, and 95% quantiles of the posterior distribution for \\(p\\). Note how similar these are to the glm-derived values above.\nThe actual posterior can be visualized by extracting and plotting the model’s sample from the posterior, using the as_draws_df() function from the posterior package.3\n\nas_draws_df(diatones_m1) %&gt;%\n  ggplot(aes(x = b_Intercept)) +\n  geom_density(fill = \"black\") +\n  scale_x_continuous(\"Prob. of stress shifting\", limits = c(0, 1)) + \n  ylab(\"Posterior probability\")\n\n\n\n\n\n\n\n\n(Usually we will just call this “plotting the results”, or similar.) This looks very similar to the analytically-computed posterior above (for all \\(n=130\\) observations).\n\nExercise 1.6  \n\nRefit the brms model, now with your prior that captures that values of \\(p\\) very near 0 or 1 are unlikely. How does the posterior look different, if at all?\n\n\n\nNow refit both of the last two models, but just using the first 20 observations from diatones. How do the posteriors of these models differ? Why? \n\n\n\n\n\n\nKurz, S. 2023. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nNicenboim, B., D. Schad, and S. Vaishth. 2024. An Introduction to Bayesian Data Analysis for Cognitive Science. https://vasishth.github.io/bayescogsci/book/. 5/2024 version.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian basics 1</span>"
    ]
  },
  {
    "objectID": "week2.html#footnotes",
    "href": "week2.html#footnotes",
    "title": "1  Bayesian basics 1",
    "section": "",
    "text": "PDF available here.↩︎\nIn more complex models, there will be many parameters.↩︎\nas_draws_* functions from this package extract posterior draws in various formats, like as_draws_matrix(), as_draws_list(). Here we need draws in dataframe format.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian basics 1</span>"
    ]
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "2  Bayesian basics 2",
    "section": "",
    "text": "2.1 Preliminaries\nLoad libraries we will need:\nlibrary(arm) # for the invlogit function\nlibrary(tidyverse)\nlibrary(brms)\n\nlibrary(broom) # for tidy model summaries\nlibrary(tidybayes)\nlibrary(languageR) # for the english dataset\nlibrary(patchwork) # for plotting\nLoad the diatones dataset from Regression Modeling for Linguistic Data, like last week:\ndiatones &lt;- read.csv(\"https://osf.io/tqjm8/download\")\nAlso load the dyads dataset discussed in Winter and Bürkner (2021):\ndyads &lt;- read.csv(\"https://osf.io/6j8kc/download\")\nTheir description:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayesian basics 2</span>"
    ]
  },
  {
    "objectID": "week3.html#sec-bb2-prelim",
    "href": "week3.html#sec-bb2-prelim",
    "title": "2  Bayesian basics 2",
    "section": "",
    "text": "Practical note\n\n\n\nIf you have loaded rethinking, you need to detach it before using brms. See Kurz (2023) Sec. 4.3.1.\n\n\n\n\n\n\n\n\n“…[dataset of] co-speech gestures by Catalan and Korean speakers. The data comes from a dyadic task performed by Brown et al. (under review) in which participants first watched a cartoon and subsequently told a partner about what they had seen. The research question was whether the social context modulates people’s politeness strategies, including nonverbal politeness strategies, such as changing the frequency of one’s co-speech gestures. The key experimental manipulation was whether the partner was a friend or a confederate, who was an elderly professor.”\n\n\n“There are two data points per participant, one from the friend condition, and one from the professor condition. The ID column lists participant identifiers for all 27 participants (14 Catalan speakers and 13 Korean speakers). The gestures column contains the primary response variable that we are trying to model, the number of gestures observed on each trial. The context predictor specifies the social context that was experimentally manipulated.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayesian basics 2</span>"
    ]
  },
  {
    "objectID": "week3.html#tidybayes-and-bayesplot",
    "href": "week3.html#tidybayes-and-bayesplot",
    "title": "2  Bayesian basics 2",
    "section": "2.2 tidybayes and bayesplot",
    "text": "2.2 tidybayes and bayesplot\nAbove we loaded tidybayes (Kay 2023), a package for integrating Bayesian modeling methods into a tidyverse + ggplot workflow, which works well with brms. We will often use this package for working with the posterior distributions of fitted brms models (meaning, samples from the posterior), in addition to the posterior package (Bürkner et al. 2024), which is “intended to provide useful tools for both users and developers of packages for fitting Bayesian models or working with output from Bayesian models” (?posterior).1\nWe’ll also start using the useful bayesplot package (Gabry and Mahr 2024; Gabry et al. 2019), an “extensive library of plotting functions for use after fitting Bayesian models (typically with MCMC)”, such as posterior predictive checks, introduced below.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayesian basics 2</span>"
    ]
  },
  {
    "objectID": "week3.html#summarizing-posterior-distributions",
    "href": "week3.html#summarizing-posterior-distributions",
    "title": "2  Bayesian basics 2",
    "section": "2.3 Summarizing posterior distributions",
    "text": "2.3 Summarizing posterior distributions\nOnce you’ve fitted a Bayesian model, you’re able to sample from the posterior over model parameter(s). Now we want to summarize and interpret the posterior using these samples:\n\nInterval estimates: “most likely” values of \\(p_{\\text{shift}}\\)\nPoint estimates: central tendency (mean/median/mode)\n\nWe will use a couple example posteriors, for our example from last week:\n\\[\\begin{align}\np_{\\text{shift}} & \\sim \\text{Uniform}(0,1) \\\\\nk & \\sim \\text{Binomial}(n, p_{\\text{shift}})\n\\end{align}\\]\n(I have written the probability of stress shift as \\(p_{\\text{shift}}\\) to avoid confusion with “probability” in general.)\n\n2.3.1 Example posterior 1\nTo see why different interval/point methods matter, it is useful to use a highly skewed posterior. McElreath’s example in 3.2 is equivalent to taking \\(n=3\\) and observing \\(k=3\\), e.g. 3 words with stress_shifted=1 for a binomial distribution. The posterior over \\(p_{\\text{shift}}\\) is then a \\(\\text{Beta}(1,4)\\) distribution, which looks like this:\n\n\nCode\n#set up a vector of p=0...1\np_vec &lt;- seq(0.0001,0.9999, by=0.01)\n\ndata.frame(p=p_vec, post=dbeta(p_vec, 4, 1)) %&gt;% \n  ggplot(aes(x=p_vec, y=post)) + \n  geom_line() + \n  xlab(\"p_shift\") +\n  ylab(\"Posterior probability (PDF)\")\n\n\n\n\n\n\n\n\n\nIt would be possible to just calculate a median, mode, etc. for this curve, but in practice we almost always work with samples from the posterior (usually from a fitted model):\n\n## take 1000 samples\nsamples_1 &lt;- rbeta(n=10000, 4, 1)\n\nIt is useful to start doing so now.\n\n\n2.3.2 Example posterior 2\nThe posterior for a model fitted as Exercise 10 last week, after seeing the first 20 observations from diatones (i.e., \\(k=20\\)):\n\n## week 3, model 1\ndiatones_m31&lt;-\n  brm(data = diatones[1:20,],\n      family = binomial(link = \"identity\"),\n      stress_shifted | trials(1) ~ 0 + Intercept,\n      prior(beta(1, 1), class = b, lb = 0, ub = 1),\n      file='models/diatones_m31.brm'\n  )\ndiatones_m31\n\n(I will suppress output/messages/warnings from brms fits from now on, unless they’re relevant for us.)\n\n\n\n\n\n\nPractical note\n\n\n\nThe file argument here saves the fitted model to a file. If the file exists already, brm() just loads the model instead of refitting. This makes it easier for me to write/compile this document, and it’s good practice to use the file argument once you are fitting “real” models (to make sure you don’t lose them once fitted). If you want to refit the model, you then need to delete the corresponding .brm file first. You (reader) can either remove this argument or create a models/ directory.\n\n\nExtract posterior samples using spread_draws() from tidybayes:\n\ndiatones_m31_draws &lt;- diatones_m31 %&gt;% spread_draws(b_Intercept)\n\nspread_draws(a) puts the samples (“draws” from the posterior) for parameter(s) a in a tibble in tidy format for subsequent processing or plotting, with many pre-coded routines available (see vignette). diatones_m31_draws has one row for each post-warmup draw (= 4000 rows):\n\ndiatones_m31_draws\n## # A tibble: 4,000 × 4\n##    .chain .iteration .draw b_Intercept\n##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;\n##  1      1          1     1       0.225\n##  2      1          2     2       0.348\n##  3      1          3     3       0.366\n##  4      1          4     4       0.154\n##  5      1          5     5       0.426\n##  6      1          6     6       0.248\n##  7      1          7     7       0.349\n##  8      1          8     8       0.298\n##  9      1          9     9       0.373\n## 10      1         10    10       0.373\n## # ℹ 3,990 more rows\n\nPlot the posterior from these samples, using the stat_slab() function from tidybayes:\n\n\nCode\ndiatones_m31_draws %&gt;%\n  ggplot(aes(x = b_Intercept)) + \n  stat_slab()+ ## one way to visualize posterior; could also e.g. use a histogram\n  xlab(\"p_shift\")+\n  ylab(\"Posterior probability\") + \n  xlim(0,1)\n\n\n\n\n\n\n\n\n\nSave the samples of \\(p_{\\text{shift}}\\) as a separate vector for convenience:\n\nsamples_2 &lt;- diatones_m31_draws$b_Intercept\n\n\n\n2.3.3 Intervals\nPosterior distributions are often summarized with a credible interval describing where X% of probability lies. Typically X = 95%, by convention. McElreath uses 89% to remind us that there is nothing sacred about 95%, and to avoid triggering NHST thinking using \\(p&lt;0.05\\).2 Credible intervals are analogous to frequentist “confidence intervals”, but with a more straightforward interpretation.\nTwo common options for CredI’s (as they’re often abbreviated) are:\n\nquantile interval (a.k.a percentile): values of \\(p\\) between 2.5% and 97.5% quantile\nhighest posterior density interval (a.k.a highest-density interval): narrowest range of \\(p\\) containing 95% posterior probability.\n\n\n\n\n\n\n\n\nLet’s calculate 89% intervals for our two example posteriors:\n\nqi(samples_1, .width = 0.89)\n##           [,1]      [,2]\n## [1,] 0.4844551 0.9862567\nhdi(samples_1, .width=0.89)\n##           [,1]      [,2]\n## [1,] 0.5765895 0.9999929\nqi(samples_2, .width=0.89)\n##           [,1]      [,2]\n## [1,] 0.1719941 0.4842852\nhdi(samples_2, .width=0.89)\n##           [,1]     [,2]\n## [1,] 0.1602519 0.468913\n\nNote how the QI and HDI for Example 1 are different, while those for a more symmetric distribution (Example 2) are practically the same.\nIn general, HPDs are better at conveying the shape of the distribution, including where the mode is, and are conceptually preferable to PIs. But HPDs have several drawbacks relative to PIs:\n\nMore computationally intensive to compute.\nMore sensitive to how many samples are drawn from the posterior.\nFor multimodal posteriors, HPD-based CredI’s can be discontinuous.\nHarder to convey to a reader.\n\nNone of this matters for our simple examples, but going forward we will usually use PI-based intervals, as brms (and Stan) do by default. “CI” or “CredI” will mean “quantile-based credible interval”, unless otherwise noted.\nIn cases where the choice between an HPD and PI-based CredI matters, you arguably should not be describing the posterior just using a point+interval estimate anyway.\n\n\n2.3.4 Point estimates\nBayesians don’t like reporting a single number to summarize the whole posterior distribution, but humans often find such a number useful. Choices:\n\nPosterior mode: MAP estimate (maximum a-posteriori)\nPosterior median\nPosterior mean\n\nThe mode makes intuitive sense. The median or mean can be justified as minimizing different kinds of loss function (McElreath 2020, 3.2) when trying to guess \\(p\\).\nFor our examples:\n\ndata.frame(\n  example=c(1,2),\n  MAP = c(Mode(samples_1), Mode(samples_2)),\n  median = c(median(samples_1), median(samples_2)),\n  mean = c(mean(samples_1), mean(samples_2)\n  )\n) %&gt;% round(3)\n##   example  MAP median  mean\n## 1       1 1.00  0.842 0.800\n## 2       2 0.31  0.312 0.318\n\nAgain, these different methods differ a lot for a non-symmetric distribution (Example 1) and less for a symmetric distribution (Example 2). Make sure you understand why mean \\(&lt;\\) median \\(&lt;\\) MAP for Example 1, given the shape of the distribution (plotted above).\n\n\n\n\n\nAll three point estimates are reported in practice. MAP is common, but  has the same disadvantages as HPD-based CredI’s. brms uses the posterior mean by default.\nThe tidybayes package will calculate these and many other quantities for you via helper functions. For example, to get the median and 95% quantile-based CIs for Example 1:\n\nmedian_qi(samples_1)\n##           y      ymin      ymax .width .point .interval\n## 1 0.8416485 0.3995363 0.9936803   0.95 median        qi\n\nHere, y, ymin, ymax are median and 95% CI boundaries.\n\n\n2.3.5 Visualizing distributions\nIn practice it is best to visualize posteriors together with point/interval summaries showing multiple probability values (not just 0.025 and 0.975). tidybayes provides many options,3 such as a “half-eye” plot, which shows the 66% and 95% (quantile-based) CIs by default:\n\n\nCode\ndiatones_m31_draws %&gt;%\n  ggplot(aes(x = b_Intercept)) +\n  stat_halfeye()+\n  xlab(\"p_shift\")+\n  ylab(\"Posterior probability\") + \n  xlim(0,1)\n\n\n\n\n\n\n\n\n\n\nExercise 2.1 Examine the output of model diatones_m31:\n\ndiatones_m31\n##  Family: binomial \n##   Links: mu = identity \n## Formula: stress_shifted | trials(1) ~ 0 + Intercept \n##    Data: diatones[1:20, ] (Number of observations: 20) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.32      0.10     0.15     0.52 1.00     1323     1898\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWhat do Estimate, l-95% CI, and u-95% CI mean? (Hint: look at calculations above.)\nWhat do you think Est.Error means?\nExtra: Verify your guess by computing this number using our sample from the posterior distribution of the model’s intercept (samples_2).\n\n\n\nExercise 2.2 Make a visualization of the posterior for Exercise 2.1 using samples_1 (the samples from the posterior). which shows both the density and point/interval summaries.\nNote that to use a tidybayes function for this, you’ll first need to make a dataframe with samples_1 as one column.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayesian basics 2</span>"
    ]
  },
  {
    "objectID": "week3.html#posterior-predictive-distribution",
    "href": "week3.html#posterior-predictive-distribution",
    "title": "2  Bayesian basics 2",
    "section": "2.4 Posterior predictive distribution",
    "text": "2.4 Posterior predictive distribution\nAbove we considered sampling from the posterior to summarize the posterior: what the model says about likely parameter values. This is analogous to frequentist models, where we get parameter estimates, confidence intervals, etc. Another use for the posterior is to understand what the model says about likely data: what McElreath calls its “implied predictions”.\nInformally: because Bayesian models are generative, once we know how likely different parameter values are (the posterior), we can simulate new data (observations), by just running the model forward. This is called dummy or fake data.\nLet \\(\\vec{y}\\) be the observed data, and \\(\\vec{\\theta}\\) be the parameter values. Fitting a model gives the posterior distribution:\n\\[\n\\underbrace{P(\\vec{\\theta} | \\vec{y})}_{\\text{posterior}} \\propto \\underbrace{\\vec{P(\\vec{y} | \\vec{\\theta})}}_{\\text{likelihood}} \\underbrace{\\vec{P(\\vec{\\theta})}}_{\\text{prior}}\n\\]\nThe probability distribution for new data, \\(\\vec{y}_{new}\\), is the posterior predictive distribution, \\(P(\\vec{y}_{new} | \\vec{y})\\). How likely different values of new data are (\\(\\vec{y}_{new}\\)) is obtained by:4\n\n\n\nFor every possible value of the parameters \\(\\vec{\\theta}\\):\nCompute how likely \\(\\vec{y}_{new}\\) is under \\(\\vec{\\theta}\\) (the likelihood).\nWeight by how likely these parameter value are (the posterior).\nSum over all parameter values.\n\nWe can think of the posterior distribution as a machine for calculating new (fake) datasets. Each simulated observation captures two sources of uncertainty:\n\nIn parameters (the posterior)\nIn observations, given the parameter values\n\nBoth sources of uncertainty are propagated by the posterior predictive distribution. McElreath (2020), Sec. 3.3.2.2, gives a very nice visualization (Fig. 3.6).\n\n2.4.1 Example\nFor the diatones_m31 model, where \\(\\vec{y}\\) is a sequence of 20 zeros and 1s, we can use posterior_predict() to simulate 4000 new draws of the data:\n\npp_ex &lt;- posterior_predict(diatones_m31) \nhead(pp_ex)\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n## [1,]    1    0    0    1    1    0    0    0    0     0     0     0     1     0\n## [2,]    0    0    0    1    0    1    0    0    0     1     1     0     0     1\n## [3,]    1    0    0    0    1    0    0    0    0     0     0     0     0     1\n## [4,]    0    0    0    0    1    0    1    0    0     0     0     0     0     0\n## [5,]    1    0    0    0    0    0    0    1    0     0     0     0     1     0\n## [6,]    0    0    0    0    1    1    0    0    0     0     0     0     0     1\n##      [,15] [,16] [,17] [,18] [,19] [,20]\n## [1,]     0     0     1     0     0     0\n## [2,]     1     1     1     1     0     0\n## [3,]     0     1     0     1     1     1\n## [4,]     1     0     1     0     0     0\n## [5,]     1     0     1     0     0     0\n## [6,]     1     1     1     0     0     0\n\nEach row is one draw, each column is one of the 20 data points. We can plot these simulated datasets by plotting the distribution of \\(k\\): the number of shifted stress words (out of 20). This distribution is:\n\n\nCode\ndata.frame(num_shifted=apply(posterior_predict(diatones_m31), 1, sum)) %&gt;% \n  ggplot(aes(x=num_shifted)) + \n  geom_histogram(bins = 20) + \n  geom_vline(aes(xintercept=6), lty=2) + \n  ## dotted line: the *observed* number of shifted words.\n  xlab(\"k: number of shifted words out of 20\")\n\n\n\n\n\n\n\n\n\nThe distribution of predicted \\(k\\) is quite wide, because it incorporates both sources of variance:\n\nThe 95% CI for \\(p_{shift}\\) is [0.14, 0.52], corresponding to \\(k \\in [2.8, 10.4]\\) (multiply \\(p_{shift}\\) by 20)\nThe variability in \\(k\\) expected from observation-level variance, for the MAP estimate of \\(p_{shift}\\) (0.32), is:\n\n\n\nCode\nk_vec &lt;- 0:20\ndata.frame(k = k_vec) %&gt;%\nggplot(aes(x=k, y=dbinom(k, 20, p=0.32))) + \n  geom_line() + \n  ylab(\"Probability (PDF)\")\n\n\n\n\n\n\n\n\n\n\nMore generally, the posterior predictive distribution allows us to examine any quantity that can be computed from the model. This strength is not apparent for our simple first model; we will return to it below.   \nWorking with (fake data from the) posterior predictive distribution is a powerful method for (McElreath 2020, 3.3):\n\nmodel checking (do predictions roughly match observations?)\nmodel design (will this model do what we expect, given all its moving parts?)\nsoftware validation (if we simulate from a known distribution, then fit a model, do we get back the right parameters?)\nforecasting / model understanding (what does this model predict for cases relevant for our RQs? For new cases?)\n\nWe’ll see these uses as we go along.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayesian basics 2</span>"
    ]
  },
  {
    "objectID": "week3.html#more-model-types",
    "href": "week3.html#more-model-types",
    "title": "2  Bayesian basics 2",
    "section": "2.5 More model types",
    "text": "2.5 More model types\nSo far we have fit one simple model: an intercept-only binomial model, with an identity link (the single parameter \\(p_{\\text{shift}}\\) was modeled directly).\nLet’s learn how to fit more types of models in brms. For now we will stick to models with one fitted parameter, with “uninformative” (flat or very shallow) priors for model parameters. We’ll then move to models with two parameters (below), models with actual predictors (next week), and better priors.\n\n\n\n\n\n2.5.1 Binomial with logit link\nTypically we fit logistic regression to data where \\(y=0\\) or 1: a binomial model with a logit link. Consider the (frequentist) model fit using glm() to the diatones data, first 20 observations. For observation \\(i\\):\n\\[\n\\text{logit}(P(y_i = 1)) = \\alpha , \\quad i=1, \\ldots n\n\\]\n\\(\\alpha\\) is the the probability that stress_shift = 1 in log-odds—the model’s intercept. (In terms of notation we’ve used for our first model, \\(\\alpha = \\text{logit}(p_{shift})\\).) To write this as a Bayesian model, we need a prior on \\(\\alpha\\).\n\\(\\alpha\\) could be any number in \\([-\\infty, \\infty]\\), so it cannot have a uniform prior. Instead we can give it an “uninformative” prior centered around 0:\n\\[\n\\alpha \\sim N(0, 25)\n\\] (read: normal distribution with mean 0, SD 25.) This is effectively the same as \\(p \\sim \\text{Beta}(1,1)\\) (uniform prior), in the sense that both of them are near-flat over values of \\(\\alpha\\) (i.e., log-odds of \\(p_{\\text{shift}}\\)) which are at all reasonable. Here’s what this prior looks like in terms of \\(\\alpha\\):\n\n\nCode\ndf &lt;- data.frame(alpha = seq(-10, 10, by=0.01) , p=dnorm(x=seq(-10, 10, by=0.01) , sd = 25))\n\n## log-odds of -5 to 5\ndf %&gt;%  ggplot(aes(x=alpha, y=p)) + \n  geom_line() + \n  xlab(expression(alpha)) + \n  ylab(\"Probability (density)\") + \n  xlim(-5,5) + \n  ylim(0, max(df$p)*1.05) + \n  theme(axis.title.x = element_text(size = 15))  \n## Warning: Removed 1000 rows containing missing values or values outside the scale range\n## (`geom_line()`).\n\n\n\n\n\n\n\n\n\nAnd in terms of \\(p_{\\text{shift}}\\):\n\n\nCode\n## corresponds to p_shift = 0.007 to 0.9933 after inverse-logit transform\n## plogis(5)\n## plogis(-5)\ndf %&gt;%  ggplot(aes(x=invlogit(alpha), y=p/sum(p))) +\n  geom_line() +\n  labs(x = expression(p[shift]~\"(=inverse logit(\"*alpha*\"))\"), y = \"Probability (density)\") +\n  xlim(0.007, 0.9933) + \n  ylim(0, max(df$p/sum(df$p))*1.05)\n## Warning: Removed 1006 rows containing missing values or values outside the scale range\n## (`geom_line()`).\n\n\n\n\n\n\n\n\n\nThe probability model, written in terms of single observations of \\(y\\) (stress_shifted):\n\\[\\begin{align}\n\\text{logit}(p_{\\text{shift}}) & \\sim N(0, 25) \\\\\ny & \\sim \\text{Binomial}(p_{\\text{shift}}, 1)\n\\end{align}\\]\nThe second row is also called a Bernoulli distribution: a binomial with \\(n=1\\).\nTo fit this model in brms using default settings:5\n\ndiatones_m32 &lt;-\n  brm(data = diatones[1:20,],\n      family = binomial,\n      stress_shifted | trials(1) ~ 1,\n      prior(normal(0, 10), class = Intercept),\n      file='models/diatones_m32.brm'\n  )\n\nModel results:\n\ndiatones_m32\n##  Family: binomial \n##   Links: mu = logit \n## Formula: stress_shifted | trials(1) ~ 1 \n##    Data: diatones[1:20, ] (Number of observations: 20) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -0.87      0.48    -1.83     0.01 1.00     1441     1927\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe estimate for \\(\\text{logit}(p_{\\text{shift}})\\) and its Est. Error are similar to a glm() (frequentist) model, shown in Section 2.6.\nWe will typically fit logistic regression models using the bernoulli family. This simplifies the model formula slightly, and the model fits slightly faster. The code for this would be (not run):\n\ndiatones_m32 &lt;-\n  brm(data = diatones[1:20,],\n      family = bernoulli,\n      stress_shifted ~ 1,\n      prior(normal(0, 10), class = Intercept),\n      file='models/diatones_m32.brm'\n  )\n\n\nExercise 2.3  \n\nVisualize the posterior of model diatones_m32. This is the posterior of the single model parameter \\(\\alpha\\), the log-odds of `stress_shifted = 1.\nExtra: visualize the posterior of \\(\\text{invlogit}(\\alpha)\\), i.e. \\(p_{\\text{shift}}\\). Does it look any different from the posterior for model diatones_m31?\n\n\n\n\n\n\n2.5.2 Poisson regression\nAnother useful single-parameter model is Poisson regression. This can be thought of as logistic regression, in the case where \\(p\\) is very small relative to \\(N\\): we are counting how many times \\(y\\) something happens, and there is no natural upper bound for \\(y\\).\n\\(y\\) could be the number of times each word appears in a corpus, the number of words in a lexicon with particular phonotactics, the number of languages spoken in a country, and so on. Poisson regression is not currently widely used for language data, but probably should be (Winter and Bürkner 2021).\nIn Poisson regression, the log of counts  are modeled as a linear function of one or more predictors:\n\\[\n\\log(y) = \\alpha + \\beta_1 x_1 + \\cdots\n\\] Because of the logarithm, the interpretation of \\(\\exp(\\alpha)\\) is “average value of \\(y\\), the interpretation of \\(\\exp(\\beta_1)\\) is”how much \\(y\\) is multiplied by when \\(x_1\\) is increased by 1”, and so on.\nNote the similarities with logistic regression: \\(\\log\\) (instead of logit) on the LHS, and no error term on the RHS. Just as logistic regression can be usefully thought of as modeling the probabilities \\(p_i\\) of events occurring, Poisson regression can be thought of as modeling the rates \\(\\lambda_i\\) of events occurring:\n\\[\\begin{eqnarray}\ny_i & \\sim \\text{Poisson}(\\lambda_i) \\\\\n\\log(\\lambda_i) & \\sim \\alpha + \\beta_1 x_1 + \\cdots\n\\end{eqnarray}\\]\nHere \\(y_i\\) follows the Poisson distribution,    which has an important property: its mean and variance are equal. This means that a Poisson regression model assumes that \\(E(y) = \\text{var}(y)\\): the amount of “dispersion” in the data is fixed.\nWe will use as an example the dyads data, with \\(y\\) the gestures count. The empirical distribution of gestures looks like:\n\n\nCode\ndyads %&gt;% ggplot(aes(x=gestures)) + geom_histogram(bins=10)\n\n\n\n\n\n\n\n\n\nFor now we will assume there are no predictors: the only coefficient is the intercept. A Poisson model is a kind of generalized linear model (GLM), like logistic regression, so to fit a frequentist model we would use glm()6\n\ndyads_freq_mod &lt;- glm(gestures ~ 1, family= poisson, data=dyads)\ntidy(dyads_freq_mod)\n## # A tibble: 1 × 5\n##   term        estimate std.error statistic p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 (Intercept)     3.90    0.0194      202.       0\n\nThe predicted mean of gestures is \\(e^{3.9}\\)=exp(3.9), which is very close to the observed mean (mean(dyads$gestures)).\nTo make a Bayesian model, we just need to add a prior for the intercept term; we use an uninformative prior. The probability model is:\n\\[\\begin{align}\ny & \\sim \\text{Poisson}(\\lambda) \\\\\n\\log(\\lambda) & \\sim N(0,10)\n\\end{align}\\]\n(Here we have defined \\(\\lambda = \\exp(\\alpha)\\).) This prior is relatively flat over the range of reasonable mean counts (\\(\\lambda\\)).\nTo fit this model in brms:\n\ndyads_m31 &lt;-\n  brm(data = dyads,\n      gestures ~ 1,\n      family = poisson,\n      prior = prior(normal(0, 10), class='Intercept'),\n      file='models/dyads_m31.brm'\n  )\n\nLet’s extract posterior samples using spread_draws() from tidybayes:\n\ndyads_m31 %&gt;% spread_draws(b_Intercept) -&gt; dyads_m31_draws\ndyads_m31_draws\n## # A tibble: 4,000 × 4\n##    .chain .iteration .draw b_Intercept\n##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;\n##  1      1          1     1        3.88\n##  2      1          2     2        3.89\n##  3      1          3     3        3.89\n##  4      1          4     4        3.92\n##  5      1          5     5        3.93\n##  6      1          6     6        3.88\n##  7      1          7     7        3.91\n##  8      1          8     8        3.87\n##  9      1          9     9        3.88\n## 10      1         10    10        3.89\n## # ℹ 3,990 more rows\n\nPlot the posterior of \\(\\lambda\\):\n\n\nCode\ndyads_m31_draws %&gt;%\n  ggplot(aes(x = b_Intercept)) +\n  stat_halfeye() +\n  scale_x_continuous(expression(lambda:~\"log(mean gestures)\")) + \n  ylab(\"Posterior probability\")\n\n\n\n\n\n\n\n\n\nThe estimate and 95% CI looks very similar to the glm() (frequentist) model above.\n\n\n\n\n\n\n\n\n\n2.5.2.1 Posterior predictive checks\nThis is a good example to introduce posterior predictive checks: checking the model graphically comparing simulated datasets to the observed data.7 The most basic PPC is to simulate \\(N\\) new datasets for \\(y\\) and plot their distributions on top of the observed distribution of \\(y\\).\nWe do this using pp_check() from brms, which is an interface to a wide range of PPCs from the useful bayesplot package.\n\n##  N=25 new datasets\npp_check(dyads_m31, ndraws=25)\n\n\n\n\n\n\n\n\nThe extreme mismatch here suggests this is not a good model. It is visually clear what the problem is: The Poisson model matches the mean of gestures well, but not its variance.\nAnother way to see this, which also introduces another useful posterior predictive check, is to plot different summaries of the distribution—such as its mean and variance:8\n\npp_check(dyads_m31, type='stat', stat='mean') /\npp_check(dyads_m31, type='stat', stat='var')\n## Using all posterior draws for ppc type 'stat' by default.\n## Using all posterior draws for ppc type 'stat' by default.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nThe underlying issue is that the mean and variance of gestures are not equal, as assumed by the Poisson distribution. We will return to this below to motivate a new model type.\n\n\n\n\n\n\nPractical note\n\n\n\nIn current practice, the first PPC plot (simulated data distribution overplotted on empirical distribution) is common as part of the standard checks one performs on a fitted model. PPC plots of posterior summaries, such as mean and variance, are not common. It’s safe to say that we should be performing a lot more posterior predictive checks to evaluate our fitted model, but which PPCs are most appropriate will depend on your exact case. McElreath (2020) 3.3.2.2 discusses this issue.\n\n\n\nExercise 2.4 What is the ratio \\(\\text{var}(y)/ \\text{mean}(y)\\) for the empirical data (gestures)?\n\n\n\n\n2.5.3 Normal distribution: linear regression\nLet’s move on to data distributed according to a normal distribution, which has two parameters:\n\n\\(\\mu\\): mean of \\(y\\) (i.e. expected value: \\(E(y)\\))\n\\(\\sigma\\): standard deviation of \\(y\\) (\\(\\text{var}(y) = \\sigma^2\\))\n\nStandard (frequentist) linear regression of \\(y\\) as a function of 1+ predictors can be written:\n\\[\\begin{align}\ny &\\sim N(\\mu, \\sigma) \\\\\n\\mu & = \\alpha + \\beta_1 x_1 + \\cdots\n\\end{align}\\]\nLet’s consider the simplest case, the intercept-only model. Two parameters are fitted:\n\n\\(\\alpha\\): “intercept”\n\\(\\sigma\\): “residual variance”\n\nEach observation \\(y_i\\) is predicted to have the same value plus normally distributed noise.\nExample data: english_250\n\nenglish dataset from languageR9\n\\(y\\): RTlexdec (lexical decision reaction time)\nTake a random subset of 250 observations, to make things more interesting.\n\n\n## set seed, so you'll get the same \"random\" sample\nset.seed(100)\nenglish_250 &lt;- english[sample(1:nrow(english), 250),]\n\nTo write down a probability model for this case, we need priors on \\(\\alpha\\) and \\(\\sigma\\). As above, it makes sense to give \\(\\alpha\\) an “uninformative” prior, which requires knowing something about likely values of \\(y\\). Its empirical distribution is:\n\nggplot(aes(x=RTlexdec), data=english_250) +\n  geom_histogram(bins = 30) \n\n\n\n\n\n\n\n\nNote the \\(x\\)-axis range. Certainly \\(\\alpha \\sim N(0,100)\\) would be an uninformative prior. (It’s essentially flat over possible values of \\(\\alpha\\), say -10 to 10, corresponding to reaction times of 0.00005 to 22026 seconds!)\nWhat should the prior for \\(\\sigma\\) be? It must satisfy two conditions:\n\nOnly allow positive values of \\(\\sigma\\) (\\(\\sigma&gt;0\\)).\n\nIt can’t be flat; it must decrease as \\(\\sigma\\) increases.\n\nOne choice, used by McElreath in Chap. 4, is \\(\\sigma \\sim \\text{Uniform}(0,10)\\)—replacing 10 by however large a number is needed given the scale of \\(y\\). This works for current purposes, but for more realistic models, neither uniform nor normal distributions turn out to be a good choice for variance parameters. Normal distributions are “light-tailed”—they decrease in probability too fast as \\(\\sigma\\) increase, making making MCMC sampling inefficient.\nIt is better to instead use a “heavy-tailed” distribution, which decreases slowly as \\(\\sigma\\) is increased. Most commonly used are the half-Cauchy and exponential distributions with a large “scale” parameter (see McElreath 2020, chap. 9).10\nWe will use the half-Cauchy distribution for now; this is the Cauchy distribution restricted to positive values. Here is what the PDF of \\(\\sigma\\) looks like when \\(\\sigma \\sim \\text{HalfCauchy}(0,5)\\) (scale = 0, shape = 5):\n\n\nCode\ndata.frame(sig=seq(0, 20, by=0.1)) %&gt;%\n  mutate(prob=dcauchy(sig, scale = 5)) %&gt;% \n  ggplot(aes(x=sig, y=prob)) +\n  geom_line() +\n  ylab(\"Prior probability (PDF)\") + \n  xlab(\"sigma\")\n\n\n\n\n\n\n\n\n\nThis distribution decreases gradually from 0, with a “long tail”: higher values of \\(\\sigma\\) are progressively less likely, but still have enough probability that they can be learned if the data strongly supports them. The scale here has been chosen so that the prior is effectively flat for possible values of \\(\\sigma\\): values of RTlexdec are always 6–7 range, so \\(\\sigma\\) is certainly not larger than ~1–2. (But if our \\(y\\) ranged from 0 to 100, we’d need to choose a much larger scale parameter for \\(\\sigma\\) to make the prior “uninformative”.) \nThe probability model for \\(y\\) (RTlexdec) is then:\n\\[\\begin{align}\ny & \\sim N(\\mu, \\sigma) \\\\\n\\mu & \\sim N(0, 100) \\\\\n\\sigma & \\sim \\text{HalfCauchy}(0,5)\n\\end{align}\\]\nTo fit this model in brms:\n\nenglish_m31 &lt;-\n  brm(data = english_250,\n      RTlexdec ~ 1,\n      family = gaussian,\n      prior = c(prior(normal(0, 100), class=Intercept),\n                prior(cauchy(0, 5), class = sigma)\n      ),\n      file='models/english_m31.brm'\n  )\n\n(Note how prior is now a vector to set multiple priors.)\nThis model results in posteriors for both the \\(\\mu\\) and \\(\\sigma\\) parameters:\n\nenglish_m31\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: RTlexdec ~ 1 \n##    Data: english_250 (Number of observations: 250) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     6.53      0.01     6.52     6.55 1.00     3931     2984\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.16      0.01     0.15     0.18 1.00     3632     2243\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nCompare to the equivalent frequentist model, shown in Section 2.6: it estimates \\(\\mu\\) and \\(\\sigma\\) (the “residual variance”), but only \\(\\mu\\) is a first-class model parameter (shown with a SE, \\(p\\), etc.).\nThis means we could examine the posterior distribution for \\(\\sigma\\) (below), or for \\(\\sigma\\) and \\(\\mu\\) (see Exercise below)—though we have no particular reason to do so in this case.\n\n\nCode\n## compare to the prior, above\nenglish_m31 %&gt;% \n  spread_draws(sigma) %&gt;%\n  ggplot(aes(x = sigma)) +\n  geom_density(fill = \"black\") +\n   ylab(\"Posterior PDF\")\n\n\n\n\n\n\n\n\n\n\n2.5.3.1 Parameter names\nNow that we are working with \\(&gt;1\\) parameter, let’s familiarize ourselves with a couple more aspects of working with a brms model.\nTo extract draws you need parameter names, which can be unclear from the brms output. You can always see all a model’s parameters using get_variables():\n\nget_variables(english_m31)\n##  [1] \"b_Intercept\"   \"sigma\"         \"Intercept\"     \"lprior\"       \n##  [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n##  [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"\n\nAnything ending with __ are internal parameters of the MCMC algorithm which we typically ignore.11\n\n\n2.5.3.2 Diagnostics: first look\nWhen fitting brms/Stan models it is important to inspect the posterior samples to make sure the chains “mixed” properly and you really are getting a good sample from the posterior. This is an important difference from frequentist models, whose fitting algorithm will typically give you something sensible with default settings, regardless of the data you feed it (within reason).\nWe will spend more time on model diagnostics soon, but for now note that plot() shows basic diagnostics: posterior distributions and trace plots for each parameter:\n\nplot(english_m31)\n\n\n\n\n\n\n\n\nThe trace plots look good: a “hairy caterpillar” with no chain visibly different from the others. This visual diagnostic is an important basic sanity check for any model fitted with MCMC, even simple ones.\n\nExercise 2.5  \n\nExamine the trace plot for dyads_m31, to help convince yourself that the model issue we found (predicted variance doesn’t match the sample) isn’t due to a fit issue.\n\n\n\nExercise 2.6  \n\nExtract posterior draws for english_m31 of both \\(\\alpha\\) and \\(\\sigma\\) in one dataframe. (Hint: change the arguments of spread_draws().\nExtra: Make a heatmap showing the posterior over both variables (\\(x\\)-axis = \\(\\alpha\\), \\(y\\)-axis = \\(\\sigma\\), color=probability density). Hint: geom_density_2d_filled(), or see (Kurz 2023, 4.3.6).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.4 Negative binomial distribution\nWe saw above that model dyads_m1 under-predicts variance. By fitting the mean of \\(y\\) correctly, the model is forced to assume the variance of \\(y\\) is also \\(\\mu\\)—but in reality, the variance is much higher. This situation is called overdispersion, and is extremely common when fitting (frequentist or bayesian) Poisson models—the data is noisier than predicted by a Poisson distribution.12\nThe negative binomial (or gamma-Poisson) model is a generalization of the Poisson distribution which allows \\(\\sigma^2\\) (variance of \\(y\\)) to be fitted, rather than assuming that it is the same as \\(\\lambda\\) (the mean of \\(y\\)).13 For for our purposes, all we need to know is that a negative-binomial distribution is analogous to a normal distribution (scale and shape parameters), but for count data.\nLet’s fit a negative-binomial (see Kurz 12.1.2 if interested)) for this case to see what happens. The scale parameter has the same interpretation as for the Poisson model (log-mean-counts), so we use the same prior as above. Let’s not worry about the interpretation of the shape parameter, and just use brms’s default prior.14\n\ndyads_m32 &lt;- brm(data = dyads, family = negbinomial,\n    gestures ~ 1,\n    prior = prior(normal(0, 10), class = Intercept),\n    file='models/dyads_m32.brm'\n)\n\n\ndyads_m32\n##  Family: negbinomial \n##   Links: mu = log; shape = identity \n## Formula: gestures ~ 1 \n##    Data: dyads (Number of observations: 54) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     3.90      0.07     3.77     4.04 1.00     2955     2566\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## shape     4.41      0.92     2.84     6.42 1.00     3142     2470\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nNote the less certain estimate for the intercept, relative to model dyads_31: this is a good thing.\nWe should always check trace plots (which here look good):\n\nplot(dyads_m32)\n\n\n\n\n\n\n\n\nThe posterior now looks much closer to the empirical data:\n\npp_check(dyads_m32, ndraws = 25)\n\n\n\n\n\n\n\n\nThis is clearly not a good model (note the mismatch especially around 25-60), which makes sense as it contains no predictors, but at least it now has the same rough shape as the empirical data. Both the predicted mean and variance now look reasonable, in the sense that the observed values lie within the distributions of predicted values:\n\npp_check(dyads_m32, type='stat', stat='mean') /\npp_check(dyads_m32, type='stat', stat='var')\n\n\n\n\n\n\n\n\nWe can examine in particular the posterior distribution of \\(\\text{var}(y)/\\text{mean}(y)\\), which we’ll call \\(\\phi\\). We do this by defining a custom function phi().\n\nphi &lt;- function(x){var(x)/mean(x)}\npp_check(dyads_m32, type='stat', stat=phi)\n\n\n\n\n\n\n\n\nThe model now better captures the amount of dispersion in the data (variance/mean ratio).\n\nExercise 2.7 Make the same plot for model dyads_m31, to make sure the model really does show overdisperson. (Observed \\(\\phi\\) value outside the predicted distribution of \\(\\phi\\).)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayesian basics 2</span>"
    ]
  },
  {
    "objectID": "week3.html#sec-bb2-extra",
    "href": "week3.html#sec-bb2-extra",
    "title": "2  Bayesian basics 2",
    "section": "2.6 Appendix: Extra material",
    "text": "2.6 Appendix: Extra material\n\n2.6.1 Frequentist logistic regression model\n\nm1_glm &lt;- glm(stress_shifted ~ 1, data=diatones[1:20,], family=\n                'binomial')\nsummary(m1_glm, conf.int = TRUE)\n## \n## Call:\n## glm(formula = stress_shifted ~ 1, family = \"binomial\", data = diatones[1:20, \n##     ])\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)  \n## (Intercept)  -0.8473     0.4879  -1.736   0.0825 .\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 24.435  on 19  degrees of freedom\n## Residual deviance: 24.435  on 19  degrees of freedom\n## AIC: 26.435\n## \n## Number of Fisher Scoring iterations: 4\n\n\n\n2.6.2 Frequentist linear regression model\n\nenglish_m1_lm &lt;- lm(RTlexdec ~ 1, data=english)\n\nsummary(english_m1_lm)\n## \n## Call:\n## lm(formula = RTlexdec ~ 1, data = english)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.34477 -0.12457  0.00037  0.10311  0.63771 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 6.550097   0.002322    2821   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1569 on 4567 degrees of freedom\n\n\n\n2.6.3 Poisson regression model with predictors\n\nExercise 2.8 Consider the empirical effects of of context and language on gesture count:\n\nggplot(aes(x=context, y=gestures), data=dyads) + geom_boxplot() + xlab(\"Context\") + ylab(\"Gestures\") + scale_y_log10()\nggplot(aes(x=language, y=gestures), data=dyads) + geom_boxplot() + xlab(\"Context\") + ylab(\"Gestures\") + scale_y_log10()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(The \\(y\\)-axis is on a log scale, as usually makes sense for count data.)\nFit this Poisson regression model:\n\ndyads_freq_mod2 &lt;- glm(gestures ~ context + language , family= poisson, data=dyads)\n\nWhat are the interpretations of the context and language coefficients?\n\n\n\n\n\nBaayen, R. H. 2008. Analyzing linguistic data: A practical introduction to statistics using R. Cambridge: Cambridge University Press.\n\n\nBaayen, R. H., and Elnaz Shafaei-Bajestan. 2019. languageR: Analyzing Linguistic Data: A Practical Introduction to Statistics. https://CRAN.R-project.org/package=languageR.\n\n\nBürkner, Paul-Christian, Jonah Gabry, Matthew Kay, and Aki Vehtari. 2024. “Posterior: Tools for Working with Posterior Distributions.” https://mc-stan.org/posterior/.\n\n\nGabry, Jonah, and Tristan Mahr. 2024. “Bayesplot: Plotting for Bayesian Models.” https://mc-stan.org/bayesplot/.\n\n\nGabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. “Visualization in Bayesian Workflow.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 182 (2): 389–402.\n\n\nGelman, Andrew. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).” Bayesian Analysis 1 (3): 515–34.\n\n\nKay, Matthew. 2023. tidybayes: Tidy Data and Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151.\n\n\nKurz, S. 2023. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nNicenboim, B., D. Schad, and S. Vaishth. 2024. An Introduction to Bayesian Data Analysis for Cognitive Science. https://vasishth.github.io/bayescogsci/book/. 5/2024 version.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.\n\n\nWinter, Bodo, and Paul-Christian Bürkner. 2021. “Poisson Regression for Linguists: A Tutorial Introduction to Modelling Count Data with Brms.” Language and Linguistics Compass 15 (11): e12439.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayesian basics 2</span>"
    ]
  },
  {
    "objectID": "week3.html#footnotes",
    "href": "week3.html#footnotes",
    "title": "2  Bayesian basics 2",
    "section": "",
    "text": "Like all “tidy” packages to do \\(x\\): you don’t have to use this package to do \\(x\\), it just makes things much easier if you are working with tidyverse packages already.↩︎\nHe also calls these compatibility intervals, to avoid the implications of “compatibility” or “confidence”, but this is non-standard.↩︎\nSee this ggdist vignette for other possible visualizations.↩︎\nIn an equation: \\[\n\\underbrace{P(\\vec{y}_{new} | \\vec{y})}_{\\text{posterior predictive distribution}} \\propto \\int P(\\vec{y}_{new} | \\vec{\\theta}) P(\\vec{\\theta} | \\vec{y}) d\\vec{\\theta}\n\\] Another description, from Nicenboim et al. Sec. 3.5: “After we have seen the data and obtained the posterior distributions of the parameters, we can now use the posterior distributions to generate future data from the model. In other words, given the posterior distributions of the parameters of the model, the posterior predictive distribution gives us some indication of what future data might look like.”↩︎\nTechnically: family = binomial means “GLM for binomial distribution with logit link”, often just called “logistic regression”.↩︎\nSpecifically, family=poisson fits a Poisson model with a log link.↩︎\nFor more detail, see Gabry et al. (2019) Sec. 4-5.↩︎\nNote that mean/var from all 4000 draws are now plotted, not just the first \\(N=25\\).↩︎\nThis dataset is from the languageR package Baayen and Shafaei-Bajestan (2019) and described in detail on its help page (?english) or in Baayen (2008). Our use of this dataset follows Sonderegger (2023) (Sec. 4.1.2), where it is used extensively for linear regression.↩︎\nMore details: see Gelman (2006), and updated recommendations in the Stan Prior Choice Recommendations doc.↩︎\nFor example, lp__ stands for “log posterior”. Stan defines the log-posterior up to a constant, which is treated as an unknown variable; lp__ is this “log kernel” (see here and here).↩︎\n“Underdispersion” is theoretically possible, but much less common in practice. Overdispersion is so common that it is bad teaching to show you Poisson regression without discussing overdispersion, which is why we’ve taken this detour to a scary-sounding model.↩︎\nSee also here or Ch. 11-12 of McElreath.↩︎\nIf you are curious: the variance is \\(\\sigma^2 = \\mu(1 + \\frac{\\mu}{\\phi})\\) and the shape parameter is \\(\\phi\\), following the alternative paramterization in Stan. Thus, \\(\\phi &gt; 0\\), and small/large \\(\\phi\\) = high/low overdispersion.)↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayesian basics 2</span>"
    ]
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "3  Bayesian Regression Models 1",
    "section": "",
    "text": "3.1 Preliminaries\nLoad libraries we will need:\nlibrary(tidyverse)\nlibrary(brms)\n\nlibrary(broom) # for tidy model summaries\nlibrary(modelr) # for the example using data_grid\n\nlibrary(tidybayes)\nlibrary(bayestestR)\n\nlibrary(languageR) # for `english' dataset\nlibrary(arm)\n\ndata_grid &lt;- modelr::data_grid",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Regression Models 1</span>"
    ]
  },
  {
    "objectID": "week4.html#preliminaries",
    "href": "week4.html#preliminaries",
    "title": "3  Bayesian Regression Models 1",
    "section": "",
    "text": "Practical note\n\n\n\n\n\nIf you have loaded rethinking, you need to detach it before using brms. See Kurz (2023) Sec. 4.3.1.\nI use the file argument when fitting brms models to make compiling this document easier (so the models don’t refit every time I compile). You may or may not want to do this for your own models. See file and file_refit arguments in ?brm.\n\n\n\n\n3.1.1 Datasets\nLoad the diatones and dyads datasets, described in Section 1.1 and Section 2.1.\n\ndiatones &lt;- read.csv(\"https://osf.io/tqjm8/download\", stringsAsFactors = TRUE)\ndyads &lt;- read.csv(\"https://osf.io/6j8kc/download\")\n\nPerform some data cleaning and recoding for diatones, described in Secs. 6.1.1 and. 6.7.2.1 of Regression Modeling for Linguistic Data (Sonderegger 2023).\n\n# make numeric versions of all categorical predictors,\n# while saving original versions\ndiatones &lt;- diatones %&gt;% mutate(\n  syll1_coda_orig = syll1_coda,\n  syll2_coda_orig = syll2_coda,\n  syll2_td_orig = syll2_td,\n  ## turns no/yes -&gt; 0/1\n  syll1_coda = ifelse(syll1_coda == \"no\", 0, 1),\n  ## turns '0'/'C'/'CC'/'CCC' -&gt; 0/1/2/3\n  syll2_coda = str_count(syll2_coda_orig, \"C\"),\n  syll2_td = ifelse(syll2_td == \"no\", 0, 1)\n)\n\n## standardize all predictors using arm::rescale\ndiatones &lt;- diatones %&gt;% mutate(\n  syll1_coda = rescale(syll1_coda_orig),\n  syll2_td = rescale(syll2_td_orig),\n  syll2_coda = rescale(syll2_coda),\n  frequency = rescale(frequency)\n)\n\nDefine the english_250 dataframe, like last week:\n\n## set seed, so you'll get the same \"random\" sample\nset.seed(100)\nenglish_250 &lt;- english[sample(1:nrow(english), 250), ]\n\nWe will also use a much smaller subset (\\(n=25\\)) for some examples:\n\nset.seed(10)\nenglish_25 &lt;- english[sample(1:nrow(english), 25), ]\n\nThe same dataset (english) is used extensively in Chapters 4 and 5 of Sonderegger (2023) in examples of frequentist linear regression models, which parallel these notes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Regression Models 1</span>"
    ]
  },
  {
    "objectID": "week4.html#regression-models-introduction",
    "href": "week4.html#regression-models-introduction",
    "title": "3  Bayesian Regression Models 1",
    "section": "3.2 Regression models: introduction",
    "text": "3.2 Regression models: introduction\nLet’s (finally) start fitting models including predictors. We will again use “uninformative” priors without discussion, and return to priors below (Section 3.5).\n\n3.2.1 Simple linear regression\nFirst example: the effect of WrittenFrequency on RTlexdec. The empirical effect, with least-squares lines of best fit:\n\n\nCode\nenglish_250 %&gt;% ggplot(aes(x = WrittenFrequency, y = RTlexdec)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nThe frequentist model in this case can be written:\n\\[\\begin{align}\ny_i &\\sim N(\\mu, \\sigma) \\\\\n\\mu & = \\alpha + \\beta_1 x_{1i},\n\\end{align}\\]\nwhere \\(x_{1i}\\) is the value of WrittenFrequency for observation \\(i\\) (\\(i=1, \\ldots, n\\)). Fit this model and report its results:1\n\nenglish_m41_freq &lt;- lm(RTlexdec ~ 1 + WrittenFrequency, data = english_250)\n\n## kable() formats the table for markdown output.\n##\n## use\n## summary(english_m41_freq)\n## in your R console to just see the uusal model table\nkable(tidy(english_m41_freq, conf.int = TRUE), format = \"markdown\", digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n6.72\n0.03\n259.42\n0\n6.67\n6.77\n\n\nWrittenFrequency\n-0.04\n0.00\n-7.65\n0\n-0.04\n-0.03\n\n\n\n\n\nWe fit a Bayesian intercept-only model for this case in Section 2.5.3, using priors for \\(\\mu\\) and \\(\\sigma\\).\n\n3.2.1.1 Prior for \\(\\sigma\\)\nIn contrast to that model, where a half-Cauchy prior was used, we will now use an exponential prior for \\(\\sigma\\). This distribution (see the link) has a single “rate” parameter \\(\\lambda\\), and expected value \\(1/\\lambda\\), so smaller \\(\\lambda\\) values make the distribution wider.\nBoth half-Cauchy and exponential have properties which make them good priors for variance parameters like \\(\\sigma\\), discussed in Section 2.5.3 :\n\nRestrict \\(\\sigma\\) to be positive\nLong tail \\(\\Rightarrow\\) high \\(\\sigma\\) values possible, but a priori unlikely\nSampling posterior (and posterior predictive distribution) with MCMC algorithms works well\n\nIt turns out that the exponential distribution is better suited for a fourth desideratum: sampling from the prior predictive distribution, using an MCMC algorithms.2\nThis is the prior we will be using for \\(\\sigma\\) in today’s linear regression models, \\(\\text{Exponential}(1)\\):\n\n\nCode\ndata.frame(sig = seq(0, 5, by = 0.1)) %&gt;%\n  mutate(prob = dexp(sig, rate = 1)) %&gt;%\n  ggplot(aes(x = sig, y = prob)) +\n  geom_line() +\n  ylab(\"Prior probability (PDF)\") +\n  xlab(expression(sigma)) +\n  theme(axis.title.x = element_text(size = 15))\n\n\n\n\n\n\n\n\n\nRecall that reasonable values of RTlexdec are 6–7, thus \\(\\sigma\\) of ~1 would be large, so this is a relatively uninformative prior.\n\n\n3.2.1.2 Prior for \\(\\beta_1\\)\nThe additional piece we need to write down the probability model here is a prior on \\(\\beta_1\\). We will use \\(\\beta \\sim N(0, 5)\\), which is “uninformative” by this logic:\n\nCenter at 0: uninformative about effect direction\nFor RTlexdec a change of 1 is huge; for WrittenFrequency a change of 10 is huge (see range of the data), so a slope of 1/10 = 0.1 is large. (So +- 3\\(\\sigma\\), when \\(\\sigma\\)=5, is really, really large.)\n\nThis graph shows that plausible effects (\\(|\\beta_1| &lt; 0.3\\)) have roughly flat prior probability:\n\n\nCode\ndnorm_limit &lt;- function(x) {\n  y &lt;- dnorm(x, sd = 5)\n  y[x &gt; 0.3 | x &lt; (-0.3)] &lt;- NA\n  return(y)\n}\n\n\ndf &lt;- data.frame(beta1 = seq(-3, 3, by = 0.001), p = dnorm(x = seq(-3, 3, by = 0.001), sd = 5))\n\n## log-odds of -5 to 5\ndf %&gt;% ggplot(aes(x = beta1, y = p)) +\n  geom_line() +\n  xlab(\"beta_1\") +\n  ylab(\"Prior probability (density)\") +\n  xlim(-3, 3) +\n  stat_function(fun = dnorm_limit, geom = \"area\", fill = \"blue\", alpha = 0.2)\n\n\n\n\n\n\n\n\n\n(I have used a very wide prior here so it is also flat for the effects of AgeSubject and polynomials of RTlexdec that we’ll consider below.)\nThe probability model for \\(y\\) (RTlexdec) is then:\n\\[\\begin{align}\ny_i & \\sim N(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\alpha + \\beta_1 x_{1i} \\\\\n\\alpha & \\sim N(0, 100) \\\\\n\\beta_1 & \\sim N(0,5) \\\\\n\\sigma & \\sim \\text{Exponential}(1) \\\\\n\\end{align}\\]\nNote the distinction here between ~ and =.\nFit this model in brms:\n\n## this uses defaults for MCMC: chains = 4, iter = 2000, warmup = 1000\nenglish_m41 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + WrittenFrequency,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 100), class = Intercept),\n      prior(normal(0, 5), class = b),\n      prior(exponential(1), class = sigma)\n    ),\n    file = \"models/english_m41.brm\"\n  )\n\n(If you fit this model you’ll see a lot of output showing the fitting process; I’ll suppress brms output throughout these notes.)\n\n\n\n\n\n\nBroader context\n\n\n\n\n\nSee Kurz (2023) Sec. 4.4.2 for unpacking of how each piece of the brm formula corresponds to the probability model above. Nicenboim, Schad, and Vaishth (2024) Sec. 3.1.1 also gives more detail.\n\n\n\nThe Intercept and WrittenFrequency estimates, based on the posterior summaries, are very similar to the frequentist model above. Always check the trace plots for hairy caterpillars and reasonable-looking posteriors (Section 2.5.3.2):\n\nplot(english_m41)\n\n\n\n\n\n\n\n\nTo get a sense of what this model predicts, consider the posterior for \\(\\alpha\\) and \\(\\beta_1\\):\n\n\nCode\nenglish_m41 %&gt;%\n  spread_draws(b_Intercept, b_WrittenFrequency) %&gt;%\n  ggplot(aes(x = b_Intercept, y = b_WrittenFrequency)) +\n  geom_hex() +\n  xlab(\"alpha\") +\n  ylab(\"beta_1\")\n\n\n\n\n\n\n\n\n\nThis gives a posterior distribution over relationships between \\(y\\) and \\(x\\), which we could imagine as a generative process (generate lines \\(y = \\alpha + \\beta_1 x\\)) as follows:\n\nChoose values of \\(\\alpha\\) and \\(\\beta_1\\).\nPredict \\(y\\) for \\(x\\) = 6, 6.01, … , 7.\nPlot \\(x\\) vs \\(y\\).\nRepeat many times (100, say), putting all lines on the same plot.\n\nKurz (2023) Sec. 4.4.1 shows how to do this by hand. We instead use tidybayes functionality (see vignette for details), as follows.\nFirst, get draws of the expected value of \\(y\\) as a function of \\(x\\). We use data_grid() from the modelR package to set up the dataframe of values of \\(x\\) to predict at, then use the tidybayes add_epred_draws() function:\n\ny_draws &lt;- english_250 %&gt;%\n  ## get grid of values on x axis\n  data_grid(WrittenFrequency = seq_range(WrittenFrequency, n = 101)) %&gt;%\n  add_epred_draws(english_m41, ndraws = 100)\n\nVisualize:\n\n\nCode\ny_draws %&gt;%\n  ggplot(aes(x = WrittenFrequency, y = RTlexdec)) +\n  geom_line(aes(y = .epred, group = .draw), alpha = .1) +\n  geom_point(data = english_250) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThis is (informally) called a spaghetti plot. It can be useful for understanding to animate the plot so you see one line at a time (a hypothetical outcome plot).\n\n\n\n3.2.2 Predicted effects 1\nIt is more common to visualize the model’s predicted effects (here, of \\(x\\)), with a “confidence interval” (a.k.a. “credibility interval”)—by showing some overall predicted effect with uncertainty (e.g. taking the median and 95% CI across the spaghetti, of \\(y\\) at each value of \\(x\\)). Using tidybayes:\n\n\nCode\n## get grid of values on x axis\nenglish_250 %&gt;%\n  data_grid(WrittenFrequency = seq_range(WrittenFrequency, n = 101)) %&gt;%\n  add_epred_draws(english_m41) %&gt;%\n  ggplot(aes(x = WrittenFrequency, y = RTlexdec)) +\n  stat_lineribbon(aes(y = .epred), alpha = 0.1) +\n  geom_point(data = english_250)\n\n\n\n\n\n\n\n\n\nEven simpler, just showing the 95% CI:\n\n\nCode\n## get grid of values on x axis\nenglish_250 %&gt;%\n  data_grid(WrittenFrequency = seq_range(WrittenFrequency, n = 101)) %&gt;%\n  add_epred_draws(english_m41) %&gt;%\n  ggplot(aes(x = WrittenFrequency, y = RTlexdec)) +\n  stat_lineribbon(aes(y = .epred), .width = c(0.95), alpha = 0.5) +\n  geom_point(data = english_250) +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Confidence intervals (or 'credibility')\")\n\n\n\n\n\n\n\n\n\nWe could alternatively plot posterior predictions—using uses the posterior for actual observations, not means. This would mean:\n\nChoose values of \\(\\alpha\\), \\(\\beta_1\\), and \\(\\sigma\\).\nPredict values of \\(y\\) for \\(x\\) = 6, … , 7 using \\(\\alpha\\) and \\(\\beta_1\\), adding noise using \\(\\sigma\\).\nPlot \\(x\\) vs \\(y\\).\nRepeat 1000 times, then plot the median + 95% PI of \\(y\\) for each value of \\(x\\).\n\nWe can do this using the tidybayes add_predicted_draws() function:\n\n\nCode\n## get grid of values on x axis\nenglish_250 %&gt;%\n  data_grid(WrittenFrequency = seq_range(WrittenFrequency, n = 101)) %&gt;%\n  add_predicted_draws(english_m41) %&gt;%\n  ggplot(aes(x = WrittenFrequency, y = RTlexdec)) +\n  stat_lineribbon(aes(y = .prediction), .width = c(.95), alpha = 1 / 4) +\n  geom_point(data = english_250) +\n  ggtitle(\"Prediction intervals\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nNote how the 95% prediction interval (PI) contains most observations, while the 95% CI does not, as expected.\nMore generally, for any values of the predictors, we can extract a posterior distribution over:\n\nthe fitted values (calculated just using predictors)\nthe model predictions (the actual \\(y_i\\) values, taking noise into account)\n\nThe terminology is not consistent across packages/authors, but the existence of these two type of “predictions” is. In brms, they are extracted using functions fitted() and predict(), respectively. In tidybayes and rstan, functions of these two types contain the strings epred and predict (as in the tidybayes functions used to make plots above).\nAs an example, using brms functions: the mean + 95% CI for the fitted values for the first 5 observations in english_250 are:\n\nfitted(english_m41) %&gt;% head(n = 5)\n##      Estimate   Est.Error     Q2.5    Q97.5\n## [1,] 6.584161 0.011296955 6.562595 6.606617\n## [2,] 6.462223 0.012807470 6.437004 6.487829\n## [3,] 6.358862 0.023808050 6.312888 6.406904\n## [4,] 6.541302 0.009289047 6.523069 6.559617\n## [5,] 6.591168 0.011830462 6.568698 6.614645\n\nThe same for model predictions:\n\npredict(english_m41) %&gt;% head(n = 5)\n##      Estimate Est.Error     Q2.5    Q97.5\n## [1,] 6.586306 0.1442737 6.301712 6.872248\n## [2,] 6.459703 0.1451474 6.183633 6.742939\n## [3,] 6.358503 0.1453746 6.079538 6.643806\n## [4,] 6.540256 0.1441920 6.254455 6.820970\n## [5,] 6.590544 0.1464116 6.301902 6.870175\n\nThe distinction here is the same as between “confidence intervals” and “prediction intervals” for frequentist models (see Sonderegger 2023, sec. 8.8.2).\n\n\n\n\n\n\nBroader context\n\n\n\n\n\nAll functionality above makes use of a key property of Bayesian models: they predict a distribution, whether using “model predictions” or “fitted values”. This is why the fitted() and predict() results show extra rows, compared to the same functions applied to a frequentist model:\n\nfitted(english_m41_freq) %&gt;% head()\n##     3786      503     3430     3696     4090     3052 \n## 6.584325 6.462561 6.359349 6.541527 6.591322 6.403717\npredict(english_m41_freq) %&gt;% head()\n##     3786      503     3430     3696     4090     3052 \n## 6.584325 6.462561 6.359349 6.541527 6.591322 6.403717\n\nIt’s still possible to get uncertainties in fitted values and predictions for frequentist models, of course (see RMLD Sec. XX), and in this case would be trivial. Adding uncertainties to frequentist model predictions just takes a bit of extra work, where the amount of work increases as model complexity increases. The point here is that Bayesian models give us uncertainty in fitted and predicted values “for free”, as part of the procedure (sampling from the posterior) by which these are computed from the model.\n\n\n\n\nExercise 3.1  \n\nFit the same model, now to english_25.\nMake a couple of the plots above (spaghetti, effects with 95% CI or PI).\nHow do these look different from the plots for model english_m41, and why?\n\n\n\n\n3.2.3 Standardizing predictors\nNote how the hex plot above (Section 3.2.1.2) shows that the posteriors for \\(\\alpha\\) and \\(\\beta_1\\) are not independent. This follows naturally from the form of the model (see exercises), and in particular the fact that predictors are not centered, which makes the estimates of \\(\\alpha\\) and \\(\\beta_1\\) correlated.\nIn general we will standardize predictors—minimally by centering, often by scaling by 1–2\\(\\sigma\\)—for this reason, as well as all reasons familiar from frequentist regression models (see Sonderegger 2023, sec. 5.5.4). The intercept becomes easier to interpret, main-effect coefficients have easier interpretations in the presence of interactions between predictors, models fit faster, etc.3\nLet’s refit the current model after centering \\(x_1\\):\n\n## scale with scale = FALSE just centers\nenglish_250 &lt;- mutate(\n  english_250,\n  WrittenFrequency_c = scale(WrittenFrequency, scale = FALSE)\n)\n\n\nenglish_m42 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + WrittenFrequency_c,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 100), class = Intercept),\n      prior(normal(0, 5), class = b),\n      prior(exponential(1), class = sigma)\n    ),\n    file = \"models/english_m42.brm\"\n  )\n\nMake a spaghetti plot for this model:\n\n\nCode\n## get grid of values on x axis\nenglish_250 %&gt;%\n  data_grid(WrittenFrequency_c = seq_range(WrittenFrequency_c, n = 101)) %&gt;%\n  add_epred_draws(english_m42, ndraws = 100) %&gt;%\n  ggplot(aes(x = WrittenFrequency_c, y = RTlexdec)) +\n  geom_line(aes(y = .epred, group = .draw), alpha = .1) +\n  geom_point(data = english_250) +\n  xlab(\"WrittenFrequency (centered)\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical note\n\n\n\n\n\nAssuming you have standardized predictors (including using a “centered” contrast coding scheme for factors), brms will usually choose sensible “weakly informative” priors by default. This is an important additional reason to standardize your data before fitting models. It is currently common practice in work using brms/Stan to just use default priors, even though this is not recommended—it’s always good to actually think through prior choices.\nThis is why we’re working through priors in some detail in this and surrounding chapters, but in a pinch, we sometimes just fit a model without specifying priors, confident that brms defaults will work.\n\n\n\n\nExercise 3.2  \n\nWhy does the hex plot of the \\(\\alpha\\), \\(\\beta_1\\) posterior for english_m41 show a negative relationship between \\(\\alpha\\) and \\(\\beta_1\\)? (Hint: 4)\nMake the same plot for model english_m42 and verify that the posteriors for \\(\\alpha\\) and \\(\\beta_1\\) now look independent. Why does this make sense once \\(x_1\\) has been centered? (Hint: 5)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Regression Models 1</span>"
    ]
  },
  {
    "objectID": "week4.html#sec-mult-lin-reg",
    "href": "week4.html#sec-mult-lin-reg",
    "title": "3  Bayesian Regression Models 1",
    "section": "3.3 Multiple linear regression",
    "text": "3.3 Multiple linear regression\nFitting a regression with multiple predictors is similar. Let’s consider AgeSubject and WrittenFrequency as predictors of RTlexdec for the english_250 data. The empirical pattern is:\n\n\nCode\nenglish_250 %&gt;% ggplot(aes(x = WrittenFrequency, y = RTlexdec)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", aes(color = AgeSubject))\n\n\n\n\n\n\n\n\n\nBy the same logic as above:\n\nCenter AgeSubject by making a SubjectOld predictor with values -0.5 and 0.5\n\\(\\beta_2\\) = effect of SubjectYoung = \\(x_2\\)\nPrior: \\(\\beta_2 \\sim N(0,5)\\)\n\n\nenglish_250 &lt;- english_250 %&gt;% mutate(\n  SubjectYoung = as.numeric(AgeSubject)-1.5\n)\n\nFit the model, with\n\nenglish_m43 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 100), class = Intercept),\n      prior(normal(0, 5), class = b),\n      prior(exponential(1), class = sigma)\n    ),\n    file = \"models/english_m43.brm\"\n  )\n\nFitted model:\n\nenglish_m43\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung \n##    Data: english_250 (Number of observations: 250) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept              6.55      0.01     6.54     6.56 1.00     4356     3342\n## WrittenFrequency_c    -0.03      0.00    -0.04    -0.03 1.00     4936     3226\n## SubjectYoung          -0.24      0.01    -0.26    -0.21 1.00     3215     2425\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.08      0.00     0.08     0.09 1.00     3411     2825\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nNote that class = b makes the \\(N(0,5)\\) prior apply to all \\(\\beta\\) coefficients (\\(\\beta_1\\), \\(\\beta_2\\), …)\nSpaghetti plot of the predicted relationship: \n\n\nCode\nenglish_250 %&gt;%\n  group_by(SubjectYoung) %&gt;%\n  data_grid(WrittenFrequency_c = seq_range(WrittenFrequency_c, n = 101), SubjectYoung = c(-0.5, 0.5)) %&gt;%\n  ## changed to 25 lines instead of 100 to\n  ## make the graph more legible\n  add_epred_draws(english_m43, ndraws = 25) %&gt;%\n  ggplot(aes(x = WrittenFrequency_c, y = RTlexdec)) +\n  geom_line(aes(y = .epred, group = paste(SubjectYoung, .draw), color = SubjectYoung, alpha = .1)) +\n  geom_point(data = english_250) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nExercise 3.3 Make an analagous plot showing prediction intervals (like the relevant plot for model english_m41 above).\n\n\n\n3.3.1 Marginal effects\nFor multiple regression models it is often/usually of interest to plot the effect of varying 1+ predictors while holding others constant (e.g. at 0, or the mean). These are usually called marginal effects, or conditional effects (or partial effects, though this is less precise); see definition/discussion in Sonderegger (2023) (Sec 5.5.4, 6.7.3, later sections). McElreath calls these “counterfactual plots”, which is more descriptive and general, but nonstandard.6\nAs always, you can use pre-written functions for marginal effects or make predictions yourself. One pre-written option is the conditional_effects() function from brms. We use this to make a partial-effect plot for WrittenFrequency_c (holds SubjectYoung = 0, means averaging over old and young subjects):\nPartial effect plot for WrittenFrequency_c using conditional_effects (holds SubjectYoung = 0, means averaging over old and young subjects):\n\nconditional_effects(english_m43, effects = \"WrittenFrequency_c\")\n\n\n\n\n\n\n\n\nThis function can be made to return a dataframe, which you can manipulate directly to make better plots:\n\nconditional_effects(english_m43, effects = \"WrittenFrequency_c\")$WrittenFrequency_c -&gt; freqPartial_df\nhead(freqPartial_df) ## lower/upper/estimate = 95% CredI + mean of posterior\n##   WrittenFrequency_c RTlexdec SubjectYoung cond__ effect1__ estimate__\n## 1          -5.187430 6.534968        0.048      1 -5.187430   6.711423\n## 2          -5.085406 6.534968        0.048      1 -5.085406   6.707919\n## 3          -4.983382 6.534968        0.048      1 -4.983382   6.704429\n## 4          -4.881357 6.534968        0.048      1 -4.881357   6.700953\n## 5          -4.779333 6.534968        0.048      1 -4.779333   6.697442\n## 6          -4.677308 6.534968        0.048      1 -4.677308   6.693975\n##         se__  lower__  upper__\n## 1 0.01518186 6.682297 6.740544\n## 2 0.01490169 6.679392 6.736576\n## 3 0.01463982 6.676453 6.732593\n## 4 0.01436069 6.673575 6.728691\n## 5 0.01408509 6.670724 6.724725\n## 6 0.01379316 6.667786 6.720763\nfreqPartial_df %&gt;% ggplot(aes(x = WrittenFrequency_c, y = estimate__)) +\n  geom_lineribbon(aes(ymin = lower__, ymax = upper__)) +\n  geom_point(aes(x = WrittenFrequency_c, y = RTlexdec), data = english_250, size = 0.5) +\n  ylab(\"RTlexdec\") +\n  ggtitle(\"Partial effect of written frequency\") +\n  ylab(\"RTlexdec: Estimate + 95% CI\")\n\n\n\n\n\n\n\n\nExample of making our own partial effect plot for AgeSubject, with prediction intervals:\n\n## Make a dataframe of values to predict at: vary SubjectYoung hold WrittenFrequency_c at 0\nagePartial_df &lt;- expand_grid(SubjectYoung = c(-0.5, 0.5), WrittenFrequency_c = 0)\nagePartial_df\n## # A tibble: 2 × 2\n##   SubjectYoung WrittenFrequency_c\n##          &lt;dbl&gt;              &lt;dbl&gt;\n## 1         -0.5                  0\n## 2          0.5                  0\n\n## for 95% CIs, we'd use 'fitted' instead of 'predict' here.\npredict(english_m43, newdata = agePartial_df) -&gt; fittedVals\nfittedVals\n##      Estimate  Est.Error     Q2.5    Q97.5\n## [1,] 6.667137 0.08236155 6.501877 6.826921\n## [2,] 6.429246 0.08451450 6.259287 6.593990\n\nagePartial_df &lt;- bind_cols(agePartial_df, data.frame(fittedVals))\n\n## add back AgeSubject for plotting\nagePartial_df &lt;- agePartial_df %&gt;% mutate(AgeSubject = factor(SubjectYoung, labels = levels(english_250$AgeSubject)))\n\n\nagePartial_df %&gt;% ggplot(aes(x = AgeSubject, y = Estimate)) +\n  geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5)) +\n  geom_jitter(aes(x = AgeSubject, y = RTlexdec), data = english_250, size = 0.5, width = 0.1, alpha = 0.25) +\n  ggtitle(\"Partial effect of AgeSubject\") +\n  ylab(\"RTlexdec: estimate + 95% PI\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical note\n\n\n\n\n\nOther pre-written options for making model predictions include the powerful emmeans Lenth (2024) and marginaleffects Arel-Bundock, Greifer, and Heiss (Forthcoming) packages, which are compatible with brms models. I highly recommend making regular use of one of these packages, which work across a range of model types. See Sonderegger (2023), Chap. 7 and throughout, for examples of using emmeans for linguistic data.\nIt’s important to bear in mind the trade-off between using pre-written functions and “rolling your own”. A pre-written option is easier, but the code you use may break in the future and you know less about the details of how predictions are made, which can be important. Making predictions yourself is harder, but the code you write will be more stable, and you’ll know all the details. In practice, I recommend finding a middle way that makes sense for you.\n\n\n\n\nExercise 3.4 Make a plot showing the posterior of the predicted RTlexdec difference between AgeSubject=old and young, holding WrittenFrequency_c at 0. This should look like a “half-eye” plot (Section 2.3.5) (Hint: 7)\n\n\n\n3.3.2 Nonlinear effects\nPlotting with a smoother instead of a line of best fit, it looks like the WrittenFrequency_c relationship could be nonlinear:\n\n\n\n\n\n\n\n\n\nThe easiest way to fit a nonlinear function is using polynomials. We will use orthogonal polynomials, implemented using the poly() function; see Sonderegger (2023) Sec. 7.5.3 for a refresher.8\nFor a polynomial of order 3 (to model the two inflection points visible in AgeSubject):\n\nenglish_m44 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + poly(WrittenFrequency_c, 3) + SubjectYoung,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 100), class = Intercept),\n      prior(normal(0, 5), class = b),\n      prior(exponential(1), class = sigma)\n    ),\n    file = \"models/english_m44.brm\"\n  )\n\n\nenglish_m44\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: RTlexdec ~ 1 + poly(WrittenFrequency_c, 3) + SubjectYoung \n##    Data: english_250 (Number of observations: 250) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## Intercept                    6.55      0.01     6.54     6.56 1.00     5341\n## polyWrittenFrequency_c31    -1.05      0.08    -1.21    -0.89 1.00     4788\n## polyWrittenFrequency_c32     0.15      0.08    -0.02     0.31 1.00     5190\n## polyWrittenFrequency_c33     0.14      0.08    -0.02     0.29 1.00     5125\n## SubjectYoung                -0.24      0.01    -0.26    -0.22 1.00     4948\n##                          Tail_ESS\n## Intercept                    3075\n## polyWrittenFrequency_c31     2852\n## polyWrittenFrequency_c32     2857\n## polyWrittenFrequency_c33     3114\n## SubjectYoung                 3123\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.08      0.00     0.08     0.09 1.00     4908     3251\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe coefficients here are not easily interpretable.9 We typically interpret nonlinear effects by examining model predictions. Make a spaghetti plot of the fitted values (not a partial effect plot of WrittenFrequency_c):\n\n\nCode\nenglish_250 %&gt;%\n  data_grid(WrittenFrequency_c = seq_range(WrittenFrequency_c, n = 101), SubjectYoung = c(-0.5, 0.5)) %&gt;%\n  add_epred_draws(english_m44, ndraws = 50) %&gt;%\n  ggplot(aes(x = WrittenFrequency_c, y = RTlexdec)) +\n  ## the factor() stuff is just to get SubjectYoung = -0.5, 0.5 to appear as blue/red in graph\n  geom_line(aes(y = .epred, group = paste(factor(SubjectYoung), .draw), color = factor(SubjectYoung)), alpha = .1) +\n  geom_point(data = english_250) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nNote how the effect is much more uncertain near the upper and lower bounds of the predictor (WrittenFrequency_c). This is familiar from a polynomial effect in a frequentist model (higher CIs near the boundaries), but a Bayesian model makes it clearer what is going on: many polynomials, which differ only near the endpoints, are consistent with this data. Avoiding such fits is a major motivation for using splines, which we’ll turn to later in the course with GAMs.\n\nExercise 3.5 Make a partial effect plot for WrittenFrequency_c for model english_m44, with 95% CIs.\n\n\n\n3.3.3 Multicollinearity: example\nCorrelations between predictors (more precisely, of the effects of predictors after controlling for others), or multicollinearity, is an important issue to take into account for all regression models. For Bayesian regression models, an example using highly-correlated predictors will help us understand what the coefficients of a fitted Bayesian model mean.\nReview Sonderegger (2023) Sec. 5.6, if needed, on the definition of multicollinearity, and the example of predicting RTlexdec using WrittenFrequency and Familiarity, which are highly correlated in english, but both negatively correlate with RTlexdec. In the english_25 subset in particular:\n\ncor(dplyr::select(english_25, RTlexdec, WrittenFrequency, Familiarity))\n##                    RTlexdec WrittenFrequency Familiarity\n## RTlexdec          1.0000000       -0.4016724  -0.5791578\n## WrittenFrequency -0.4016724        1.0000000   0.8313906\n## Familiarity      -0.5791578        0.8313906   1.0000000\n\npairscor.fnc(dplyr::select(english_25, RTlexdec, WrittenFrequency, Familiarity))\n\n\n\n\n\n\n\n\nWe will fit a linear regression for english_25:\n\n\\(y\\): RTlexdec\n\\(x_1\\), \\(x_2\\), \\(x_3\\) : WrittenFrequency_c, SubjectYoung, Familiarity_c\nSame priors as for models above.\n\n\n# center predictors\nenglish_25 &lt;- mutate(english_25,\n  WrittenFrequency_c = scale(WrittenFrequency, scale = FALSE),\n  Familiarity_c = scale(Familiarity, scale = FALSE),\n  SubjectYoung = as.numeric(AgeSubject) - 1.5\n)\n\n\nenglish_collin_m1 &lt;-\n  brm(\n    data = english_25,\n    RTlexdec ~ 1 + WrittenFrequency_c + Familiarity_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 100), class = Intercept),\n      prior(normal(0, 5), class = b),\n      prior(exponential(1), class = sigma)\n    ),\n    file = \"models/english_collin_m1.brm\"\n  )\n\n\nenglish_collin_m1\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: RTlexdec ~ 1 + WrittenFrequency_c + Familiarity_c + SubjectYoung \n##    Data: english_25 (Number of observations: 25) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept              6.54      0.02     6.51     6.57 1.00     3039     2410\n## WrittenFrequency_c    -0.02      0.02    -0.06     0.02 1.00     2280     2204\n## Familiarity_c         -0.05      0.03    -0.10     0.01 1.00     2306     2337\n## SubjectYoung          -0.21      0.04    -0.28    -0.14 1.00     2548     2396\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.08      0.01     0.06     0.12 1.00     2720     2686\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nVisualize joint posterior for \\(\\beta_1\\) and \\(\\beta_3\\):\n\n\nCode\n# heatmap of posterior for two coefficients, with lines at beta_1, beta_3 = 0\nenglish_collin_m1 %&gt;%\n  spread_draws(b_WrittenFrequency_c, b_Familiarity_c) %&gt;%\n  ggplot(aes(x = b_WrittenFrequency_c, y = b_Familiarity_c)) +\n  geom_hex() +\n  xlab(\"beta_1: WrittenFrequency_c\") +\n  ylab(\"beta_3: Familiarity_c\") +\n  geom_vline(aes(xintercept = 0), lty = 2) +\n  geom_hline(aes(yintercept = 0), lty = 2)\n\n\n\n\n\n\n\n\n\nVisualize marginal posterior for these two coefficients:\n\n\nCode\n# dotted line: 0\nmcmc_plot(english_collin_m1, type = \"hist\", variable = c(\"b_Familiarity_c\", \"b_WrittenFrequency_c\")) + geom_vline(aes(xintercept = 0), lty = 2)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe fact that these overlap 0 corresponds to the overlapping 95% CredI for WrittenFrequency_c and Familiarity_c in the model table above.\n\n\n\n\nOptional exercises for this example are in Section 3.8.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Regression Models 1</span>"
    ]
  },
  {
    "objectID": "week4.html#multiple-regression-for-glms",
    "href": "week4.html#multiple-regression-for-glms",
    "title": "3  Bayesian Regression Models 1",
    "section": "3.4 Multiple regression for GLMs",
    "text": "3.4 Multiple regression for GLMs\nGLMs work analogously to linear regression.\n\n3.4.1 Example: logistic regression\nRefit the model of the diatones data from Sonderegger (2023) Sec. 6.7.2.1 (see empirical plots in Sec. 6.1.1 of the book):\n\ndiatones_m41 &lt;- brm(\n  data = diatones,\n  stress_shifted | trials(1) ~ syll2_coda + syll2_td + frequency + syll1_coda,\n  family = binomial,\n  prior = c(\n    prior(normal(0, 5), class = Intercept),\n    prior(normal(0, 3), class = b)\n  ),\n  file = \"models/diatones_m41.brm\"\n)\n\n\ndiatones_m41\n##  Family: binomial \n##   Links: mu = logit \n## Formula: stress_shifted | trials(1) ~ syll2_coda + syll2_td + frequency + syll1_coda \n##    Data: diatones (Number of observations: 130) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     -1.91      0.29    -2.50    -1.37 1.00     3623     2787\n## syll2_coda    -1.30      0.64    -2.58    -0.08 1.00     3259     2980\n## syll2_td       0.81      0.59    -0.34     1.97 1.00     4166     3368\n## frequency     -0.61      0.49    -1.56     0.35 1.00     4277     3102\n## syll1_coda     1.23      0.55     0.15     2.31 1.00     3876     2840\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe coefficients look very similar to the analagous frequentist model:\n\nmlogreg_mod_1 &lt;- glm(\n  stress_shifted ~ syll2_coda + syll2_td +\n    frequency + syll1_coda,\n  data = diatones, family = \"binomial\"\n)\n\nkable(tidy(mlogreg_mod_1, conf.int = TRUE), format = \"markdown\", digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-1.83\n0.28\n-6.63\n0.00\n-2.43\n-1.33\n\n\nsyll2_coda\n-1.31\n0.64\n-2.06\n0.04\n-2.62\n-0.11\n\n\nsyll2_td\n0.84\n0.58\n1.46\n0.15\n-0.29\n2.00\n\n\nfrequency\n-0.61\n0.49\n-1.24\n0.22\n-1.58\n0.37\n\n\nsyll1_coda\n1.24\n0.55\n2.25\n0.02\n0.15\n2.34\n\n\n\n\n\n\nExercise 3.6 Fit the Poisson regression model gestures ~ context + language from Section 2.6.3, but now using brm(). You will need to first determine appropriate “uninformative” priors for \\(\\beta_1\\) and \\(\\beta_2\\), by thinking about what a very large change in log(gestures) would be for this data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Regression Models 1</span>"
    ]
  },
  {
    "objectID": "week4.html#sec-brm1-priors",
    "href": "week4.html#sec-brm1-priors",
    "title": "3  Bayesian Regression Models 1",
    "section": "3.5 Priors",
    "text": "3.5 Priors\nChoosing priors is one of the trickier parts of fitting Bayesian models, at first, and completely new relative to frequentist models.\nDifferent kinds of priors, and motivations for using them, are important to know about. Unfortunately there is no standard terminology; we’ll use common terms, but you may come upon new ones.\nPriors differ in their informativity: how much information about likely values of parameter \\(\\beta\\) (“prior knowledge”) they bring to the model. These roughly fall on a continuum from less to more informative:\n\nUninformative\n\nFlat/uninformative : e.g. \\(\\text{Uniform}(p)\\). Often these are improper (not actually probability distributions).\nvague but proper: as flat as possible while still being a probability distribution, like \\(\\beta \\sim N(0, 100000)\\). (example below: “weak” prior)\n\nWeakly informative:\n\nencode some information about possible datasets, weakly (as in the example in the next section)\n\nInformative / subjective\n\nContains a lot of prior knowledge, from previous work, or theory.\nExample: \\(\\beta \\sim N(3.2, 5)\\), where 3.2 and 5 are from a meta-analysis of existing studies.\n\n\nThis is a continuum. Also important is the notion of a regularizing prior: one that downweights extreme values (e.g. \\(\\beta\\) far from 0, or \\(p\\) close to 0 or 1).\nIt used to be common (~10 years ago?) in Bayesian data analysis  to default to flat priors, or a related class (“invariant”, “vague”, “uninformative”). This feels like the most objective option, and can be a reasonable default—especially when comparing with frequentist methods, which can usually be cast as approximating a Bayesian method with a flat prior. However, there are important issues with flat priors (e.g. Lemoine 2019).\nFirst, flat priors are often not “uninformative”: for example, \\(p \\sim N(0, 100)\\) for a binomial model actually gives high prior probability to \\(p\\) near 0 and 1 (see McElreath 2020, sec. 11.1). It is in fact non-trivial to decide what “uninformative” means.\nSecond, flat priors minimize the advantages of using Bayesian methods, especially regularization, which encodes knowledge about what models are very implausible (e.g., mean reaction time can’t be 10 seconds). In short, if flat priors are similar to using a frequentist method, we’ll have the same disadvantages as a frequentist method.\nIn current practice, most sources  recommend two principles for prior choice:\n\nReduce instability / run time of model fitting by using regularizing priors.\nRule out completely unreasonable parameter values, while allowing others (even unexpected ones) to be learned from enough data, by using weakly informative priors.\n\nIn current “real-world” applications, it’s common to default to priors at the intersection of (1) and (2): weakly informative regularizing priors.\nConfusingly, such priors are used so often that it has become common to use either “weakly informative” or “regularizing” to just mean both.\nMcElreath in particular typically uses weakly informative regularizing priors—though they are often just called “weakly informative”, and he doesn’t use either term until later chapters. Here is one definition:\n\n“what we mean [by weakly informative] is that, if there’s a reasonably large amount of data, the likelihood will dominate, and the prior will not be important. If the data are weak, though, this”weakly informative prior” will strongly influence the posterior inference. The phrase “weakly informative” is implicitly in comparison to a default flat prior.” - Stan prior choice recommendations, cites Gabry et al. (2019).\n\nLemoine (2019) discusses weakly informative priors, their advantages (including regularizing), and how to choose them. The use of weakly informative priors is realted to prior predictive checks, using the prior predictive distribution, which we’ll exemplify below.10\n\n\n\n\n\n\nBroader context: using informative priors\n\n\n\n\n\nThe fact that weakly informative regularizing priors are a good default doesn’t mean that (strongly) informative priors or non-regularizing priors are bad, in the right setting. They can capture what’s known from prior work, or adjudicating between different priors can in fact be the focus of a study. In more complex nonlinear models, strong priors can be necessary for models to fit at all. See XX for more discussion.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Regression Models 1</span>"
    ]
  },
  {
    "objectID": "week4.html#prior-predictive-checks",
    "href": "week4.html#prior-predictive-checks",
    "title": "3  Bayesian Regression Models 1",
    "section": "3.6 Prior predictive checks",
    "text": "3.6 Prior predictive checks\n\n\n\n\n\n\nPractical note\n\n\n\n\n\nMcElreath and other current sources (like Nicenboim, Schad, and Vaishth (2024)) recommend that prior predictive simulation be part of the standard Bayesian data analysis workflow. In reality, this is not (currenlty) common in Bayesian analysis of linguistic data. So while prior predictive simulation is important to learn about and recommended for your data analysis, it’s important to not let it intimidate you. You don’t need to do a full PPS every time you run a Bayesian linear regression.\n\n\n\n\n3.6.1 Example 1\nAs an example of prior predictive simulation, let’s sample from the prior alone for model english_m43, using the prior predictive distribution. To do this, we need to change our brm() call to:\n\nUse sample_prior = 'only'\nIncrease number of samples (2000 \\(\\rightarrow\\) 20000), since sampling from the prior can take longer, because it is much more diffuse than the posterior.\n\n\nenglish_m43_prior &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian, sample_prior = \"only\",\n    iter = 20000,\n    prior = c(\n      prior(normal(0, 10), class = Intercept),\n      prior(normal(0, 5), class = b),\n      prior(exponential(1), class = sigma)\n    ),\n    file = \"models/english_m43_prior.brm.rds\"\n  )\n\nExamine partial effect of WrittenFrequency_c:\n\n\nCode\nenglish_250 %&gt;%\n  group_by(SubjectYoung) %&gt;%\n  data_grid(WrittenFrequency_c = seq_range(WrittenFrequency_c, n = 101), SubjectYoung = 0) %&gt;%\n  add_epred_draws(english_m43_prior, ndraws = 100) %&gt;%\n  ggplot(aes(x = WrittenFrequency_c, y = RTlexdec)) +\n  geom_line(aes(y = .epred, group = .draw), alpha = .1)\n\n\n\n\n\n\n\n\n\nNote our use of data_grid() here to set up a dataframe of predictor values, now for the case of varying one predictor (WrittenFrequency) while holding the other constant (AgeSubject).\nThese lines show what the model thinks are possible WrittenFrequency_c effects before seeing any data—its state of prior knowledge. Domain knowledge tells us these are crazy predictions. Just pressing a button takes humans at least ~175 msec, so 150 msec is a reasonable minimum value for a predicted RT; this is RTlexdec=5 (log(150)=5.01). Decisions longer than a couple seconds mean something is wrong; let’s take RTledec = 8 as the maximum value for a predicted RT (= 3 seconds). Replotting the graph above with these bounds:\n\n\nCode\nenglish_250 %&gt;%\n  group_by(SubjectYoung) %&gt;%\n  data_grid(WrittenFrequency_c = seq_range(WrittenFrequency_c, n = 101), SubjectYoung = 0) %&gt;%\n  add_epred_draws(english_m43_prior, ndraws = 100) %&gt;%\n  ggplot(aes(x = WrittenFrequency_c, y = RTlexdec)) +\n  geom_line(aes(y = .epred, group = .draw), alpha = .5) +\n  geom_hline(aes(yintercept = 5), lty = 2, color = \"blue\") +\n  geom_hline(aes(yintercept = 8), lty = 2, color = \"blue\")\n\n\n\n\n\n\n\n\n\nHow can we do better? As motivated when we fit the original model:\n\nReasonable values of RTlexdec are in \\([5,8]\\), so we could choose a prior for the intercept which has most probability in this range—a normal distribution such that these are the +- 3\\(\\sigma\\) bounds:\n\n\\(\\alpha \\sim N(6.5, 0.5)\\)\n\nA slope for WrittenFrequency_c of -0.1 would be huge; for SubjectYoung a slope of -2 would be huge. We could let these values be 2 \\(\\sigma\\) from the prior mean for \\(\\beta_1\\) and \\(\\beta_2\\). In addition, we would be very surprised if either effect were not negative (younger speakers and more frequent words have shorter RT), so let’s make the prior means negative.\n\n\\(\\beta_1 \\sim N(-0.05, 0.025)\\)\n\\(\\beta_2 \\sim N(-0.5, 0.75)\\)\n\n\n\nenglish_m45_prior &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian, sample_prior = \"only\",\n    iter = 20000,\n    prior = c(\n      prior(normal(6.5, 0.5), class = Intercept),\n      prior(normal(-0.05, 0.025 ), class = b, coef = WrittenFrequency_c),\n      prior(normal(-0.5, 0.75), class = b, coef = SubjectYoung),\n      prior(exponential(1), class = sigma)\n    ),\n    file = \"models/english_m45_prior.brm\"\n  )\n\n\n\nCode\nenglish_250 %&gt;%\n  data_grid(WrittenFrequency_c = seq_range(WrittenFrequency_c, n = 101), SubjectYoung = 0) %&gt;%\n  add_epred_draws(english_m45_prior, ndraws = 100) %&gt;%\n  ggplot(aes(x = WrittenFrequency_c, y = RTlexdec)) +\n  geom_line(aes(y = .epred, group = .draw), alpha = .5) +\n  geom_hline(aes(yintercept = 5), lty = 2, color = \"blue\") +\n  geom_hline(aes(yintercept = 8), lty = 2, color = \"blue\")\n\n\n\n\n\n\n\n\n\nThis seems more reasonable; we might even want to make the (slope?) priors a bit less informative. We could make other predictive plots, like of the SubjectYoung partial effect, or predictions as a function of both SubjectYoung and WrittenFrequency_c (= two predicted lines per draw from the prior).\nCheck if this prior makes any difference to the fitted model (i.e. the posterior):\n\nenglish_m45 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n      prior(normal(6.5, 0.5), class = Intercept),\n      prior(normal(-0.05, 0.025), class = b, coef = WrittenFrequency_c),\n      prior(normal(-0.5, 0.25), class = b, coef = SubjectYoung),\n      prior(exponential(1), class = sigma)\n    ),\n    file = \"models/english_m45.brm\"\n  )\n\nThe posterior looks very similar to the model with weaker priors (english_m43), as we can see by applying posterior_summary() or mcmc_plot() to each model.\n\nmcmc_plot(english_m43, type = \"hist\") +\n  ggtitle(\"Posterior distribution: weaker priors\")\nmcmc_plot(english_m45, type = \"hist\") +\n  ggtitle(\"Posterior distribution: stronger priors\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is because \\(n=250\\) observations was enough for the likelihood (the data) to overwhelm the prior.\n(Note: what our figures call “weaker” and “stronger” priors correspond to “flat”/“uninformative” and “weakly informative” in the taxonomy above.)\nWe’d expect the prior to make more of a difference for a smaller dataset. Let’s fit models with the same two priors to english_25, which has \\(n=25\\) observations:\n\nenglish_m46 &lt;-\n  brm(\n    data = english_25,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 100), class = Intercept),\n      prior(normal(0, 5), class = b),\n      prior(exponential(1), class = sigma)\n    ),\n    file = \"models/english_m46.brm\"\n  )\n\nenglish_m47 &lt;-\n  brm(\n    data = english_25,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n      prior(normal(6.5, 0.5), class = Intercept),\n      prior(normal(-0.05, 0.025), class = b, coef = WrittenFrequency_c),\n      prior(normal(-0.5, 0.75), class = b, coef = SubjectYoung),\n      prior(exponential(1), class = sigma)\n    ),\n    file = \"models/english_m47.brm\"\n  )\n\nThe posteriors are slightly narrower for the model with stronger priors:\n\n## ignore the lp___ lines\nposterior_summary(english_m46) \n##                          Estimate  Est.Error         Q2.5        Q97.5\n## b_Intercept            6.53836217 0.01845433   6.50196687   6.57404141\n## b_WrittenFrequency_c  -0.04901830 0.01128223  -0.07106459  -0.02665495\n## b_SubjectYoung        -0.23340083 0.03792993  -0.30868792  -0.15901430\n## sigma                  0.08945753 0.01426544   0.06557531   0.12077997\n## Intercept              6.55236622 0.01831929   6.51681381   6.58805880\n## lprior               -10.67363471 0.01427671 -10.70526677 -10.64972072\n## lp__                  12.52561066 1.51741568   8.75156583  14.46203528\nposterior_summary(english_m47)\n##                         Estimate  Est.Error        Q2.5       Q97.5\n## b_Intercept           6.53848390 0.01768429  6.50431144  6.57348920\n## b_WrittenFrequency_c -0.04904812 0.01043104 -0.06942174 -0.02800759\n## b_SubjectYoung       -0.23346989 0.03568194 -0.30595350 -0.16397522\n## sigma                 0.08844702 0.01394459  0.06615899  0.12049948\n## Intercept             6.55249210 0.01756967  6.51777487  6.58716219\n## lprior                1.66629299 0.13806210  1.30950223  1.77944045\n## lp__                 25.05601074 1.46951405 21.26935084 26.89074865\n\nA more informative prior will matter more when:\n\nThere is less data, relative to model complexity.\nOther settings where the inference problem is “more complex”, like when predictors are correlated.\n\n\n\n3.6.2 Example 2\nA good example is our multicollinearity example from above: english_collin_m1 could not tease apart the effect of word frequency and familiarity, but was confident that some combination of them (negatively) affects RT. Let’s re-fit this model with our strong prior (setting the Familiarity_c \\(\\beta\\) prior to the same value as the WrittenFrequency_c prior, as the variables have similar ranges):\n\nenglish_collin_m2 &lt;-\n  brm(\n    data = english_25,\n    RTlexdec ~ 1 + WrittenFrequency_c + Familiarity_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n      prior(normal(6.5, 0.5), class = Intercept),\n      prior(normal(-0.05, 0.025), class = b, coef = WrittenFrequency_c),\n      prior(normal(-0.05, 0.025), class = b, coef = Familiarity_c),\n      prior(normal(-0.5, 0.25), class = b, coef = SubjectYoung),\n      prior(exponential(1), class = sigma)\n    ),\n    file = \"models/english_collin_m2.brm\"\n  )\n\nThe heatmap of the two key coefficients is now:\n\n# heatmap of posterior for two coefficients, with lines at beta_1, beta_3 = 0\nenglish_collin_m2 %&gt;%\n  spread_draws(b_WrittenFrequency_c, b_Familiarity_c) %&gt;%\n  ggplot(aes(x = b_WrittenFrequency_c, y = b_Familiarity_c)) +\n  geom_hex() +\n  xlab(\"beta_1: WrittenFrequency_c\") +\n  ylab(\"beta_3: Familiarity_c\") +\n  geom_vline(aes(xintercept = 0), lty = 2) +\n  geom_hline(aes(yintercept = 0), lty = 2)\n\n\n\n\n\n\n\n\nPlot the coefficient estimates for each model (except the intercept, for scale):\n\nmcmc_plot(english_collin_m1, variable = c(\"b_Familiarity_c\", \"b_WrittenFrequency_c\", \"sigma\")) + ggtitle(\"With weak prior\")\nmcmc_plot(english_collin_m2, variable = c(\"b_Familiarity_c\", \"b_WrittenFrequency_c\", \"sigma\")) + ggtitle(\"With strong prior\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe 95% CredI for both the frequency and familiarity coefficients now do not overlap zero. Because the model had stronger prior knowledge about likely values of these coefficients, it was better able to tease the effects of frequency and familiarity apart.\n\nExercise 3.7 Do a prior predictive check for model english_m45_prior for the partial effect of AgeSubject. Interpret the resulting plot.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Regression Models 1</span>"
    ]
  },
  {
    "objectID": "week4.html#effect-size-and-significance",
    "href": "week4.html#effect-size-and-significance",
    "title": "3  Bayesian Regression Models 1",
    "section": "3.7 Effect size and significance",
    "text": "3.7 Effect size and significance\nTo test a point hypothesis for a Bayesian model (or an interval hypothesis: \\(\\beta\\) is v. small), different methods measure effect existence of parameter \\(\\beta\\) (is it zero?) and significance (or size)—the latter meaning “important enough to care about”. Note that “significance” here is different from frequentist “significance” from \\(p\\)-values.\nMakowski, Ben-Shachar, and Lüdecke (2019) discuss four kinds of indices, assuming frequentist \\(p\\)-values as a starting point (their Table 3 summarizes):\n\nprobability of direction (\\(pd\\))\nMAP probability (we won’t consider this)\nBayes factor (BF)\n\nMust define a null model:\nPoint-null (most common)\nVersus the ROPE\n\nROPE (region of practical equivalence)\n\nPercentage of 95% (or 89%) CI for \\(\\beta\\) that contains ROPE\nPercentage of whole posterior for \\(\\beta\\) that contains ROPE (most common?)\n\n\n\n\n\n\n\n(From Makowski, Ben-Shachar, and Lüdecke (2019)) “Bayesian indices of effect existence and significance. (A) The probability of Direction (pd) is defined as the proportion of the posterior distribution that is of the median’s sign (the size of the yellow area relative to the whole distribution). (B) The MAP-based p-value is defined as the density value at 0 – the height of the red lollipop, divided by the density at the Maximum A Posteriori (MAP) – the height of the blue lollipop. (C) The percentage in ROPE corresponds to the red area relative to the distribution [with or without tails for ROPE (full) and ROPE (95%), respectively]. (D) The Bayes factor (vs. 0) corresponds to the point-null density of the prior (the blue lollipop on the dotted distribution) divided by that of the posterior (the red lollipop on the yellow distribution), and the Bayes factor (vs. ROPE) is calculated as the odds of the prior falling within vs. outside the ROPE (the blue area on the dotted distribution) divided by that of the posterior (the red area on the yellow distribution).”\n\n\n\n\n\nFor futher understanding, the bayestestR vignettes are helpful, e.g. on probability of direction or Bayes factors.\n\\(pd\\) and MAP probability measure existence, and don’t provide useful info about evidence for the null. BF and ROPE methods measure significance (mostly), and can be used for evidence for the null.\nWe will show examples with functions from bayestestR.\n\n3.7.1 Indices of existence\nMost commonly reported is \\(p_d\\), which is conceptually similar to a \\(p\\)-value (but subtracted from 1).\nCalculate \\(pd\\) for the two multicollinearity models of english_25 from above:\n\np_direction(english_collin_m1)\n## Probability of Direction\n## \n## Parameter          |     pd\n## ---------------------------\n## (Intercept)        |   100%\n## WrittenFrequency_c | 83.03%\n## Familiarity_c      | 95.58%\n## SubjectYoung       |   100%\np_direction(english_collin_m2)\n## Probability of Direction\n## \n## Parameter          |     pd\n## ---------------------------\n## (Intercept)        |   100%\n## WrittenFrequency_c | 98.22%\n## Familiarity_c      | 99.12%\n## SubjectYoung       |   100%\n\nThis is what we’d expect from examining the posteriors/CIs—similar interpretation to \\(p\\)-values. With a stronger prior, there is more evidence for the existence of both WrittenFrequency_c and Familiarity_c effects.\nFor the diatones_m41 model:\n\np_direction(diatones_m41)\n## Probability of Direction\n## \n## Parameter   |     pd\n## --------------------\n## (Intercept) |   100%\n## syll2_coda  | 98.05%\n## syll2_td    | 91.50%\n## frequency   | 89.62%\n## syll1_coda  | 98.65%\n\nThere is progressively more evidence for the existence of the frequency, syll2_td, syll2_coda, and syll1_coda effects. If we impose a cutoff analagous to \\(p &lt; 0.05\\), we’d say that only the syll1_coda and syll2_coda effects “exist”.\n\n\n3.7.2 Indices of significance: ROPE\nTo use a ROPE method (including BF-ROPE), we must define a region of values of the parameter of interest (e.g. \\(\\beta_1\\)) that we consider practically equivalent to 0. Kruschke (2018) suggests -0.1/0.1 and -0.18/0.18 as defaults for linear and logistic regression, on a standardized parameter scale. This means a change of 0.1 SD (for a linear regression) in \\(y\\) is considered the same as 0, for a standardized predictor (it’s been \\(z\\)-scored). For example, for RTlexdec for english_25, 0.1 SD of \\(y\\) is:\n\nsd(english_25$RTlexdec) * 0.1\n## [1] 0.01550091\n\nSo the ROPE for \\(\\beta_1\\) is \\([-0.016, 0.016]\\). This -0.1/0.1 interval is based on Cohen’s “negligible” effect size, and should be changed based on domain knowledge when available (see e.g. here). You also need to adjust the ROPE when the predictor is not standardized (so that a change of 1 SD in \\(x\\) corresponds to a change of 0.1 SD in \\(y\\)).\nThe ROPE is not appropriate when predictors are highly correlated (see here), so we don’t apply it to our collinearity examples.\n\n3.7.2.1 Example\nLet’s instead calculate \\(p_{ROPE}\\) for the diatones_m41 model, assuming the default is OK (-0.18/0.18, as this is a logistic regression):11\n\nrope(diatones_m41, ci = 1)\n## # Proportion of samples inside the ROPE [-0.18, 0.18]:\n## \n## Parameter  | inside ROPE\n## ------------------------\n## Intercept  |      0.00 %\n## syll2_coda |      2.67 %\n## syll2_td   |      9.10 %\n## frequency  |     14.10 %\n## syll1_coda |      2.33 %\n\nFor each coefficient, \\(p_{ROPE}\\) is the percentage of the posterior that lies inside the ROPE. These values are fairly similar to \\(pd\\) (e.g. compare 1-0.26 to 98.28% for syll2_coda), with some differences.\nTo see the differences in interpretation better, let’s consider \\(p_{ROPE}\\) for the 95%HDI: the percentage of the 95% HDI of each parameter that lies inside the ROPE.\n\nrope(diatones_m41, ci = 0.95)\n## # Proportion of samples inside the ROPE [-0.18, 0.18]:\n## \n## Parameter  | inside ROPE\n## ------------------------\n## Intercept  |      0.00 %\n## syll2_coda |      1.08 %\n## syll2_td   |      9.58 %\n## frequency  |     14.84 %\n## syll1_coda |      0.26 %\n\nNote that syll1_coda now has \\(p_{rope} &lt; 0.005\\), but \\(pd\\) around 0.99. The model is very certain that the effect is important, and slightly less sure of its direction/existence. This is weird, but possible. More common is a situation where \\(pd\\) indicates strong evidence but \\(p_{rope}\\) does not—the model is sure the effect exists, but not that it is large enough to be meaningful.\n\n\n\n3.7.3 Indices of significance: Bayes Factors\nBayes factors are both widely used and controversial. The central issues are sensitivity to the prior, and proper interpretation. Schad et al. (2023) (= Nicenboim, Schad, and Vaishth (2024) Chap. 16 is a nice but very detailed discussion. This page is shorter, with references. These sources make clear that Bayes Factors are not a method where you should just use a package’s defaults without thinking. We’ll show a couple examples.\nBayes factors can be used to compare any two models. In practice they are often used to assess evidence in favor of each effect in a model, either using: - A null interval (\\(\\beta\\) is in the ROPE) - A point-null hypothesis (\\(\\beta = 0\\)).\nThe point-null version is most common. In either case, the interpretation is “how much more credible has the absence of an effect become, given the observed data?”\nThere are different scales for interpreting a BF; see “Interpretation” on the Wikipedia page. Most common is Jeffrey’s scale:\n\nvalues of 1–3.16 are evidence “barely worth mentioning” in favor of the model; thus, values of 0.31–1 are evidence “barely worth mentioning” in favor of the null model.\nvalues of 3.16–10 are “substantial” evidence for the model.\nvalues of 10–31: “strong” evidence\n\nAs a first example, the ROPE-based BF for our diatones_m31 model, with very weak priors, is:\n\nbf_rope(diatones_m41)\n## Sampling priors, please wait...\n## Bayes Factor (Null-Interval)\n## \n## Parameter   |       BF\n## ----------------------\n## (Intercept) | 9.53e+04\n## syll2_coda  |     1.80\n## syll2_td    |    0.476\n## frequency   |    0.292\n## syll1_coda  |     2.52\n## \n## * Evidence Against The Null: [-0.181, 0.181]\n\nThus, we would say there is:\n\n“Barely” evidence for effects of syll2_coda and syll1_coda\nLittle evidence either for or against the effects of syll2_td\nWeak evidence against an effect of frequency.\n\nA nice property of Bayes Factors is that they allow us to assess evidence for or against an effect, with sufficiently a sufficiently small BF providing evidence “for the null”. This is an important contrast to frequentist \\(p\\)-values, which only allow us to conclude “significant” (effect “exists”) versus “not significant” (effect could exist or not). High \\(p\\)-values are frequently misinterpreted as providing evdience “for the null”.12\n\n\n\n\n\n\nPractical note\n\n\n\n\n\nIt is common when interpreting Bayes Factors to use a scale like the one given above to reach a decision—most commonly that an effect “exists” if BF &gt; 3 or 10, “doesn’t exist” if BF &lt; 1/3 or 1/10, and “we can’t say” if 1/3 &lt; BF &lt; 3 or 1/10 &lt; BF &lt; 10. Schad et al. (2023) argues this is not a good idea, in the absence of a utility function that tells us the consequences of correct and incorrect decisions:\n\n“In the cognitive sciences, because it is often unclear how to define good utility functions, we argue that Bayesian decision making is premature: Inferences based on continuous Bayes factors should be reported instead of decisions.”\n\n(They then show a complicated procedure by which you could still define a decision rule, given your data/model/priors.)\nI think the upshot is similar to other applications of Bayesian methods: we should ideally report continuous measures summarizing our models (like \\(pd\\), a Bayes Factor, a 95% CredI) and try to resist applying discrete cutoffs in interpretation. So for example, while we probably shouldn’t say for the diatones_m41 model that there is “no effect” of frequency, we could say there is less evidence for a frequency effect than for a syll1_coda effect (smaller BF).\n\n\n\n\nAs examples of computing point-null BFs, we’ll consider the two english models fit above with weaker priors, with \\(n=250\\) and \\(n=25\\) observations: these are english_m43 and english_m46 above.\nCalculating point-null BFs uses “bridge sampling”, which requires a much larger number of posterior samples than usual to give a precise answer. It is recommended to use at least 40k posterior samples, or as many as needed to give the same result when you calculate Bayes Factors several times. (I have used 80k posterior samples below because 40k wasn’t enough for this criterion.)\nSo we first refit the models with more samples:\n\n## Note: 40k samples per chain x 4 chains, \n## of which 50% are discarded as warm-up = \n## 80k samples total\nenglish_m43_1 &lt;- brm(\n    data = english_250,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n        prior(normal(0, 100), class = Intercept),\n        prior(normal(0, 5), class = b),\n        prior(exponential(1), class = sigma)\n    ), chains = 4, iter = 40000, cores = 4,\n    file = \"models/english_m43_1.brm\"\n)\n\nenglish_m46_1 &lt;- brm(\n    data = english_25,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n        prior(normal(0, 100), class = Intercept),\n        prior(normal(0, 5), class = b),\n        prior(exponential(1), class = sigma)\n    ), chains = 4, iter = 40000, cores = 4,\n    file = \"models/english_m46_1.brm\"\n)\n\nenglish_m47_1 &lt;-\n  brm(\n    data = english_25,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n      prior(normal(6.5, 0.5), class = Intercept),\n      prior(normal(-0.05, 0.025), class = b, coef = WrittenFrequency_c),\n      prior(normal(-0.5, 0.75), class = b, coef = SubjectYoung),\n      prior(exponential(1), class = sigma)\n    ), chains = 4, iter = 40000, cores = 4,\n    file = \"models/english_m47_1.brm\"\n  )\n\nThen calculate point-null Bayes Factors:\n\nbf_pointnull(english_m43_1)\n## Sampling priors, please wait...\n## Bayes Factor (Savage-Dickey density ratio)\n## \n## Parameter          |       BF\n## -----------------------------\n## (Intercept)        |      Inf\n## WrittenFrequency_c | 3.76e+14\n## SubjectYoung       | 4.39e+25\n## \n## * Evidence Against The Null: 0\nbf_pointnull(english_m46_1) \n## Sampling priors, please wait...\n## Bayes Factor (Savage-Dickey density ratio)\n## \n## Parameter          |       BF\n## -----------------------------\n## (Intercept)        |      Inf\n## WrittenFrequency_c |     4.73\n## SubjectYoung       | 1.50e+03\n## \n## * Evidence Against The Null: 0\n\nCompare to \\(pd\\) for each model:\n\np_direction(english_m43_1)\n## Probability of Direction\n## \n## Parameter          |   pd\n## -------------------------\n## (Intercept)        | 100%\n## WrittenFrequency_c | 100%\n## SubjectYoung       | 100%\np_direction(english_m46_1)\n## Probability of Direction\n## \n## Parameter          |     pd\n## ---------------------------\n## (Intercept)        |   100%\n## WrittenFrequency_c | 99.99%\n## SubjectYoung       |   100%\n\nIn both cases, the model is very sure the effect exists. But the model fitted to just 25 observations gives only moderate evidence for the WrittenFrequency_c effect’s significance versus the null, while the model fitted to 250 observations gives very strong evidence. This is counterintuitive at first, but follows from how \\(p_d\\) (and \\(p\\)-values) are defined:\n\n\\(pd\\): the observed data is unlikely if \\(\\beta_1=0\\) (for \\(n=25\\))\nBF: the observed data is about as likely if \\(\\beta_1=0\\) (under this prior) as when \\(\\beta_1 \\ne 0\\)\n\nWe see another interesting pattern comparing \\(pd\\) and BF for the models fit to the english_25 data, with the weaker versus stronger priors:\n\np_direction(english_m46_1)\n## Probability of Direction\n## \n## Parameter          |     pd\n## ---------------------------\n## (Intercept)        |   100%\n## WrittenFrequency_c | 99.99%\n## SubjectYoung       |   100%\np_direction(english_m47_1)\n## Probability of Direction\n## \n## Parameter          |      pd\n## ----------------------------\n## (Intercept)        |    100%\n## WrittenFrequency_c | 100.00%\n## SubjectYoung       |    100%\n\n\nbf_pointnull(english_m46_1)\n## Sampling priors, please wait...\n## Bayes Factor (Savage-Dickey density ratio)\n## \n## Parameter          |       BF\n## -----------------------------\n## (Intercept)        |      Inf\n## WrittenFrequency_c |     4.77\n## SubjectYoung       | 1.46e+03\n## \n## * Evidence Against The Null: 0\nbf_pointnull(english_m47_1)\n## Sampling priors, please wait...\n## Bayes Factor (Savage-Dickey density ratio)\n## \n## Parameter          |       BF\n## -----------------------------\n## (Intercept)        |      Inf\n## WrittenFrequency_c | 1.17e+03\n## SubjectYoung       | 8.39e+03\n## \n## * Evidence Against The Null: 0\n\nAgain, the \\(pd\\) values are the same: the data is unlikely if \\(\\beta_0\\) in each case, for the WrittenFrequency_c and SubjectYoung parameters. The prior does not matter for this calculation.\nBut the BF values are much higher for the model with the strong prior. This is because the BF considers how likely the data is relative to the prior. Under the “strong” prior, \\(\\beta\\) values of 0 for WrittenFrequency_c and SubjectYoung are unlikely—we defined the prior to capture that these coefficients are probably negative. So the model with a strong prior thinks there is strong evidence for both effects, relative to the prior; the model with a weak prior thinks there is much weaker evidence.\n\nExercise 3.8  \n\nFor the dyads model fitted in Exercise 3.6: assume the default ROPE, and calculate \\(pd\\) and either \\(p_{rope}\\) or BF (point-null). What do these indices say about the (two) effects of interest?\nWrite up these results in a paragraph, following the model from Makowski, Ben-Shachar, and Lüdecke (2019).\n\n\n\n\n3.7.4 Recommendations\nSince they give different information, it seems like a good idea to use one index of each type (existence, significance) when reporting results, if you are summarizing effects using indices.\nCommon practice (for linguistic data) is currently to report in the regression table, for each effect (one row):\n\nEstimate\nError\n95% CredI\n\\(pd\\)\n\nwhich basically replicates a frequentist regression model table (replacing \\(p\\) by \\(pd\\)). This doesn’t give any information about importance/significance of effects, and leaves deciding what an “important” value is up to the reader. A better option would be to include a column including BF or \\(p_{ROPE}\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Regression Models 1</span>"
    ]
  },
  {
    "objectID": "week4.html#sec-brm1-extra",
    "href": "week4.html#sec-brm1-extra",
    "title": "3  Bayesian Regression Models 1",
    "section": "3.8 Extra",
    "text": "3.8 Extra\n\nExercise 3.9  \n\n\nHow does multicollinearity play out in the example in Section 3.3.3, in the coefficients for WrittenFrequency_c and Familiarity_c?\nExamine the heatmap of the posterior for \\(\\beta_1\\) and \\(\\beta_3\\) (the coefficients for those two predictors) and the hypothesis tests above. How do you interpret what the posterior says about how a word’s written frequency and familiarity affect reaction time? (Hint: use the samples from the posterior to test the hypothesis “\\(\\beta_1 &lt; 0\\) OR \\(\\beta_3 &lt; 0\\).” You can’t use hypothesis for this.)\n\n\n\n\n\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. Forthcoming. “How to Interpret Statistical Models Using marginaleffects in R and Python.” Journal of Statistical Software, Forthcoming. https://marginaleffects.com.\n\n\nGabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. “Visualization in Bayesian Workflow.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 182 (2): 389–402.\n\n\nKruschke, John K. 2018. “Rejecting or Accepting Parameter Values in Bayesian Estimation.” Advances in Methods and Practices in Psychological Science 1 (2): 270–80.\n\n\nKurz, S. 2023. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nLemoine, Nathan P. 2019. “Moving Beyond Noninformative Priors: Why and How to Choose Weakly Informative Priors in Bayesian Analyses.” Oikos 128 (7): 912–28.\n\n\nLenth, Russell V. 2024. Emmeans: Estimated Marginal Means, Aka Least-Squares Means. https://CRAN.R-project.org/package=emmeans.\n\n\nMakowski, Dominique, Mattan S Ben-Shachar, and Daniel Lüdecke. 2019. “bayestestR: Describing Effects and Their Uncertainty, Existence and Significance Within the Bayesian Framework.” Journal of Open Source Software 4 (40): 1541.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nNicenboim, B., D. Schad, and S. Vaishth. 2024. An Introduction to Bayesian Data Analysis for Cognitive Science. https://vasishth.github.io/bayescogsci/book/. 5/2024 version.\n\n\nSchad, Daniel J, Bruno Nicenboim, Paul-Christian Bürkner, Michael Betancourt, and Shravan Vasishth. 2023. “Workflow Techniques for the Robust Use of Bayes Actors.” Psychological Methods 28 (6): 1404–26.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.\n\n\nWesner, Jeff S, and Justin PF Pomeranz. 2021. “Choosing Priors in Bayesian Ecological Models by Simulating from the Prior Predictive Distribution.” Ecosphere 12 (9): e03739.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Regression Models 1</span>"
    ]
  },
  {
    "objectID": "week4.html#footnotes",
    "href": "week4.html#footnotes",
    "title": "3  Bayesian Regression Models 1",
    "section": "",
    "text": "(1 + WrittenFrequency and WrittenFrequency are equivalent in the model formula, as the intercept is added automatically. We use 1 + WrittenFrequency for clarity.)↩︎\nI think that sampling is more effective for an exponential prior because it drops off more quickly as \\(\\sigma\\) increases; see a comparison here.↩︎\nStandardizing predictors is arguably more important for Bayesian models because they take longer to fit than equivalent frequentist models, and unnecessary collinearity increases model fitting time (how many MCMC samples must be taken, and how long each posterior update takes), especially once models get more complex. It could also be seen as less important, because a correctly-specified Bayesian model will always “converge”, even with highly-correlated predictors—it will just take longer to sample from the posterior.↩︎\nLook at the lines in the spaghetti plot above, and think about what \\(\\alpha\\) and \\(\\beta_1\\) mean for each line, graphically.↩︎\n\\(\\alpha\\) is now the value of RTlexdec when WrittenFrequency_c=0.↩︎\nThis equivalence only holds if we assume that all predictors affect \\(y\\) without any causal dependencies between them. In this case, the conterfactual plot for \\(x_1\\) is just “the effect on \\(y\\) as \\(x_1\\) is varied, holding all other predictors at 0”. This is the case we are assuming (at least for now, possibly throughout the course): independent causes. “Marginal effects” doesn’t make sense in a setting where some predictors influence each other.↩︎\nUse a parameter fitted by the model, not a partial effect plot.↩︎\nThese are like the polynomials McElreath (2020) Sec. 4.5.1. covers, but with each component defined to be orthogonal to previous components. It is almost never a good idea to use raw polynomials as predictors: \\(x\\), \\(x^2\\), \\(x^3\\)…↩︎\nThough we might take the 95% CI for the nonlinear terms (polyWrittenFrequency_c32, polyWrittenFrequency_c33) overlapping zero to indicate there is not strong evidence for a nonlinear effect.↩︎\nMany examples with prior predictive checks are given by Wesner and Pomeranz (2021) (useful to read with Lemoine), Nicenboim, Schad, and Vaishth (2024), and McElreath (2020).↩︎\nRunning p_rope(diatones_m41) doesn’t currently work (due to a bug?), but should give the same result as rope(diatones_m41, ci = 1), per ?p_rope.↩︎\nIt’s still possible to “test the null” in a frequentist framework, using equivalence testing.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Regression Models 1</span>"
    ]
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "4  Bayesian Regression Models 2",
    "section": "",
    "text": "4.1 Preliminaries\nLoad libraries we will need:\nlibrary(tidyverse)\nlibrary(brms)\n\nlibrary(broom) # for tidy model summaries\n\nlibrary(tidybayes)\nlibrary(bayestestR)\n\nlibrary(languageR) # for `english' dataset\nlibrary(arm)\n\nlibrary(bayesplot)\nlibrary(loo)\n\nlibrary(emmeans) # for working with multi-level factors\nWe’ll make greater use today of the bayesplot package, which we previously used for posterior predictive checks (Chapter 2). This package also has extensive functionality for plotting MCMC draws and plotting MCMC diagnostics (such as trace plots), as covered in useful vignettes and the bayesplot paper (Gabry et al. 2019).\nWe’ll start using the loo package Vehtari et al. (2024), which implements methods for calculating model quality criteria from Vehtari, Gelman, and Gabry (2017) that use “pointwise out-of-sample prediction accuracy”: LOO-CV and WAIC. See vignette.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Regression Models 2</span>"
    ]
  },
  {
    "objectID": "week5.html#preliminaries",
    "href": "week5.html#preliminaries",
    "title": "4  Bayesian Regression Models 2",
    "section": "",
    "text": "Practical note\n\n\n\n\n\nIf you have loaded rethinking, you need to detach it before using brms. See Kurz (2023) Sec. 4.3.1.\nI use the file argument when fitting brms models to make compiling this document easier (so the models don’t refit every time I compile). You may or may not want to do this for your own models. See file and file_refit arguments in ?brm.\n\n\n\n\n\n\n4.1.1 Datasets\nLoad the diatones dataset and perform some data cleaning and recoding (see Section 1.1, Section 3.1.1):\n\ndiatones &lt;- read.csv(\"https://osf.io/tqjm8/download\", stringsAsFactors = TRUE)\n\n# make numeric versions of all categorical predictors, while saving original versions\ndiatones &lt;- diatones %&gt;% mutate(\n  syll1_coda_orig = syll1_coda,\n  syll2_coda_orig = syll2_coda,\n  syll2_td_orig = syll2_td,\n  ## turns no/yes -&gt; 0/1\n  syll1_coda = ifelse(syll1_coda == \"no\", 0, 1),\n  ## turns '0'/'C'/'CC'/'CCC' -&gt; 0/1/2/3\n  syll2_coda = str_count(syll2_coda_orig, \"C\"),\n  syll2_td = ifelse(syll2_td == \"no\", 0, 1)\n)\n\n## standardize all predictors using arm::rescale\ndiatones &lt;- diatones %&gt;% mutate(\n  syll1_coda = rescale(syll1_coda_orig),\n  syll2_td = rescale(syll2_td_orig),\n  syll2_coda = rescale(syll2_coda),\n  frequency = rescale(frequency)\n)\n\nWe also will use the english dataset from languageR, defining \\(n=250\\) and \\(n = 25\\) subsets after centering variables we’ll use in models below—all as in previous chapters.\n\n# center predictors\nenglish &lt;- mutate(english,\n  WrittenFrequency_c = scale(WrittenFrequency, scale = FALSE),\n  Familiarity_c = scale(Familiarity, scale = FALSE),\n  SubjectYoung = as.numeric(AgeSubject) - 1.5\n)\n\n## set seed, so you'll get the same \"random\" sample\nset.seed(100)\nenglish_250 &lt;- english[sample(1:nrow(english), 250), ]\n\n## set seed, so you'll get the same \"random\" sample\nset.seed(10)\nenglish_25 &lt;- english[sample(1:nrow(english), 25), ]\n\nFinally, we load the french_cdi_24 dataset. It is described in Sec. 7.1.1. of RMLD (and in ?sec-hw1 of this e-book), where you can learn more if needed.\n\nfrench_cdi_24 &lt;- read.csv(file = \"https://osf.io/uhd2n/download\", stringsAsFactors = TRUE)\n\nPerform pre-processing described in Sec. 7.1.1. of RMLD:\n\nfrench_cdi_24 &lt;- filter(french_cdi_24, data_id == 140275) %&gt;%\n  filter(lexical_class != \"other\") %&gt;%\n  mutate(lexical_class = fct_relevel(lexical_class, \"function_words\", \"verbs\", \"adjectives\", \"nouns\")) %&gt;%\n  droplevels()\n\nThis restricts the data to a single child (140275), aged 24 months, and relevels lexical_class in the theoretically-expected order.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Regression Models 2</span>"
    ]
  },
  {
    "objectID": "week5.html#model-quality-metrics",
    "href": "week5.html#model-quality-metrics",
    "title": "4  Bayesian Regression Models 2",
    "section": "4.2 Model quality metrics",
    "text": "4.2 Model quality metrics\n{#sec-model-quality}\nSummarizing McElreath (2020) Sec. 7.4:\nThe ideal for model quality metrics is out-of-sample deviance. This can never be computed, so we approximate using leave-one-out-cross-validation. This is usually impractical to compute—it would require refitting the model \\(n\\) times, where \\(n\\) = number of observations. So we approximate, in one of two ways\n\nPSIS-LOO-CV (“Pareto-smoothed importance-sampling leave-one-out cross-validation”), a.k.a. PSIS\nImplemented as loo() in the loo package\nWAIC (“Widely-applicable information criterion”)\nImplemented as waic()\n\nIn principle, these are different approaches, based on cross-validation versus calculating an information criterion, analagous to AIC/BIC for frequentist models. In practice, both PSIS and WAIC measure the same thing (out-of-sample deviance), and the larger the dataset, the more similar they will be.\nIf they give different qualitative results (with no errors in WAIC/PSIS calculation) you should be circumspect.\n\n\n\n\n\n\nPractical note: PSIS or WAIC?\n\n\n\n\n\nWhich of PSIS or WAIC should actually be used for model comparison in a concrete case? McElreath notes that PSIS and WAIC may each be better for different model types (Sec. 7.4.3), but seems to recommend defaulting to PSIS, because it “has a distinct advantage in warning the user about when it is unreliable” via the \\(k\\)-values it computes for each observation. However, WAIC is faster to compute—much faster, for large datasets or complex models—and the current WAIC implementation in loo also reports when it’s probably unreliable (and recommends using PSIS instead).\nMy usual workflow for model comparison is:\n\nFirst use WAIC\nIf there are warnings, switch to PSIS (a.k.a. “loo”, in the loo package)\nIf there are warnings about Pareto \\(k\\) values being too large, follow the package’s recommendation to compute PSIS with moment matching instead (see ?loo_moment_match).\n\nMy understanding is that Options 1–3 are (usually) progressively more accurate and slower.\n\n\n\n\n4.2.1 Example: nonlinear effect of WrittenFrequency\nThis section assumes as background the introduction to non-linear effects of predictors in Sec. 7.5 of RMLD, especially Sec. 7.5.3, where a similar example for frequentist linear regression is given.\nThe empirical effect of frequency (WrittenFrequency) on reaction time (RTlexdec) for the english_250 data is:\n\n\nCode\nenglish_250 %&gt;% ggplot(aes(x = WrittenFrequency, y = RTlexdec)) +\n  geom_point() +\n  geom_smooth(aes(color = AgeSubject))\n## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIt’s not immediately clear here whether the effect is linear or non-linear, and what degree the non-linear effect would be.\nWe assess this by comparing models fit with AgeSubject and different effects of WrittenFrequency:\n\nLinear effect of WrittenFrequency (equivalent to english_m43 from Section 3.3)\nPolynomial effect of WrittenFrequency, degree=2 (quadratic)\nPolynomial effect of WrittenFrequency, degree=3 (cubic)\nPolynomial effect of WrittenFrequency, degree=4 (quartic)\n\nFit these models, using the same priors as in Section 3.3:\n\nprior_1 &lt;- c(\n  prior(normal(0, 100), class = Intercept),\n  prior(normal(0, 5), class = b),\n  prior(exponential(1), class = sigma)\n)\n\nenglish_m51 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian,\n    prior = prior_1,\n    file = \"models/english_m51.brm\"\n  )\n\nenglish_m52 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + poly(WrittenFrequency_c, 2) + SubjectYoung,\n    family = gaussian,\n    prior = prior_1,\n    file = \"models/english_m52.brm\"\n  )\n\n\nenglish_m53 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + poly(WrittenFrequency_c, 3) + SubjectYoung,\n    family = gaussian,\n    prior = prior_1,\n    file = \"models/english_m53.brm\"\n  )\n\nenglish_m54 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + poly(WrittenFrequency_c, 4) + SubjectYoung,\n    family = gaussian,\n    prior = prior_1,\n    file = \"models/english_m54.brm\"\n  )\n\nWhile the loo() and waic() functions can be applied to a fitted model to calculate PSIS or WAIC, the recommended workflow is instead to add them to the fitted model using add_criterion(). This saves the specified criterion to the model file (indicated by the file argument when you fit the model) so it is only calculated once per fitted model. This is useful because PSIS/WAIC take a lot of time to compute, especially for more complex models.\n\n## add both WAIC and LOO\nenglish_m51 &lt;- add_criterion(english_m51, c(\"waic\", \"loo\"))\nenglish_m52 &lt;- add_criterion(english_m52, c(\"waic\", \"loo\"))\nenglish_m53 &lt;- add_criterion(english_m53, c(\"waic\", \"loo\"))\nenglish_m54 &lt;- add_criterion(english_m54, c(\"waic\", \"loo\"))\n\nExample WAIC and LOO output for one model:\n\nwaic(english_m53)\n## \n## Computed from 4000 by 250 log-likelihood matrix.\n## \n##           Estimate   SE\n## elpd_waic    263.9 12.3\n## p_waic         5.4  0.7\n## waic        -527.8 24.5\nloo(english_m53)\n## \n## Computed from 4000 by 250 log-likelihood matrix.\n## \n##          Estimate   SE\n## elpd_loo    263.9 12.3\n## p_loo         5.4  0.7\n## looic      -527.8 24.5\n## ------\n## MCSE of elpd_loo is 0.0.\n## MCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.7]).\n## \n## All Pareto k estimates are good (k &lt; 0.7).\n## See help('pareto-k-diagnostic') for details.\n\nThe most important output here is:\n\nThe Estimate and SE of PSIS (a.k.a. “LOO”) and WAIC, in the waic and looic rows.\n\nNote that a rough 95% CredI of WAIC or PSIS would be Estimate +- 1.96*SE.\n\nPareto \\(k\\) estimates\n\nThese values, one per observation, go into the calculation of PSIS-LOO.\nWhen \\(k&gt;\\) some threshold, by default 0.7, LOO is unreliable.\nObservations with \\(k&gt;\\) threshold are influential/potential “outliers”.\n\n\nOther output is less important.1\nNote how similar LOO and WAIC are for this dataset, as expected for large enough \\(n\\) (here, \\(n=250\\)).\nThese metrics can be used for model comparison via the loo_compare() function from loo:\n\nloo_compare(english_m51, english_m52, english_m53, english_m54, criterion = \"waic\")\n##             elpd_diff se_diff\n## english_m53  0.0       0.0   \n## english_m52 -0.6       1.4   \n## english_m54 -0.7       0.2   \n## english_m51 -1.2       1.8\nloo_compare(english_m51, english_m52, english_m53, english_m54, criterion = \"loo\")\n##             elpd_diff se_diff\n## english_m53  0.0       0.0   \n## english_m52 -0.6       1.4   \n## english_m54 -0.8       0.3   \n## english_m51 -1.2       1.8\n\nFor example: the difference in deviance (ELPD) based on WAIC between the cubic (english_m53) and quadratic (english_m52) models is 0.6, with standard error of 1.4. Thus, a rough 95% CredI for this difference in deviance is \\([0.6 - 1.96*1.4, 0.6 + 1.96*1.4]\\) = \\([-2.1, 3.3]\\).\nIn terms of LOO or WAIC, the cubic model (english_m53) wins: it has lower LOO than model english_52 by 0.6, which is lower than english_m54 by 0.2 (difference between 0.8 and 0.6), and so on. So we’d choose the cubic model.\nThe SE values suggest a slightly more complex picture: the 95% CIs for the difference in deviance with the next-best model (quadratic) or the least-good model (linear) overlap, but the 95% CI with the next-next-best model (quartic) does not:\n\n# 95% CI of diff: overlaps 0\nloo_compare(english_m53, english_m52)\n##             elpd_diff se_diff\n## english_m53  0.0       0.0   \n## english_m52 -0.6       1.4\n# 95% CI of diff: doesn't overlap 0\nloo_compare(english_m53, english_m54)\n##             elpd_diff se_diff\n## english_m53  0.0       0.0   \n## english_m54 -0.8       0.3\n# 95% CI of diff: overlaps 0\nloo_compare(english_m53, english_m51)\n##             elpd_diff se_diff\n## english_m53  0.0       0.0   \n## english_m51 -1.2       1.8\n\nThe most conservative option would be to just choose the linear model (english_m51), which doesn’t differ from any nonlinear model by the 95% CredI method:\n\n## 95% CredI all overlap 0\nloo_compare(english_m53, english_m51)\n##             elpd_diff se_diff\n## english_m53  0.0       0.0   \n## english_m51 -1.2       1.8\nloo_compare(english_m52, english_m51)\n##             elpd_diff se_diff\n## english_m52  0.0       0.0   \n## english_m51 -0.6       1.6\nloo_compare(english_m54, english_m51)\n##             elpd_diff se_diff\n## english_m54  0.0       0.0   \n## english_m51 -0.4       1.8\n\nWhen it is of interest to choose a single “best” model”, both methods are used in current practice:\n\nChoose model with lowest WAIC/PSIS\nChoose the simplest model using 95% CredI on WAIC/PSIS differences.\n\nThe second method is more conservative.\n\n\n\n\n\n\nBroader context: Using PSIS/WAIC for model selection\n\n\n\n\n\nThe two methods above—choosing the model with lowest PSIS (or WAIC) versus choosing the model which beats others by at least 2 SE—can be thought of as two options on a continuum, where you choose the best model depending on which one beats others by at least X SE (where X = 0 or 2). There is no right answer here, and no reason you need to be restricted to X = 0 or 2. These are just conventional choices for how conservative you want to be, like the commonly used AIC and BIC for frequentist models just correspond to different penalty terms in “data likelihood minus penalty”.\nIn fact, there is no reason you need to choose a “best model”—McElreath advises against it. Flego and Forrest (2021) is a nice example from phonetics where different X are used to differentiate between models which are more and less likely for the shape of vowel formant trajectories (linear, quadratic, an interpolation between two points, etc.).\nAn interesting option is model averaging, where instead of choosing a best model, you ask, “what combination of a set of models best predicts the data”, to get a weight for each model (where the weights add up to one). This is implemented in the loo package (see ?loo_model_weights).\nFor example, for the four models considered above:\n\nloo_model_weights(english_m51, english_m52, english_m53, english_m54)\n## Method: stacking\n## ------\n##             weight\n## english_m51 0.062 \n## english_m52 0.171 \n## english_m53 0.767 \n## english_m54 0.000\n\nThe data is best described as 77% the cubic model, 17% the quadratic model, and 6% the linear model.\n\n\n\nFor comparison, shown in Section 4.5: when we compare frequentist linear regression models for the same case, we get that the linear model or cubic model are best, depending on the method used for model comparison (e.g. AIC vs. BIC).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.1  \n\nFit the same four models as above, but now to english_25, and recalculate LOO/WAIC.\nWhich model is best, in terms of LOO?\nWhat conclusion do the 95% CIs of LOO differences suggest?\nDoes this conclusion fit your intuition from plotting the data geom_smooth() of WrittenFrequency vs RTlexdec)? If you get different answers for (b) and (c), which better fits this plot?\nYour “best model” from (b) should be different from the english_250 case. Why is this?\nExtra: Try (a)–(d) for a model fit to the entire english dataset. (You should now find clear evidence for a nonlinear effect.)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Regression Models 2</span>"
    ]
  },
  {
    "objectID": "week5.html#checking-a-fitted-model",
    "href": "week5.html#checking-a-fitted-model",
    "title": "4  Bayesian Regression Models 2",
    "section": "4.3 Checking a fitted model",
    "text": "4.3 Checking a fitted model\nOnce fitted, a model needs to be checked to be confident in its results. Methods for this are discussed in McElreath Sec. 9.4-9.5.\n\n4.3.1 Posterior plots\nThe most basic visual check of a fitted model is examining the posterior distribution of model parameters:\n\nEach parameter’s (marginal) distribution\nPairwise distributions\n\n\nis crucial, while (2) is nice but not always feasible as the number of parameters increases.\n\nLet’s see examples of what these plots look like for a “good” model, and then one where something has gone wrong.\n\n4.3.1.1 Example: good model\nConsider model english_m53, the “cubic” model chosen in Section 4.2.1 as having the lowest WAIC/PSIS. We first plot the (marginal) posterior distribution of each parameter, using mcmc_plot() from brms—this is a convenience function for calling MCMC plotting functions from the bayesplot package on brms models.\n\nmcmc_plot(english_m53, type = \"hist\", bins = 20)\n\n\n\n\n\n\n\n\nNow consider pairwise posterior distributions, restricting just to parameter starting with b (which are the regression coefficients):\n\nmcmc_pairs(english_m53,\n  regex_pars = \"^b\",\n  off_diag_args = list(size = 1 / 5, alpha = 1 / 5)\n)\n\n\n\n\n\n\n\n\nHere we use mcmc_pairs(), one of several functions for bivariate posterior distribution summaries provided by bayesplot.\nIt is visually clear that there are enough samples to tell the shape of each marginal and pairwise distribution (ellipses/bell curves = multivariate Gaussians).\n\n\n4.3.1.2 Example: bad model\nWhat would posteriors plots (here) and MCMC diagnostics (next section) look like for a model where we hadn’t sampled for long enough, or there was a problem with the model specification?\nLet’s use the high-collinearity example model from Section 3.3.3 (english_collin_m1), but now refit using:\n\n4 chains\n100 samples, of which 25/75 are warmup/real samples.\n\nThis is obviously too few samples, but this will let us see what plots for a bad model look like.\nFit this model:\n\n# make sure you get the same \"random\" result:\nset.seed(100)\nenglish_iter100_m51 &lt;-\n  brm(\n    data = english_25,\n    RTlexdec ~ 1 + WrittenFrequency_c + Familiarity_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n      prior(normal(0, 100), class = Intercept),\n      prior(normal(0, 5), class = b),\n      prior(exponential(1), class = sigma)\n    ), iter = 100, warmup = 25,\n    file = \"models/english_iter100_m51\"\n  )\n\nModel output:\n\nenglish_iter100_m51\n## Warning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be\n## careful when analysing the results! We recommend running more iterations and/or\n## setting stronger priors.\n## Warning: There were 70 divergent transitions after warmup. Increasing\n## adapt_delta above 0.8 may help. See\n## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: RTlexdec ~ 1 + WrittenFrequency_c + Familiarity_c + SubjectYoung \n##    Data: english_25 (Number of observations: 25) \n##   Draws: 4 chains, each with iter = 100; warmup = 25; thin = 1;\n##          total post-warmup draws = 300\n## \n## Regression Coefficients:\n##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept              6.53      0.02     6.50     6.58 1.10       96       53\n## WrittenFrequency_c    -0.02      0.02    -0.06     0.03 1.12       29       45\n## Familiarity_c         -0.05      0.03    -0.12     0.01 1.12       27       63\n## SubjectYoung          -0.20      0.03    -0.28    -0.14 1.08      114      132\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.10      0.02     0.07     0.14 1.66        7       74\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThere is a lot of output here suggesting this is not a good fit, discussed more below (Section 4.3.2.2). For the moment, consider marginal and pairwise posterior plots:\n\nmcmc_plot(english_iter100_m51, type = \"hist\", bins = 20)\n\n## plot just parameters starting with 'b' or 'sig'(regression coeffs, sigma):\nmcmc_pairs(english_iter100_m51,\n  regex_pars = \"^(b|sig)\",\n  off_diag_args = list(size = 0.75, alpha = 0.5)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese plots do not look like enough samples have been taken to approximate the distribution, for most parameters.\nSee the bayesplot vignette for various functions for plotting MCMC draws (from the posterior).\n\nExercise 4.2 In Section 3.2.1.2, we made a “hex plot” showing the bivariate posterior distribution of two coefficients for model english_m41. This exercise is to make a similar hex plot for the high-colinearity example model from Section 3.3.3 (english_collin_m1).\n\nFigure out what bayesplot function is used to make this kind of plot.\nWhat are the names of the regression coefficients in model english_collin_m1 corresponding to the predictors for word frequency (WrittenFrequency_c) and familiarity (Familiarity_c)?\nApply the function from (a) to english_coliln_m1 (which you’ll have to load or re-fit) to show a hex plot for the predictors from (b).\n\n\n\n\n\n4.3.2 MCMC Diagnostics\nWe will show plots of some MCMC diagnostics mentioned in brms output:\n\nRhat: measures mixing of chains—related to ratio of within-chain versus between-chain variance of samples.\n\nWe want Rhat near 1 for all parameters. Rhat above 1.1 or 1.05 is cause for concern (though these are arbitrary cutoffs).\n\nESS: effective sample size, measures how independent samples within the same chain are (degree of autocorrelation).\n\nESS \\(&lt;\\) “total post-warmup draws” = some autocorrelation. Not a problem, necessarily, but we’ll need more samples to summarize the posterior. ESS far below “total post-warmup draws” suggests a problem.\nESS \\(&gt;\\) “total post-warmup draws” = anticorrelated samples. This is great, but not necessary.\n\n\nThe bayesplot vignette on MCMC diagnostics is very useful and informative. We will show just a few kinds of plots demonstrated there, using the mcmc_plot() function from brms that interfaces with bayesplot.2\n\n4.3.2.1 Example: good model\nFirst, some plots for a well-behaved model: english_m53, our cubic effect of WrittenFrequency model, fitted with default brms settings:\n\n4 chains\nEach chain has 1000 warmup and 1000 post-warmup draws\nTotal post-warmup draws: 4000\n\nTrace plots:\n\nmcmc_plot(english_m53, type = \"trace\") +\n  ## the following two lines just make the output legible, and\n  ## are optional\n  theme(strip.text = element_text(size = 10)) +\n  facet_wrap(~parameter, nrow = 3, ncol = 2, scales = \"free\")\n## No divergences to plot.\n\n\n\n\n\n\n\n\nAutocorrelation plots, for samples from the posterior for model parameters:\n\nmcmc_plot(english_m53, type = \"acf\") +\n  ## following line just makes the output more legible\n  ## / is optional\n  theme(strip.text = element_text(size = 8))\n\n\n\n\n\n\n\n\n\\(\\hat{R}\\) plot, showing Rhat values for each model parameter:\n\nmcmc_plot(english_m53, type = 'rhat') +\n  ## adds parameter names on y-axis\n  yaxis_text(hjust = 1)\n\n\n\n\n\n\n\n\nThese values of \\(R_{hat}\\) are fine—none is anywhere near the 1.05 cutoff.\nSince realistic (e.g. mixed-effects) models have dozens-hundreds of parameters, the default display for an rhat plot doesn’t show any labels (just take out the yaxis_text(hjust = 1) line above to see this.)\nESS plot (i.e., \\(n_{eff}\\)) for each coefficient:\n\nneff_ratio(english_m53) %&gt;% mcmc_neff(size = 2) + yaxis_text(hjust = 1)\n\n\n\n\n\n\n\n\nSampling is very efficient (\\(N_{eff}/N &gt; 0.5\\)).\n\nExercise 4.3 (Extra) The \\(\\hat{R}\\) and ESS plots above would be cleaner if they (a) just showed fitted model parameters (regression coefficients and \\(\\sigma\\)), and (b) put the parameters in a sensible order: Intercept, then other b_ parameters, then sigma.\nFigure out how to make versions of these plots implementing (a) and (b).\nThis is good practice in either reading documentation or interacting with a chatbot (ChatGPT or GitHub Copilot).\n\n\n\n4.3.2.2 Example: bad model\nTrace plots:\n\nmcmc_plot(english_iter100_m51, type = \"trace\") +\n  ## the following lines is optional / makes the output legible\n  theme(strip.text = element_text(size = 8))\n\n\n\n\n\n\n\n\nThese do not look like hairy caterpillars:\n\nThe chains have not mixed—they are not on top of each other (for some parameters, like sigma, SubjectYoung)\nThey are not stationary: there is a definite trend in e.g. the sigma chains, as opposed to flat horizontal lines, with lots of vertical “hair” indicating effective sampling.\nThey include many divergences (as also indicated in the model output above).\n\n\nExercise 4.4  \n\nMake the other diagnostic plots as above, now for model english_iter100_m51. What problems do you see?\nWhich parameter(s) are particularly poorly estimated by the model, or otherwise problematic?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.5 (Extra)  \n\nRefit the model with iter=200, warmup=100. What parameter(s) are still problematic?3\nTry to install the shinystan package, and run shinystan::launch_shinystan() on one of your models to explore MCMC diagnostics.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Regression Models 2</span>"
    ]
  },
  {
    "objectID": "week5.html#sec-brm2-contrasts",
    "href": "week5.html#sec-brm2-contrasts",
    "title": "4  Bayesian Regression Models 2",
    "section": "4.4 Working with multi-level factors and contrasts",
    "text": "4.4 Working with multi-level factors and contrasts\nThis is conceptually similar to frequentist models, covered in Chap. 7 of Sonderegger (2023). Any multi-level factor requires choosing a contrast coding scheme, and it is possible to interpret the fitted model either by interpreting the coefficient corresponding to a contrast (e.g. “level 2 minus level 1”: RMLD Sec. 7.2), or using post-hoc tests (RMLD Sec. 7.3).\nIn a Bayesian regression model, a “post-hoc test” can be thought of as summarizing the posterior:\n\nTo ask the model, “What is the predicted difference in \\(y\\) between level 2 and level 1 of factor \\(x\\)?” :\n\nCalculate many times using the posterior: predictions for \\(y\\) when \\(x\\) = level 1 and \\(x\\) = level2, with other predictors held constant. (That is, choose a draw of the model coefficients, then use these to calculate the two predictions.)\nCalculate the difference in these predictions, \\(\\delta\\), for each posterior draw.\nSummarize the posterior distribution of \\(\\delta\\) as desired (e.g. mean + 95% CI).\n\n\nLet’s use as an example the french_cdi_24 data. We recode lexical_class using Helmert contrasts (RMLD Sec. 7.2.8):\n\ncontrasts(french_cdi_24$lexical_class) &lt;- contr.helmert(4)\n\nNow, fit a Bayesian model like cdi_cc_mod_1, the frequentist (logistic regression) model described there. We’ll use similar “uninformative” priors to the diatones model (Section 3.4.1), which also used logistic regression.4\n\nfrench_cdi_m51 &lt;- brm(\n  data = french_cdi_24,\n  produces | trials(1) ~ lexical_class,\n  family = binomial,\n  prior = c(\n    prior(normal(0, 5), class = Intercept),\n    prior(normal(0, 3), class = b)\n  ), file = \"models/french_cdi_m51\"\n)\n\n\nfrench_cdi_m51\n##  Family: binomial \n##   Links: mu = logit \n## Formula: produces | trials(1) ~ lexical_class \n##    Data: french_cdi_24 (Number of observations: 560) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept         -0.28      0.11    -0.50    -0.07 1.00     3893     3269\n## lexical_class1     0.47      0.16     0.16     0.78 1.00     3765     3088\n## lexical_class2     0.21      0.10     0.02     0.41 1.00     3699     2926\n## lexical_class3     0.21      0.04     0.13     0.30 1.00     4049     3196\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nWe could do the whole process described above by getting model predictions for each level of lexical_class, for many posterior distributions (e.g. using spread_draws() from tidybayes). Instead we can use the emmeans package, which works with brms models analogously to its use with frequentist models. See Section 3.3.1 (and the “Practical note” box there) for more on emmeans / similar packages for computing “marginal effects”.\nTo test for pairwise differences (“Tukey HSD”) between levels, for example:\n\nemm1 &lt;- emmeans(french_cdi_m51, ~lexical_class)\ncontrast(emm1, \"tukey\")\n##  contrast                    estimate lower.HPD upper.HPD\n##  function_words - verbs        -0.939    -1.537    -0.311\n##  function_words - adjectives   -1.112    -1.796    -0.419\n##  function_words - nouns        -1.545    -2.071    -0.984\n##  verbs - adjectives            -0.166    -0.807     0.440\n##  verbs - nouns                 -0.604    -1.059    -0.168\n##  adjectives - nouns            -0.430    -0.967     0.112\n## \n## Point estimate displayed: median \n## Results are given on the log odds ratio (not the response) scale. \n## HPD interval probability: 0.95\n\nWe might say the 95% HPDs which don’t overlap zero correspond to levels which “are different”, so:\n\nfunction_words \\(&lt;\\) verbs, adjectives, nouns\nverbs \\(&lt;\\) nouns\n\nVisualize the actual posterior of these differences (code from here), using the gather_emmeans_draws() function from tidybayes:\n\ncont &lt;- contrast(emm1, \"tukey\")\ncont_posterior &lt;- gather_emmeans_draws(cont)\n\nggplot(\n  cont_posterior,\n  aes(y = contrast, x = .value, fill = contrast, group = contrast)\n) +\n  geom_halfeyeh(alpha = 0.5) +\n  geom_vline(xintercept = 0, color = \"red\", lty = 2)\n## Warning in geom_halfeyeh(alpha = 0.5): 'geom_halfeyeh' is deprecated.\n## Use 'stat_halfeye' instead.\n## See help(\"Deprecated\") and help(\"tidybayes-deprecated\").\n\n\n\n\n\n\n\n\nAnother plot, by level:\n\nfrench_cdi_m51 %&gt;%\n  emmeans(~lexical_class) %&gt;%\n  gather_emmeans_draws() %&gt;%\n  ggplot(aes(x = lexical_class, y = .value)) +\n  geom_eye() +\n  stat_summary(aes(group = NA), fun.y = mean, geom = \"line\") +\n  theme_light()\n## Warning in geom_eye(): 'geom_eye' is deprecated.\n## Use 'stat_eye' instead.\n## See help(\"Deprecated\") and help(\"tidybayes-deprecated\").\n## Warning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\n## ℹ Please use the `fun` argument instead.\n\n\n\n\n\n\n\n\nCompare to frequentist model:\n\ncdi_cc_mod_1 &lt;- glm(produces ~ lexical_class, data = french_cdi_24, family = \"binomial\")\ntidy(cdi_cc_mod_1, conf.int = TRUE)\n## # A tibble: 4 × 7\n##   term           estimate std.error statistic    p.value conf.low conf.high\n##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)      -0.277    0.107      -2.59 0.00965     -0.489    -0.0690\n## 2 lexical_class1    0.462    0.160       2.88 0.00398      0.153     0.784 \n## 3 lexical_class2    0.212    0.101       2.09 0.0368       0.0126    0.411 \n## 4 lexical_class3    0.213    0.0449      4.74 0.00000213   0.125     0.302\n\n\ncontrast(emmeans(cdi_cc_mod_1, ~lexical_class), \"tukey\")\n##  contrast                    estimate    SE  df z.ratio p.value\n##  function_words - verbs        -0.923 0.321 Inf  -2.880  0.0208\n##  function_words - adjectives   -1.096 0.364 Inf  -3.014  0.0138\n##  function_words - nouns        -1.525 0.281 Inf  -5.424  &lt;.0001\n##  verbs - adjectives            -0.173 0.322 Inf  -0.536  0.9502\n##  verbs - nouns                 -0.602 0.225 Inf  -2.673  0.0377\n##  adjectives - nouns            -0.429 0.283 Inf  -1.515  0.4287\n## \n## Results are given on the log odds ratio (not the response) scale. \n## P value adjustment: tukey method for comparing a family of 4 estimates\n\nNote that the Bayesian method does not explicitly correct for multiple comparisons, in part because there are no hypothesis tests per se in a (fully) Bayesian framework. How/whether to correct for “multiplicity” in Bayesian models is a big debate, but the short answer is that:\n\nMaking multiple comparisons using the posterior will often give a similar result to a frequentist procedure where multiple comparisons are explicitly corrected for.\nIn common cases, emmeans will do something sensible for you.\n\n\nExercise 4.6 (Extra) It may be of interest to calculate other summaries of this child’s lexical knowledge than can be easily calculated by emmeans. Suppose we were interested in the noun/verb ratio: the odds of knowing a word if it’s a noun (\\(p_{noun}\\)/(\\(1-p_{noun}\\))), divided by the odds of knowing a word if it’s a verb.\n\nSample from the posterior, at each draw getting predicted values for each level of lexical_class, in probability\nFor each draw, calculate \\(p_{noun}\\) and \\(p_{verb}\\), and add these to the dataframe.\nFor each draw, calculate the “noun/verb ratio” defined above.\nPlot the posterior distribution of the N/V ratio.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Regression Models 2</span>"
    ]
  },
  {
    "objectID": "week5.html#extra-3",
    "href": "week5.html#extra-3",
    "title": "4  Bayesian Regression Models 2",
    "section": "4.5 Extra",
    "text": "4.5 Extra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlego, Stefon, and Jon Forrest. 2021. “Leveraging the Temporal Dynamics of Anticipatory Vowel-to-Vowel Coarticulation in Linguistic Prediction: A Statistical Modeling Approach.” Journal of Phonetics 88: 101093.\n\n\nGabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. “Visualization in Bayesian Workflow.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 182 (2): 389–402.\n\n\nKurz, S. 2021. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.\n\n\nVehtari, Aki, Jonah Gabry, Måns Magnusson, Yuling Yao, Paul-Christian Bürkner, Topi Paananen, and Andrew Gelman. 2024. “Loo: Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models.” https://mc-stan.org/loo/.\n\n\nVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and WAIC.” Statistics and Computing 27: 1413–32. https://doi.org/10.1007/s11222-016-9696-4.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Regression Models 2</span>"
    ]
  },
  {
    "objectID": "week5.html#footnotes",
    "href": "week5.html#footnotes",
    "title": "4  Bayesian Regression Models 2",
    "section": "",
    "text": "elpd_waic is just WAIC/(-2)—it’s in units of log-probability rather than “deviance”. This doesn’t matter for model comparison. p_waic is the effective number of model parameters. It’s not usually used, but can be more intuitive than ELPD/WAIC. Note that the model has 5.2 “effective” parameters, despite having 6 actual parameters, suggesting that some of them are not doing much. elpd_loo/p_loo are analogous to the same for WAIC.↩︎\nFor example, mcmc_plot(my_brms_model, type = 'trace') shows the same thing as mcmc_plot(my_rstan_model). This section of Kurz (2023) shows how to use the actual bayesplot functions with brms models.↩︎\n(Don’t read until you’ve answered.). This illustrates a very general fact about regression models: estimating means (the “population-level effects”) takes less data / sampling from the posterior than estimating variances (here, “Family-specific parameters”).↩︎\nIn a real model of this data we’d want to figure out weakly informative priors, but it will make no difference given the size of this dataset.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Regression Models 2</span>"
    ]
  },
  {
    "objectID": "hw1.html",
    "href": "hw1.html",
    "title": "14  Homework 1",
    "section": "",
    "text": "14.1 Preliminaries\nLibraries we’ll need:\nlibrary(tidyverse)\n\nlibrary(GGally)\nlibrary(Hmisc)\nlibrary(patchwork)\nLoad data:\n# change path for your system\nwordbank_data &lt;- read.csv(file = \"../../data/wordbank_data_hw1.csv\")\n\n# In F2024: this data should be at\n# people.linguistics.mcgill.ca/~morgan/ling683-f2024/wordbank_data_hw1.csv\nThis data comes from Wordbank, an open database of children’s vocabulary growth which aggregates data from thousands of parent-report questionnaires on lexical acquisition—the MacArthur Bates Communicative Development Inventory (CDI)—across many languages. Frank et al. (2021) (ebook) describes this remarkable resource in more detail.\nFor our purposes we are using data from Braginsky et al. (2019) (paper), which covers:\nWe will just consider data aggregated across children, restricted to children aged 24 months, to be able to apply models we’ve learned about so far (i.e., no grouping structure).\nRelevant columns:\nIf you’d like more detail, Chap. 10 of Frank et al. (2021) discusses most of the word properties. The French CDI data discussed below is explored and modeled using frequentist methods in Chapters 7 and 9 ofRMLD (Sonderegger 2023).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "hw1.html#preliminaries",
    "href": "hw1.html#preliminaries",
    "title": "14  Homework 1",
    "section": "",
    "text": "Children learning one of 8 languages\nFor 664 words (the “WS” list).1\n\n\n\n\nlanguage: language X\ndefinition: the word, in language X\nuni_lemma: the word’s gloss in English\nnum_true: number of children aged 24 months who are reported to be able to produce this word.\nnum_false: same for “not able to produce”\n\nThus, there are num_true + num_false children for language X. This number is the same for every word within a given language.\n\nprop: num_true/(num_true+num_false)\nStructural properties:\n\nlexical_class: broad syntactic category\nnum_phons: length of word, in phones\n\nMeasures of environmental input\n\nfrequency: Word’s frequency (in CHILDES)\nMLU: mean length of utterance (in CHILDES)—proxy for syntactic complexity\n\nMeaning-based properties:\n\nvalence, concreteness, arousal",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "hw1.html#introduction",
    "href": "hw1.html#introduction",
    "title": "14  Homework 1",
    "section": "14.2 Introduction",
    "text": "14.2 Introduction\nOur research questions in this homework are:\n\nHow does lexical class affect when words are acquired? - Of particular interest are noun word bias and function word bias, which are hypothesized to be consistent across languages (nouns/function words acquired earliest/latest, on average) - See Sec. 7.1.2 of RMLD for context, and see Ch.11 of Frank et al. (2021) for more detail.2\nHow does environmental input affect when words are acquired? - In particular: frequency and MLU\n\nnum_phons must also be considered, as it is correlated with other predictors, but it is not otherwise of direct interest.\n\n14.2.1 Example: Quebec French\nTo motivate, I will use just data from Quebec French. In this homework we will use “number of kids reported to know the word at 24 months” as a proxy for “when words are acquired”.\n\ndata_fr &lt;- filter(wordbank_data, language == \"French (Quebec)\")\n\nPreprocessing: get rid of ‘other’ lexical category, reorder lexical_class to the theoretically-expected order, and remove rows for which predictors of interest aren’t defined:\n\n\ndata_fr &lt;- data_fr %&gt;%\n  filter(lexical_class != \"other\") %&gt;%\n  mutate(lexical_class = fct_relevel(lexical_class, \"function_words\", \"verbs\", \"adjectives\", \"nouns\")) %&gt;%\n  droplevels() %&gt;%\n  filter(!is.na(lexical_class) & !is.na(frequency) & !is.na(MLU) & !is.na(num_phons))\n\nSome empirical plots:\n\n\nCode\n# fraction of kids who know each word (num_true / (num_true + num_false))\nemp_plot_1 &lt;- data_fr %&gt;% ggplot(aes(x = prop)) +\n  geom_histogram() +\n  xlab(\"Proportion of children\") +\n  ggtitle(\"% children who know each word\")\n# words range from 0% to near-100% known\n\n# plotted as num_true\nemp_plot_2 &lt;- data_fr %&gt;% ggplot(aes(x = num_true)) +\n  geom_histogram() +\n  xlab(\"Number of children\") +\n    ggtitle(\"Number of children who know each word\")\n\nemp_plot_1 / emp_plot_2\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLexical class and frequency:\n\n\nCode\nlex_class_plot_1 &lt;- data_fr %&gt;% ggplot(aes(x = lexical_class, y = num_true)) +\n  stat_summary(fun.data = \"mean_cl_boot\") +\n  geom_jitter(alpha = 0.2, width = 0.2) +\n  labs(y = \"Number of children\", x = \"Lexical class\")\n\n\nlex_class_plot_2 &lt;- data_fr %&gt;% ggplot(aes(x = frequency, y = num_true)) +\n  geom_smooth() +\n  geom_point(aes(color = lexical_class)) +\n  labs(x = \"Word frequency (log)\", y = \"Number of children\", color = \"Lexical class\")\n  \n\nlex_class_plot_1 / lex_class_plot_2\n## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nMLU and word length:\n\n\nCode\nmlu_plot &lt;- data_fr %&gt;% \n  ggplot(aes(x = MLU, y = num_true)) +\n  geom_smooth() +\n  geom_point() + \n  labs(x = \"Mean length of utterance (MLU)\", y = \"N children\")\n\nnum_phons_plot &lt;- data_fr %&gt;%\n  ggplot(aes(x = num_phons, y = num_true)) +\n  geom_smooth() +\n  geom_point() + \n  labs(x = \"Word length (phones)\", y = \"N children\")\n\nmlu_plot / num_phons_plot\n## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNote that these predictors are somewhat correlated:\n\ndata_fr %&gt;%\n  dplyr::select(MLU, frequency, lexical_class, num_phons) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\n\nEspecially important, given RQ1, is the relationship between lexical_class and frequency (function words have higher frequency), which presumably exists across languages.\nYour goal is to fit a reasonable model addressing RQs 1–2, step by step, for one of the 8 languages:\n\nunique(wordbank_data$language)\n## [1] \"Croatian\"           \"Danish\"             \"English (American)\"\n## [4] \"French (Quebec)\"    \"Italian\"            \"Norwegian\"         \n## [7] \"Russian\"            \"Spanish (Mexican)\"  \"Turkish\"\n\n\n\n14.2.2 Domain knowledge\nTo interpret your results, and to choose weakly informative priors, you’ll need to know a bit about lexical acquisition. You can assume that:\n\nThere is a lot of variability in which words kids know, and how many kids know a given word, at age 24 months, even after accounting for all known predictors\n\nThus, the effect of any individual predictor \\(x\\) may be large, but not giant (e.g. predicting 95% vs 5% of kids as \\(x\\) is changed).\n\nIn many (but not all) languages, on average:\n\nNouns are acquired earlier than predicates (verbs, adjectives)\nFunction words are acquired later than predicates.\nMore frequent words are acquired earlier.\n\n\nThis tells us what directions are expected for frequency and lexical_class. For the other predictors, we might expect words with higher MLU (proxy for syntactic complexity), or longer words (num_phons) to be acquired later.\nThese are all violable expectations. Our priors might capture something about expected direction, but should not rule out any effect (e.g. by saying frequency must have a positive effect).\n\n\n14.2.3 This assignment\nPlease choose one language per student, coordinating with each other (e.g. on Piazza) to make sure you all have different languages.\nYou can model \\(y\\) = num_true using any of the following models, none of which is quite right for this data (but are fine for this homework):\n\nLinear regression\n\n(But: num_true are counts)\n\nPoisson regression or negative binomial regression\n\n(But: num_true is bounded, by num_true + num_false)\n\nBinomial regression, using num_true/num_false as successes/failures\n\nThis would be most appropriate, but we have not directly covered it in class.\nInterpretation of coefficients similar to logistic regression.\nRisks overdispersion, similarly to Poisson regression.\n\n\nChoose one model type, by whatever criterion you wish, and assume this kind of model in answering the questions below. You can answer questions by just writing code chunks and text directly into the .qmd file. You should submit:\n\nYour final .qmd file\nA compiled version, as PDF or HTML.\n\nYou are encouraged to work together (though this is not required), but you must write up your code and (prose) results independently, for your own language. See the syllabus for the Generative AI policy.\n\n\n14.2.4 Tips\nThe main issues students have had with this homework / areas to focus on:\n\nCorrect application and interpretation of contrasts for lexical_class.\n\nYou’ll need to use a “centered” contrast coding scheme.\n\nDon’t interpret factor levels (e.g. lexical_class1 = nouns) instead of contrasts.\nReview Chapter 7 of RMLD if needed for contrast coding and related topics like post-hoc tests.\n\nInclude and properly justify prior parameters.\n\nThere are examples of such justification in the lecture notes and readings.\nA prior predictive checks is required.\n\nInclude and interpret posterior predictive checks",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "hw1.html#question-1-preliminaries",
    "href": "hw1.html#question-1-preliminaries",
    "title": "14  Homework 1",
    "section": "14.3 Question 1: Preliminaries",
    "text": "14.3 Question 1: Preliminaries\nRestrict to data just for your language, and do preprocessing (the “Preprocessing” step done for French above).\n(a). Make 3-4 empirical plots exploring the effects of predictors of interest. Briefly discuss the patterns you see (a few sentences).\n\nThese do not need to be the plots I made for French, which are not necessarily the best ones for your language. Your plots should be chosen to illustrate the main empirical patterns in you language. (Also, your plots should be better-looking and have proper axis labels, etc.)\nMake sure you use geom_smooth() for continuous variables, to detect possible nonlinear effects.\n\n(b). Standardize the predictors. This means minimally using a “centered” contrast coding scheme for lexical_class, and centering the other predictors.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "hw1.html#question-2-choosing-priors",
    "href": "hw1.html#question-2-choosing-priors",
    "title": "14  Homework 1",
    "section": "14.4 Question 2: choosing priors",
    "text": "14.4 Question 2: choosing priors\nYou will fit a first regression model, of num_true (using your chosen model class), as a function of these predictors (no interactions or nonlinear terms): frequency, MLU, num_phons, lexical_class\n(a). Name the parameters of your model, giving the interpretation of each one. (Ex: for a linear regression, there would be one intercept, four \\(\\beta\\) coefficients and one \\(\\sigma\\) variance parameter.)\n(b). Choose a “weakly informative” prior for each term, giving brief motivation for each, and performing at least one prior predictive check (e.g. for the lexical_class or frequency effect)—you don’t need to do this for each parameter.\nRemember that:\n\nYour reasoning can include domain knowledge, the range of \\(y\\), and sanity checks.\nYour reasoning should not include actually looking at patterns in the empirical data (such as the direction of the frequency effect)!\n\nYou should describe each prior in words or an equation, but not in brms model format (yet).\nNB: your prior for lexical_class does not need to capture any expectation about the order of levels (like that nouns \\(&gt;\\) predicates). You are welcome to do this, but it’s not required (and is not trivial).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "hw1.html#question-3-fitting-a-first-model",
    "href": "hw1.html#question-3-fitting-a-first-model",
    "title": "14  Homework 1",
    "section": "14.5 Question 3: fitting a first model",
    "text": "14.5 Question 3: fitting a first model\nSpecify and fit your model, using brms.\nExamine MCMC diagnostics to convince yourself the model fit properly, and change any necessary fitting parameter if it did not. You only need to include a brief summary of this in your writeup and it won’t be directly graded. (You could examine trace plots, Rhat values, posterior histograms, etc.)\n(a). Once you have a satisfactory fit, summarize the model’s results:\n\nVisually: By making partial effect plots, illustrating the effect of each predictor as others are held constant.\nQuantitatively: By discussing the estimates for frequency, MLU, and num_phons, and showing appropriate post-hoc comparisons (pairwise is fine) for the effect of lexical_class.\n\nYour discussion should include: the 95% CI, and/or both “existence” (like \\(p_{d}\\)) and “significance” metrics (like ROPE). In other words, you must meaningfully discuss both the existence and importance of effects. Your discussion should not include any hard cutoffs, like \\(p_d &gt; 0.95\\).\n\n\nThe “quantitatively” part should be about a paragraph, or a table plus a shorter paragraph. NB: Part of the assignment is to find examples/guidance for how to report results of a Bayesian regression model.\n(b). Critique the model:\n\nPerform at least two posterior predictive checks (show the plots).\n\nDoes it suggest anything about how well the model fits, and what might need to be changed about it?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "hw1.html#question-4-the-frequency-lexical_class-relationship",
    "href": "hw1.html#question-4-the-frequency-lexical_class-relationship",
    "title": "14  Homework 1",
    "section": "14.6 Question 4: the frequency-lexical_class relationship",
    "text": "14.6 Question 4: the frequency-lexical_class relationship\nIntuitively, there are two conflicting expectations:3\n\nHigher-frequency words should be known by more children\nFunction words should be known by fewer children\n\nThe conflict arises because function words will have higher frequency (on average) than words of other lexical_class values. The goal here is to determine whether, and how, your model has captured this aspect of the data.\n(a). What two (or more) quantities predicted by the model should be related, which would reflect this conflict? Explain your reasoning.4\nYour answer might involve two observations (e.g. “\\(y\\) for nouns, with other predictors held constant), or model parameters (e.g. ”\\(\\beta_{frequency}\\)“) or functions of model parameters.\n(b). Call these two quantities \\(x_1\\) and \\(x_2\\). Sample from a posterior distribution over each one (using e.g. 1000 samples from the posterior). This should result in a data frame with 1000 rows, and (at least) two columns—one for \\(x_1\\) and one for \\(x_2\\).\n(c). Make a plot, using these values, which addresses how/whether your model captures the conflict above. Explain.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "hw1.html#question-5-a-second-model",
    "href": "hw1.html#question-5-a-second-model",
    "title": "14  Homework 1",
    "section": "14.7 Question 5: a second model",
    "text": "14.7 Question 5: a second model\nAn empirical plot of frequency vs. the dependent variable, for your language, should suggest a possibly nonlinear effect. (As in the French example above.) You may have also found other evidence suggesting this (or not).\n(a). What range of degrees of nonlinear effect of frequency—whether fit with (orthogonal) polynomials or splines—seems possible here?5\n(b). Fit 2 or more additional models capturing these possible nonlinear effects. (If you flagged any simple issue to be changed in 3(b), you could also make this change in these models.) Compare all models fitted—these additional models, as well as Model 1 above—using an appropriate quantitative metric, to choose a “best” model.\nThe “Examine MCMC diagnostics….” paragraph above applies here as well.\n(c). Plot the partial effects of frequency and lexical_class for this new model. How have your conclusions about the effect of each predictor changed from above, if at all? Can you explain why?\n(d). Optional: repeat 4(a)–(c) for this new model.\n\n\n\n\nBraginsky, Mika, Daniel Yurovsky, Virginia A Marchman, and Michael C Frank. 2019. “Consistency and Variability in Children’s Word Learning Across Languages.” Open Mind 3: 52–67.\n\n\nFrank, Michael C, Mika Braginsky, Daniel Yurovsky, and Virginia A Marchman. 2021. Variability and Consistency in Early Language Learning: The Wordbank Project. https://langcog.github.io/wordbank-book/. MIT Press.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "hw1.html#footnotes",
    "href": "hw1.html#footnotes",
    "title": "14  Homework 1",
    "section": "",
    "text": "We used raw data in uni_joined.RData here)↩︎\nFor McGill students: we used this data in LING 620, so this should be review.↩︎\nContact me if these are not borne out in the empirical data for your language.↩︎\nHint: One quantity might reflect function words relative to other lexical classes.↩︎\nNote that you don’t need to try nonlinear effects of other predictors—though you’re welcome to do so, with justification.↩︎",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Homework 1</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Arel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. Forthcoming.\n“How to Interpret Statistical Models Using\nmarginaleffects in R and\nPython.” Journal of Statistical Software,\nForthcoming. https://marginaleffects.com.\n\n\nArif, Suchinta, and M Aaron MacNeil. 2022. “Predictive Models\nAren’t for Causal Inference.” Ecology Letters 25 (8):\n1741–45. https://doi.org/10.1111/ele.14033.\n\n\nBaayen, R. H. 2008. Analyzing linguistic data:\nA practical introduction to statistics using R. Cambridge:\nCambridge University Press.\n\n\nBaayen, R. H., and Elnaz Shafaei-Bajestan. 2019. languageR:\nAnalyzing Linguistic Data: A Practical Introduction to Statistics.\nhttps://CRAN.R-project.org/package=languageR.\n\n\nBarreda, Santiago, and Noah Silbert. 2023. Bayesian Multilevel\nModels for Repeated Measures Data: A Conceptual and Practical\nIntroduction in r. Taylor & Francis. https://santiagobarreda.com/bmmrmd/.\n\n\nBürkner, Paul-Christian. 2024. “Estimating Multivariate Models\nwith Brms.” https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html.\n\n\nBürkner, Paul-Christian, Jonah Gabry, Matthew Kay, and Aki Vehtari.\n2024. “Posterior: Tools for Working with Posterior\nDistributions.” https://mc-stan.org/posterior/.\n\n\nBürkner, Paul-Christian, and Matti Vuorre. 2019. “Ordinal\nRegression Models in Psychology: A Tutorial.” Advances in\nMethods and Practices in Psychological Science 2 (1): 77–101.\n\n\nCiaccio, Laura Anna, and João Verı́ssimo. 2022. “Investigating\nVariability in Morphological Processing with Bayesian Distributional\nModels.” Psychonomic Bulletin & Review 29 (6):\n2264–74.\n\n\nCroissant, Yves. 2020. “Estimation of Random Utility Models in\nR: The mlogit Package.”\nJournal of Statistical Software 95 (11): 1–41. https://doi.org/10.18637/jss.v095.i11.\n\n\nFlego, Stefon, and Jon Forrest. 2021. “Leveraging the Temporal\nDynamics of Anticipatory Vowel-to-Vowel Coarticulation in Linguistic\nPrediction: A Statistical Modeling Approach.” Journal of\nPhonetics 88: 101093.\n\n\nGabry, Jonah, and Tristan Mahr. 2024. “Bayesplot: Plotting for\nBayesian Models.” https://mc-stan.org/bayesplot/.\n\n\nGabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and\nAndrew Gelman. 2019. “Visualization in Bayesian Workflow.”\nJournal of the Royal Statistical Society: Series A (Statistics in\nSociety) 182 (2): 389–402.\n\n\nGelman, Andrew. 2006. “Prior Distributions for Variance Parameters\nin Hierarchical Models (Comment on Article by Browne and\nDraper).” Bayesian Analysis 1 (3): 515–34.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data\nanalysis using regression and multilevel/hierarchical\nmodels. Cambridge: Cambridge University Press.\n\n\nGronau, Quentin F., Henrik Singmann, and Eric-Jan Wagenmakers. 2020.\n“bridgesampling: An R\nPackage for Estimating Normalizing Constants.” Journal of\nStatistical Software 92 (10): 1–29. https://doi.org/10.18637/jss.v092.i10.\n\n\nGubian, Michele, Jonathan Harrington, Mary Stevens, Florian Schiel, and\nPaul Warren. 2019. “Tracking the New Zealand English NEAR/SQUARE\nMerger Using Functional Principal Components Analysis.” In\nProceedings of Interspeech 2019, 296–300. https://doi.org/10.21437/Interspeech.2019-2115.\n\n\nHapp, Clara, and Sonja Greven. 2018. “Multivariate Functional\nPrincipal Component Analysis for Data Observed on Different\n(Dimensional) Domains.” Journal of the American Statistical\nAssociation 113: 649–59. https://doi.org/10.1080/01621459.2016.1273115.\n\n\nHapp-Kurz, Clara. 2022. MFPCA: Multivariate Functional Principal\nComponent Analysis for Data Observed on Different Dimensional\nDomains. https://github.com/ClaraHapp/MFPCA.\n\n\nHernan, M. A., and J. M. Robins. 2024. Causal Inference: What\nIf. CRC Press.\n\n\nKay, Matthew. 2023. tidybayes: Tidy Data\nand Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151.\n\n\nKirby, James, and Morgan Sonderegger. 2018. “Mixed-Effects Design\nAnalysis for Experimental Phonetics.” Journal of\nPhonetics 70: 70–85.\n\n\nKruschke, John K. 2018. “Rejecting or Accepting Parameter Values\nin Bayesian Estimation.” Advances in Methods and Practices in\nPsychological Science 1 (2): 270–80.\n\n\nKurz, S. 2023. Statistical Rethinking (Second Edition) with Brms,\nGgplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nLemoine, Nathan P. 2019. “Moving Beyond Noninformative Priors: Why\nand How to Choose Weakly Informative Priors in Bayesian\nAnalyses.” Oikos 128 (7): 912–28.\n\n\nLenth, Russell V. 2024. Emmeans: Estimated Marginal Means, Aka\nLeast-Squares Means. https://CRAN.R-project.org/package=emmeans.\n\n\nLevshina, Natalia. 2015. How to do linguistics\nwith R: Data exploration and statistical analysis. John\nBenjamins.\n\n\nLobanov, Boris. 1971. “Classification of Russian Vowels Spoken by\nDifferent Speakers.” Journal of the Acoustical Society of\nAmerica 49 (2B): 606–8. https://pubs.aip.org/asa/jasa/article-abstract/49/2B/606/747097/Classification-of-Russian-Vowels-Spoken-by.\n\n\nMakowski, Dominique, Mattan S Ben-Shachar, and Daniel Lüdecke. 2019.\n“bayestestR: Describing Effects and Their Uncertainty, Existence\nand Significance Within the Bayesian Framework.” Journal of\nOpen Source Software 4 (40): 1541.\n\n\nMalinsky, Daniel, and David Danks. 2018. “Causal Discovery\nAlgorithms: A Practical Guide.” Philosophy Compass 13\n(1): e12470. https://doi.org/10.1111/phc3.12470.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A\nBayesian Course with Examples in R and\nStan. Second. Chapman; Hall/CRC.\n\n\nNicenboim, B., D. Schad, and S. Vaishth. 2024. An Introduction to\nBayesian Data Analysis for Cognitive Science. https://vasishth.github.io/bayescogsci/book/. 5/2024\nversion.\n\n\nPearl, Judea. 2009. “Causal inference in\nstatistics: An overview.” Statistics Surveys 3:\n96–146. https://doi.org/10.1214/09-SS057.\n\n\nPuggaard-Rode, Rasmus. 2023. “The /t/ Release in Jutland Danish.\nDecomposing the Spectrum with Functional PCA.” In Proceedings\nof the 20th International Congress of Phonetic Sciences, 3262–66.\nPrague: Guarant International. https://rpuggaardrode.github.io/icphs2023/.\n\n\nRamsay, James. 2024. Fda: Functional Data Analysis. https://CRAN.R-project.org/package=fda.\n\n\nReddy, Siva, Diana McCarthy, and Suresh Manandhar. 2011. “An\nEmpirical Study on Compositionality in Compound Nouns.” In\nProceedings of 5th International Joint Conference on Natural\nLanguage Processing, 210–18.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and\nCausation: Graphical Causal Models for Observational Data.”\nAdvances in Methods and Practices in Psychological Science 1\n(1): 27–42. https://doi.org/10.1177/2515245917745.\n\n\nSchad, Daniel J, Bruno Nicenboim, Paul-Christian Bürkner, Michael\nBetancourt, and Shravan Vasishth. 2023. “Workflow Techniques for\nthe Robust Use of Bayes Actors.” Psychological\nMethods 28 (6): 1404–26.\n\n\nSimpson, Gavin L. 2024. gratia: Graceful\nggplot-Based Graphics and Other Functions\nfor GAMs Fitted Using mgcv. https://gavinsimpson.github.io/gratia/.\n\n\nSmith, I., M. Sonderegger, and The SPADE Consortium. 2024. “Modelled Multivariate Overlap: A method for measuring\nvowel merger.” In Proceedings of Interspeech\n2024, 457–61.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic\nData. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.\n\n\nSonderegger, Morgan, Jane Stuart-Smith, and Jeff Mielke. 2023.\n“How Variable Are English Sibilants?” In\nProceedings of the 20th International\nCongress of Phonetic\nSciences, 3196–3200. Prague.\n\n\nSóskuthy, Márton. 2017. “Generalised Additive Mixed Models for\nDynamic Analysis in Linguistics: A Practical Introduction.”\narXiv Preprint arXiv:1703.05339.\n\n\n———. 2021. “Evaluating Generalised Additive Mixed Modelling\nStrategies for Dynamic Speech Analysis.” Journal of\nPhonetics 84: 101017.\n\n\nStanley, Joey. 2020. Joeysvowels: Datasets Based on My Speech.\nhttps://github.com/joeystanley/joeysvowels.\n\n\nTabak, Wieke M, Robert Schreuder, and R Harald Baayen. 2005.\n“Lexical Statistics and Lexical Processing: Semantic Density,\nInformation Complexity, Sex, and Irregularity in Dutch.” In\nLinguistic Evidence, edited by Stephan Kepser and Marga Reis,\n529–56. De Gruyter Mouton.\n\n\nvan Rij, Jacolien, Martijn Wieling, R. Harald Baayen, and Hedderik van\nRijn. 2022. itsadug: Interpreting Time Series and\nAutocorrelated Data Using GAMMs.\n\n\nVehtari, Aki, Jonah Gabry, Måns Magnusson, Yuling Yao, Paul-Christian\nBürkner, Topi Paananen, and Andrew Gelman. 2024. “Loo: Efficient\nLeave-One-Out Cross-Validation and WAIC for Bayesian Models.” https://mc-stan.org/loo/.\n\n\nVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical\nBayesian Model Evaluation Using Leave-One-Out Cross-Validation and\nWAIC.” Statistics and Computing 27: 1413–32. https://doi.org/10.1007/s11222-016-9696-4.\n\n\nWesner, Jeff S, and Justin PF Pomeranz. 2021. “Choosing Priors in\nBayesian Ecological Models by Simulating from the Prior Predictive\nDistribution.” Ecosphere 12 (9): e03739.\n\n\nWinter, Bodo, and Paul-Christian Bürkner. 2021. “Poisson\nRegression for Linguists: A Tutorial Introduction to Modelling Count\nData with Brms.” Language and Linguistics Compass 15\n(11): e12439.\n\n\nWolock, T. M. 2020. “Distributional Regression Models: A Brms\nTutorial.” https://www.tmwolock.com/index.php/2020/12/18/distributional-regression-models-brms-tutorial/.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction\nwith R. Second. Boca Raton, FL: Chapman; Hall/CRC.\n\n\nZhou, Yidong, Han Chen, Su I Iao, Poorbita Kundu, Hang Zhou, Satarupa\nBhattacharjee, Cody Carroll, et al. 2024. fdapace: Functional Data Analysis and Empirical\nDynamics. https://CRAN.R-project.org/package=fdapace.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "week5.html#sec-brm2-extra",
    "href": "week5.html#sec-brm2-extra",
    "title": "4  Bayesian Regression Models 2",
    "section": "4.5 Extra",
    "text": "4.5 Extra\n\n4.5.1 Frequentist models for nonlinear effect of WrittenFrequency\nFit frequentist linear models corresponding to Bayesian models in Section 4.2.1:\n\nenglish_m51_freq &lt;- lm(RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung, data = english_250)\nenglish_m52_freq &lt;- lm(RTlexdec ~ 1 + poly(WrittenFrequency_c, 2) + SubjectYoung, data = english_250)\nenglish_m53_freq &lt;- lm(RTlexdec ~ 1 + poly(WrittenFrequency_c, 3) + SubjectYoung, data = english_250)\nenglish_m54_freq &lt;- lm(RTlexdec ~ 1 + poly(WrittenFrequency_c, 4) + SubjectYoung, data = english_250)\n\nChoose the best model using \\(F\\)-tests, AIC, or BIC for model comparison:\n\n\nanova(english_m51_freq, english_m52_freq, english_m53_freq, english_m54_freq)\n## Analysis of Variance Table\n## \n## Model 1: RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung\n## Model 2: RTlexdec ~ 1 + poly(WrittenFrequency_c, 2) + SubjectYoung\n## Model 3: RTlexdec ~ 1 + poly(WrittenFrequency_c, 3) + SubjectYoung\n## Model 4: RTlexdec ~ 1 + poly(WrittenFrequency_c, 4) + SubjectYoung\n##   Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n## 1    247 1.7321                              \n## 2    246 1.7105  1 0.0216134 3.1191 0.07863 .\n## 3    245 1.6908  1 0.0196451 2.8350 0.09351 .\n## 4    244 1.6908  1 0.0000362 0.0052 0.94247  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAIC(english_m51_freq, english_m52_freq, english_m53_freq, english_m54_freq)\n##                  df       AIC\n## english_m51_freq  4 -525.5652\n## english_m52_freq  5 -526.7044\n## english_m53_freq  6 -527.5923\n## english_m54_freq  7 -525.5977\nBIC(english_m51_freq, english_m52_freq, english_m53_freq, english_m54_freq)\n##                  df       BIC\n## english_m51_freq  4 -511.4794\n## english_m52_freq  5 -509.0971\n## english_m53_freq  6 -506.4636\n## english_m54_freq  7 -500.9474\n\n\nBIC or \\(F\\)-tests: linear model best\nAIC: cubic model best\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlego, Stefon, and Jon Forrest. 2021. “Leveraging the Temporal Dynamics of Anticipatory Vowel-to-Vowel Coarticulation in Linguistic Prediction: A Statistical Modeling Approach.” Journal of Phonetics 88: 101093.\n\n\nGabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. “Visualization in Bayesian Workflow.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 182 (2): 389–402.\n\n\nKurz, S. 2023. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.\n\n\nVehtari, Aki, Jonah Gabry, Måns Magnusson, Yuling Yao, Paul-Christian Bürkner, Topi Paananen, and Andrew Gelman. 2024. “Loo: Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models.” https://mc-stan.org/loo/.\n\n\nVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and WAIC.” Statistics and Computing 27: 1413–32. https://doi.org/10.1007/s11222-016-9696-4.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Regression Models 2</span>"
    ]
  },
  {
    "objectID": "hw3.html",
    "href": "hw3.html",
    "title": "15  Homework 3",
    "section": "",
    "text": "15.1 Preliminaries\nLoad libraries we will need:\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(itsadug)\n\nlibrary(ggeffects)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "hw3.html#dataset",
    "href": "hw3.html#dataset",
    "title": "6  Homework 3",
    "section": "7.1 Dataset",
    "text": "7.1 Dataset\nThis homework uses data from Kirby (2018): abstract here. I processed the raw data from the supplementary materials for the paper to produce the file kirby_if0_spc.csv, which contains spectral measures data reported in Kirby (2018).\nLoad this data:\n\nspc &lt;- read.csv(\"../../data/kirby_if0_spc.csv\", stringsAsFactors = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "hw3.html#context-general",
    "href": "hw3.html#context-general",
    "title": "6  Homework 3",
    "section": "7.2 Context: general",
    "text": "7.2 Context: general\nRelevant context about this data, corresponding to different columns of the dataframe:\n\nEach subject read each item (item), for their native language, three times (repetition)\nEach item is a syllable CV\n\nbeginning in consonant onset, values d l n t tʰ\nV = vowel\ntone = tone for the tone languages (language = Thai, Vietnamese); for the non-tone language Khmer, this is the same as vowel\n\ncontext: words read either in isolation or in a carrier sentence\nt: time, in msec, relative to the onset of the vowel\nf0: F0, in Hz\nst: F0, transformed to semitones\n\nSpeaker normalization: divide by speaker’s mean F0\nTransform to auditory scale: log-base-2-transform and multiply (that value) by 12.\n\n\nThe general question of interest for this data is the relationship between onset and F0, across three languages.\nOf primary interest is differences between onset =\n\ntʰ, t, d: expected to decrease in F0/st (e.g., t &gt; d), called intrinsic F0 effects.\n\nn: included as a sonorant, against which other F0 perturbations from the C (for obstruents) can be measured.\n\nIt is of interest not just whether F0/st differs by onset, but how the trajectory differs as a function of t (time).\nHere are some plots of F0 over time (in semitones) as a function of onset, in a similar format to Kirby (2018), from just the Thai data:\n\n## filtering out timepoints which can't be part of the stop, for clarity\nfilter(spc,language=='Thai' & onset %in% c('d','t','tʰ', 'n') & t&gt;(-50)) %&gt;% ggplot(aes(x=t, y=st)) + geom_smooth(aes(color=onset)) + facet_grid(context~tone)\n## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n## Warning: Removed 66 rows containing non-finite outside the scale range\n## (`stat_smooth()`).\n\n\n\n\n\n\n\n\nPitch is greatly influenced by tone, as we’d expect. It looks like there are differences in F0 trajectory between different onsets, which may be modulated by tone and context.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "hw3.html#context-this-assignment",
    "href": "hw3.html#context-this-assignment",
    "title": "6  Homework 3",
    "section": "7.3 Context: this assignment",
    "text": "7.3 Context: this assignment\n\n\n\n\nKirby, James P. 2018. “Onset Pitch Perturbations and the Cross-Linguistic Implementation of Voicing: Evidence from Tonal and Non-Tonal Languages.” Journal of Phonetics 71: 326–54.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "hw3.html#context",
    "href": "hw3.html#context",
    "title": "15  Homework 3",
    "section": "15.2 Context",
    "text": "15.2 Context\n\n15.2.1 Dataset\nThis homework uses data from Kirby (2018): abstract here. I processed the raw data from the supplementary materials for the paper to produce the file kirby_if0_spc.csv, which contains spectral measures data reported in Kirby (2018).\nLoad this data:\n\nspc &lt;- read.csv(\"../../data/kirby_if0_spc.csv\", stringsAsFactors = TRUE)\n\n\n\n15.2.2 Context: general\nHere is context about this data, corresponding to different columns of the dataframe. You can skip to “Context: this assignment” if you wish, and just come back to this section if you need more detail about the data.\n\nEach subject read each item (item), for their native language, three times (repetition)\nEach item is a syllable CV\n\nbeginning in consonant onset, values d l n t tʰ\nV = vowel\ntone = tone for the tone languages (language = Thai, Vietnamese); for the non-tone language Khmer, this is the same as vowel\n\ncontext: words read either in isolation or in a carrier sentence\nt: time, in msec, relative to the onset of the vowel\nf0: F0, in Hz\nst: F0, transformed to semitones\n\nSpeaker normalization: divide by speaker’s mean F0\nTransform to auditory scale: log-base-2-transform and multiply (that value) by 12.\n\n\nThe general question of interest for this data is the relationship between onset and F0, across three languages.\nOf primary interest is differences between onset =\n\ntʰ, t, d: expected to decrease in F0/st (e.g., t &gt; d), called intrinsic F0 effects.\n\nn: included as a sonorant, against which other F0 perturbations from the C (for obstruents) can be measured.\n\nIt is of interest not just whether F0/st differs by onset, but how the trajectory differs as a function of t (time).\nHere are some plots of F0 over time (in semitones) as a function of onset, in a similar format to Kirby (2018), from just the Thai data:\n\n## filtering out timepoints which can't be part of the stop, for clarity\nfilter(spc, language == \"Thai\" & onset %in% c(\"d\", \"t\", \"tʰ\", \"n\") & t &gt; (-50)) %&gt;% ggplot(aes(x = t, y = st)) +\n  geom_smooth(aes(color = onset)) +\n  facet_grid(context ~ tone)\n\n\n\n\n\n\n\n\nPitch is greatly influenced by tone, as we’d expect. It looks like there are differences in F0 trajectory between different onsets, which may be modulated by tone and context.\n\n\n15.2.3 Context: this assignment\nThe dependent variable is st, which is a speaker-normalized version of f0 (in Hz), transformed to an auditory scale (semitones). To make this data more intuitive, let’s rename and define some variables::\n\nst \\(\\to\\) pitch\ntrajectory: a single pitch trajectory = a unique value of item:context:repetition, for a unique subject\n\n\nspc &lt;- spc %&gt;%\n  mutate(\n    pitch = st,\n    trajectory = interaction(item, repetition, context, subject)\n  ) %&gt;%\n  droplevels()\n\nFor example, here are all pitch trajectories for subject TF1:\n\nspc %&gt;% filter(subject == 'TF1') %&gt;% ggplot(aes(x = t, y = pitch)) + geom_line(aes(group = trajectory, color = onset))\n## Warning: Removed 2 rows containing missing values or values outside the scale range\n## (`geom_line()`).\n\n\n\n\n\n\n\n\nTo keep things simple, we will consider just:\n\nonset = d and th\nThai data\nt &gt; 0 (meaning: pitch measured during the vowel)\n\n\nthai_df &lt;- filter(spc, t &gt;= 0 & onset %in% c(\"d\", \"tʰ\") & language == \"Thai\") %&gt;% droplevels()\n\nSome processing of factors:\n\nthai_df &lt;- thai_df %&gt;% mutate(\n  ## tone: falling, mid, low\n  tone = fct_relevel(tone, \"mid\", after = 1)\n)\n\nThai is a tone language, and it is of interest whether there is any difference in trajectories for each onset. It is possible that tone languages do not show intrinsic F0 effects, or that any effect is variable across contexts and speakers.\nOur research questions are:\n\nPrimary\n\nDo the pitch trajectories differ by onset (between d and tʰ)?\n\n\nThis is largely a question about shape—how large is the onset effect at its maximum (t = 0), and how long does any difference persist into the vowel?\n\nSecondary\n\nIs any onset effect robust? Namely, how much does it differ by tone and context (isolation vs. carrier phrase)?\nDoes the onset effect differ between speakers?\n\n\nIn this homework, you’ll carry out an analysis addressing these research questions.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "hw3.html#part-1",
    "href": "hw3.html#part-1",
    "title": "6  Homework 3",
    "section": "6.3 Part 1",
    "text": "6.3 Part 1\nYou’ll first build and interpret a model without random effects.\n\n6.3.1 Model structure / Exploratory analysis\nTo motivate our model structure, we consider (1) domain knowledge, (2) the experimental design, and (3) exploratory plots. We are primarily interested in time trajectories—smooths of t. Each predictor mentioned above—tone, onset, and context–could affect the height or shape of these trajectories. Any such effect could differ by speaker.\nWe first abstract away from speaker variability to consider “overall” effects of each predictor, motivated by (1) and (2):\n\ntone: expected to affect trajectory height and shape (this is the definition of tone)\nonset: whether it affects trajectory height and shape are required for RQ1\ncontext: expected from previous work to affect trajectory height and shape (words in isolation have different pitch trajectories)\ncontext and tone interactions with onset:this is relevant for RQ2. From previous work: different onset trajectories may be more separated in isolation context, and in high (=high-falling) tone words.\n\nAddressing RQ2 requires at least accounting for context:onset and tone:onset interactions w.r.t trajectory height\nUnclear: trajectory shape.\n\n\nThese observations alone motivate a basic model structure. Let’s also examine (3) empirical plots of the onset effect, by tone and context, as a sanity check:\n\nthai_df %&gt;% \n  ggplot(aes(x=t, y=pitch)) + geom_smooth(aes(color=onset)) + facet_wrap(~context+tone)\n## Note that the scale of y-axis is changed to better show onset effect\n\n\n\n\n\n\n\n\nTrajectory height and shape are clearly affected by context and tone, while the overall effect of onset is unclear (good—this is RQ 1), as is possible modulation of onset by context/tone (RQ 2).\n\n\n6.3.2 A first model\nQuestion 1\n\nWhat is the formula for a GAM model motivated by the discussion above? Your model should contain terms capturing all bullet points except the “Unclear” one.\n\n\nI recommend coding onset, context, and tone so the model contains “difference smoooths”.\n\n\nFit this model, without autocorrelation.\nRe-fit the model accounting for within-trajectory autocorrelation. Save the resulting model as thai_m1.\n\nPrint a summary of this model:\n\nsummary(thai_m1)\n\n\n\n6.3.3 Interpretation\nBecause this model doesn’t include random effects, we shouldn’t perform actual model comparisons / examine its \\(p\\)-values. Let’s instead interpret its results visually:\nAssuming your model contains a term called onset_ord—this code produces a plot of this model’s predicted trajectories for each onset value, marginalizing over tone and context:\n\nggemmeans(thai_m2, terms=c('t', 'onset_ord')) %&gt;% plot()\n\nQuestion 2\n\nMake a single plot showing predicted trajectories by onset, for each (pair of) values of tone and context (like one of the facet plots above). It should also include SEs.1\nPlot the difference curve, for the difference in predicted trajectories between d and th, for each pair of values of tone and context.\n\nTry to get these plots in 6 panels of a single figure, rather than 6 plots printed vertically one after the other.\n\nWhat would you conclude about RQs 1-2, just based on these plots, plus the one immediately above (which used ggemmeans())?\n\nExercise\n\nMake one or more plots visualizing by-subject variation in trajectories, showing trajectories as a function of at least subject and onset (of primary iterest). To think about: should you average over context and tone, or show these in the plot?\n\nThe plot should suggest at least inter-speaker differences in:\n\nTrajectory height and shape\nDifferences in trajectory height and shape, by onset\n\n\nDo your plot(s) suggest other potential inter-speaker differences that are important to control for for the research questions? Can you think of others which may matter, which should be checked by plotting in a more thorough analysis?\n\nIn terms of our model: because our primary RQ involves the effect of onset, we’ll at least need to include random-effect terms allowing speakers to differ in this effect. This is also crucial to address RQ3.\nAs the effects of context and tone are secondary RQs, it’s less important to account for by-speaker variability, but you’re welcome to do so if your answer to the exercise above suggests it’s a good idea.\n\n\n\n\nKirby, James P. 2018. “Onset Pitch Perturbations and the Cross-Linguistic Implementation of Voicing: Evidence from Tonal and Non-Tonal Languages.” Journal of Phonetics 71: 326–54.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "hw3.html#footnotes",
    "href": "hw3.html#footnotes",
    "title": "15  Homework 3",
    "section": "",
    "text": "Note that Thai “high” tone is phoentically “high-falling”, corresponding to the falling level of tone.↩︎\nHint: you’ll need something different from the ggemmeans command above.↩︎",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "hw3.html#part-1-gam",
    "href": "hw3.html#part-1-gam",
    "title": "15  Homework 3",
    "section": "15.3 Part 1: GAM",
    "text": "15.3 Part 1: GAM\nYou’ll first build and interpret a model without random effects.\nThis is not a “good” model—for any data with repeated measures (here, by-subject), a valid model needs random effects. However, such a model is often considered as a first step towards a “real” GAMM model. This homework gives practice with this method of “building up” a model for repeated-measures data.\n\n15.3.1 Model structure / Exploratory analysis\nTo motivate our model structure, we consider (1) domain knowledge, (2) the experimental design, and (3) exploratory plots. We are primarily interested in time trajectories—smooths of t. Each predictor mentioned above—tone, onset, and context–could affect the height or shape of these trajectories. Any such effect could differ by speaker.\nWe first abstract away from speaker variability to consider “overall” effects of each predictor, motivated by (1) and (2):\n\ntone: expected to affect trajectory height and shape (this is the definition of tone).\nonset: whether it affects trajectory height and shape are required for RQ1.\ncontext: expected from previous work to affect trajectory height and shape (words in isolation have different pitch trajectories).\ncontext and tone interactions with onset:this is relevant for RQ2.\n\nFrom previous work: different onset trajectories may be more separated in isolation context, and in high tone words.1\nAddressing RQ2 requires at least accounting for context:onset and tone:onset interactions w.r.t trajectory height\nUnclear: trajectory shape.\n\n\nThese observations alone motivate a basic model structure. Let’s also examine (3) empirical plots of the onset effect, by tone and context, as a sanity check:\n\nthai_df %&gt;%\n  ggplot(aes(x = t, y = pitch)) +\n  geom_smooth(aes(color = onset)) +\n  facet_wrap(~ context + tone)\n\n\n\n\n\n\n\n\nTrajectory height and shape are clearly affected by context and tone, while the overall effect of onset is unclear (good—this is RQ 1), as is possible modulation of onset by context/tone (RQ 2).\n\n\n15.3.2 A first model\nQuestion 1\n\nWhat is the formula for a GAM model motivated by the discussion above? Your model should contain terms capturing all bullet points except the “Unclear” one.\n\n\nI recommend coding onset, context, and tone so the model contains “difference smoooths”.\nNote that you do not need to allow the smooths to vary by the interaction of context:onset or tone:onset. You do need to allow the height of the smooths to vary by these interactions.\n\n\nFit this model, without autocorrelation.\nRe-fit the model accounting for within-trajectory autocorrelation. Save the resulting model as thai_m1.\n\nPrint a summary of this model:\n\nsummary(thai_m1)\n\n\n\n15.3.3 Interpretation\nBecause this model doesn’t include random effects, we shouldn’t perform actual model comparisons / examine its \\(p\\)-values. Let’s instead interpret its results visually:\nAssuming your model contains a term called onset_ord—this code produces a plot of this model’s predicted trajectories for each onset value, marginalizing over tone and context:\n\nggemmeans(thai_m2, terms = c(\"t\", \"onset_ord\")) %&gt;% plot()\n\nQuestion 2\n\nMake a single plot showing predicted trajectories by onset, for each (pair of) values of tone and context (like one of the facet plots above). It should also include SEs.2\nPlot the difference curve, for the difference in predicted trajectories between d and th, for each pair of values of tone and context.\n\n\nTry to get these plots in 6 panels of a single figure, rather than 6 plots printed vertically one after the other.\n\n\nWhat would you conclude about RQs 1-2, just based on these plots, plus the one immediately above (which used ggemmeans())?",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "hw3.html#exploratory-analysis",
    "href": "hw3.html#exploratory-analysis",
    "title": "6  Homework 3",
    "section": "7.1 Exploratory analysis",
    "text": "7.1 Exploratory analysis\nIn terms of our model: to address RQs 1 and 3, I claim that we need to minimally include random-effect terms capturing the following:\n\nSpeaker variation in trajectory height\nSpeaker variation in trajectory shape\nSpeaker variation in differences in trajectory height and shape by onset\n\n(Note that this may not mean three distinct random-effect terms.)\nQuestion 3\n\nList the random-effect terms that need to be added to model thai_m1 to capture these three types of speaker variation.\nExplain why each is necessary to address RQs 1 and 3.\n\nThere are many other types of by-subject variation that could be accounted for in the model, but all are relevant for a secondary RQ (RQ 2) or for no RQ. It is often not computationally feasible for GAMMs to build a “maximal” model with all possible random effects (as for a GLMM). Instead, we make some plots to see if there are any other important sources of by-subject variation.\nQuestion 4\n\nMake one or more plots visualizing by-subject variation in trajectories, as a function of onset, context, and tone.\n\n\n\nTo think about: should you examine one predictor at a time, or show them all varying on one plot? If the former, what do you do about the other predictors (e.g. context, tone) when plotting how the effect of another (e.g. onset) differs by subject\n\nThe plot(s) should suggest at least inter-speaker differences in:\n\nTrajectory height and shape\nDifferences in trajectory height and shape, by onset\n\nWe’ve already decided that we’re including terms in the model below to account for these.\n\nDo your plot(s) suggest other large by-subject differences that we could account for? If so, which random-effect terms should be added to the model? If not, justify your answer. (That is, why doesn’t this matter for our research questions?)\n\n\n\n7.1.1 Model\nQuestion 5\n\nFit GAMM model thai_m2, which is the same as thai_m1 but with the random-effect terms added that were motivated in Questions 3–4.\n\n\n\n\nPrint a summary of this model:\n\n\nsummary(thai_m2)\n\n\n\n7.1.2 Interpretation\nWe can interpret the results of this GAMM visually (as in Question 2), and quantitatively.\nQuestion 6\n\nRepeat Question 2, for model thai_m2. (I encourage you to compare the results to Question 2, but you don’t need to write this up. thai_m2 is a superior model.)\nCarry out a model comparison, or consider relevant row(s) of the model table, that answers RQ 1. Briefly interpret the result (what does it mean, in words?). [1-2 sentences]\nPerform a hypothesis test, via model comparison, that addresses RQ3. Use this result, and your plot(s) from Question 4(a), to answer RQ3. [~2-3 sentences]\n\n\n\n\n\nKirby, James P. 2018. “Onset Pitch Perturbations and the Cross-Linguistic Implementation of Voicing: Evidence from Tonal and Non-Tonal Languages.” Journal of Phonetics 71: 326–54.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "hw3.html#part-2-gamm",
    "href": "hw3.html#part-2-gamm",
    "title": "15  Homework 3",
    "section": "15.4 Part 2: GAMM",
    "text": "15.4 Part 2: GAMM\nLet’s now consider by-speaker variability. This will motivate the random effects to add to model thai_m1 to make a GAMM.\n\n15.4.1 Exploratory analysis\nIn terms of our model: to address RQs 1 and 3, I claim that we need to minimally include random-effect terms capturing the following:\n\nSpeaker variation in trajectory height\nSpeaker variation in trajectory shape\nSpeaker variation in differences in trajectory height and shape by onset\n\n(Note that this may not mean three distinct random-effect terms.)\nQuestion 3\n\nList the random-effect terms that need to be added to model thai_m1 to capture these three types of speaker variation.\nExplain why each is necessary to address RQs 1 and 3.\n\nThere are many other types of by-subject variation that could be accounted for in the model, but all are relevant for a secondary RQ (RQ 2) or for no RQ. It is often not computationally feasible for GAMMs to build a “maximal” model with all possible random effects (as for a GLMM). Instead, we make some plots to see if there are any other important sources of by-subject variation.\nQuestion 4\n\nMake one or more plots visualizing by-subject variation in trajectories, as a function of onset, context, and tone.\n\n\n\nTo think about: should you examine one predictor at a time, or show them all varying on one plot? If the former, what do you do about the other predictors (e.g. context, tone) when plotting how the effect of another (e.g. onset) differs by subject\n\nThe plot(s) should suggest at least inter-speaker differences in:\n\nTrajectory height and shape\nDifferences in trajectory height and shape, by onset\n\nWe’ve already decided that we’re including terms in the model below to account for these.\n\nDo your plot(s) suggest other large by-subject differences that we could account for? If so, which random-effect terms should be added to the model? If not, justify your answer. (That is, why doesn’t this matter for our research questions?)\n\n\n\n\n15.4.2 Model\nQuestion 5\n\nFit GAMM model thai_m2, which is the same as thai_m1 but with the random-effect terms added that were motivated in Questions 3–4.\n\n\n\n\nPrint a summary of this model:\n\n\nsummary(thai_m2)\n\n\n\n15.4.3 Interpretation\nWe can interpret the results of this GAMM visually (as in Question 2), and quantitatively.\nQuestion 6\n\nRepeat Question 2, for model thai_m2. (I encourage you to compare the results to Question 2, but you don’t need to write this up. thai_m2 is a superior model.)\nCarry out a model comparison, or consider relevant row(s) of the model table, that answers RQ 1. Briefly interpret the result (what does it mean, in words?). [1-2 sentences]\nPerform a hypothesis test, via model comparison, that addresses RQ3. Use this result, and your plot(s) from Question 4(a), to answer RQ3. [~2-3 sentences]\n\n\n\n\n\nKirby, James P. 2018. “Onset Pitch Perturbations and the Cross-Linguistic Implementation of Voicing: Evidence from Tonal and Non-Tonal Languages.” Journal of Phonetics 71: 326–54.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Homework 3</span>"
    ]
  },
  {
    "objectID": "week9.html",
    "href": "week9.html",
    "title": "5  Bayesian Hierarchical Models 1",
    "section": "",
    "text": "5.1 Preliminaries\nLoad libraries we will need:\nlibrary(brms)\nlibrary(lme4)\nlibrary(arm)\nlibrary(tidyverse)\n\nlibrary(tidybayes)\nlibrary(bayestestR)\n\nlibrary(bayesplot)\nlibrary(loo)\n\nlibrary(broom) ## for tidy model summaries\nlibrary(broom.mixed) ## for tidy model summaries for lme4 models\n\nlibrary(patchwork)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bayesian Hierarchical Models 1</span>"
    ]
  },
  {
    "objectID": "week9.html#preliminaries",
    "href": "week9.html#preliminaries",
    "title": "5  Bayesian Hierarchical Models 1",
    "section": "",
    "text": "Practical notes\n\n\n\n\n\n\nIf you have loaded rethinking, you need to detach it before using brms. See Kurz (2023) Sec. 4.3.1.\nI use the file argument when fitting brms models to make compiling this document easier (so the models don’t refit every time I compile). You may or may not want to do this for your own models. See file and file_refit arguments in ?brm.\nHere I set the file_refit option so “brms will refit the model if model, data or algorithm as passed to Stan differ from what is stored in the file.”\n\n\noptions(brms.file_refit = \"on_change\")\n\n\nI use chains = 4, cores = 4 when fitting brm models below—this means 4 chains, each to be run on one core on my laptop. cores = 4 may need to be adjusted for your computer. (You may have fewer or more cores; I have 8 cores, so this leaves 50% free.) You should figure out how to use multiple cores on your machine. Starting today, we are fitting complex-enough models that this matters.\n\n\n\n\n\n5.1.1 Data\nLoad the diatones dataset and perform some data cleaning and recoding (see Section 1.1, Section 3.1.1):\n\ndiatones &lt;- read.csv(\"https://osf.io/tqjm8/download\", stringsAsFactors = TRUE)\n\n# make numeric versions of all categorical predictors, while saving original versions\ndiatones &lt;- diatones %&gt;% mutate(\n  syll1_coda_orig = syll1_coda,\n  syll2_coda_orig = syll2_coda,\n  syll2_td_orig = syll2_td,\n  ## turns no/yes -&gt; 0/1\n  syll1_coda = ifelse(syll1_coda == \"no\", 0, 1),\n  ## turns '0'/'C'/'CC'/'CCC' -&gt; 0/1/2/3\n  syll2_coda = str_count(syll2_coda_orig, \"C\"),\n  syll2_td = ifelse(syll2_td == \"no\", 0, 1)\n)\n\n## standardize all predictors using arm::rescale\ndiatones &lt;- diatones %&gt;% mutate(\n  syll1_coda = rescale(syll1_coda_orig),\n  syll2_td = rescale(syll2_td_orig),\n  syll2_coda = rescale(syll2_coda),\n  frequency = rescale(frequency)\n)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bayesian Hierarchical Models 1</span>"
    ]
  },
  {
    "objectID": "week9.html#a-first-mixed-effects-model-random-intercepts",
    "href": "week9.html#a-first-mixed-effects-model-random-intercepts",
    "title": "5  Bayesian Hierarchical Models 1",
    "section": "5.2 A first mixed-effects model: random intercepts",
    "text": "5.2 A first mixed-effects model: random intercepts\nMcElreath makes a compelling case, similar to Gelman and Hill (2007), for using “varying intercept”, “varying slope”, etc., instead of “random intercept”, etc. I will nonetheless use random intercept/slope terminology, at least at first, to make the connections with (frequentist) mixed-effects models clearer.\nFor the diatones data, words (individual observations) with the same prefix (column prefix) may not be independent, and it is reasonable to consider models with a by-prefix random intercept. (See discussion in RMLD Sec. 9.2.) To illustrate, here is an empirical plot showing the proportion of words with shifted stress by prefix:\n\n\nCode\ndiatones %&gt;% ggplot(aes(x = prefix, y = stress_shifted)) +\n  stat_summary(fun.data = \"mean_cl_boot\") +\n  labs(x = \"Prefix\", y = \"Proportion with shifted stress\")\n## Warning: Removed 1 row containing missing values or values outside the scale range\n## (`geom_segment()`).\n\n\n\n\n\n\n\n\n\nNote that the prefixes with average 0 or 1 have a small number of observations, but (except for “pre”) not just 1:\n\ndiatones %&gt;% count(prefix)\n##    prefix  n\n## 1      aC 20\n## 2    comn  9\n## 3      de 16\n## 4     dis 17\n## 5      eC  8\n## 6      ex  5\n## 7      iN  3\n## 8     mis  2\n## 9     pre  1\n## 10    pro  2\n## 11     re 42\n## 12    sup  2\n## 13    sur  3\n\nThe simplest possible frequentist mixed-effects model in this case would be this logistic regression:\n\ndiatones_freq_m81 &lt;- glmer(\n  stress_shifted ~ 1 + (1 | prefix), \n  data = diatones, \n  family = \"binomial\"\n  )\n## Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n## Model failed to converge with max|grad| = 0.0236227 (tol = 0.002, component 1)\n\nModel summary:\n\ntidy(diatones_freq_m81)\n## # A tibble: 2 × 7\n##   effect   group  term            estimate std.error statistic p.value\n##   &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 fixed    &lt;NA&gt;   (Intercept)        -1.49   0.00298     -501.       0\n## 2 ran_pars prefix sd__(Intercept)     1.08  NA             NA       NA\n\nMore verbose:\n\nsummary(diatones_freq_m81)\n## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: binomial  ( logit )\n## Formula: stress_shifted ~ 1 + (1 | prefix)\n##    Data: diatones\n## \n##      AIC      BIC   logLik deviance df.resid \n##    117.6    123.3    -56.8    113.6      128 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -0.7845 -0.3961 -0.3826 -0.3708  2.6967 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  prefix (Intercept) 1.176    1.084   \n## Number of obs: 130, groups:  prefix, 13\n## \n## Fixed effects:\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -1.493044   0.002983  -500.5   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## optimizer (Nelder_Mead) convergence code: 0 (OK)\n## Model failed to converge with max|grad| = 0.0236227 (tol = 0.002, component 1)\n\nThe fitted parameters are:\n\n\\(\\beta_0\\) : overall intercept (log-odds of shifting stress)\n\\(\\sigma_{p}\\): SD of the by-prefix random intercept\n\nWe can also extract the random intercept for each prefix (its offset from \\(\\beta_0\\), in log-odds):\n\nranef(diatones_freq_m81)$prefix\n##      (Intercept)\n## aC   -0.49104599\n## comn  0.15400282\n## de   -0.31000610\n## dis  -0.35914303\n## eC   -0.83525861\n## ex    0.05129309\n## iN    1.63689589\n## mis  -0.32766047\n## pre  -0.18499887\n## pro   1.29292642\n## re   -0.42822641\n## sup  -0.32766047\n## sur   1.00763985\n\nSome notes:\n\nThere is a convergence issue.\n\n\nThis can be easily solved by changing the optimizer, but it’s typical of glmer() that even this very simple model doesn’t fit correctly with default settings.\n\n\nThere is an estimate \\(\\hat{\\sigma}_{p}\\), but no SE.\nThere are estimates of the random effects, but no SEs.\n\nWe could add SEs for 2-3 (see RMLD Sec. TODO), but this is not the default, for good reasons—extra computation time, approximations which may not always be appropriate.\nAll of these issues are solved for free in a Bayesian model!1\n\nTo fit the ayesian version, we just need priors on \\(\\beta_0\\) and \\(\\sigma_{p}\\). Let’s use the same weakly informative priors as McElreath (2020) Sec. 13.2.1. The probability model for \\(y_i\\) (column stress_shifted) is then:\n\\[\\begin{align}\ny_i & \\sim \\text{Binomial}(1, p_i) \\\\\n\\text{logit}(p_i) & = \\beta_0 + \\alpha_{\\text{prefix}[i]} \\\\\n\\alpha_j & \\sim N(0, \\sigma),  \\\\\n\\beta_0 & \\sim N(0, 1.5) \\\\\n\\sigma & \\sim \\text{Exponential}(1),\n\\end{align}\\]\nwhere \\(i=1, \\ldots n\\), the number of observations, and \\(j = 1 \\ldots, n_{prefix}\\), the number of prefixes.\nThese are weakly-informative priors for \\(\\beta_0\\) and \\(\\sigma\\).2\nFit this model in brms:\n\n## so you get the same 'random' result\nset.seed(5)\n\ndiatones_m81 &lt;- brm(\n  data = diatones,\n  family = binomial,\n  stress_shifted | trials(1) ~ 1 + (1 | prefix),\n  prior = c(\n    prior(normal(0, 1.5), class = Intercept), ## beta_0\n    prior(exponential(1), class = sd) ## sigma\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  file = \"models/diatones_m81.brm\"\n)\n## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n\n(Note the notation for the random intercept is the same as glmer().)\nModel summary:\n\ndiatones_m81\n##  Family: binomial \n##   Links: mu = logit \n## Formula: stress_shifted | trials(1) ~ 1 + (1 | prefix) \n##    Data: diatones (Number of observations: 130) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~prefix (Number of levels: 13) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     1.13      0.66     0.08     2.61 1.00      935     1412\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -1.40      0.45    -2.25    -0.41 1.00     1458     1736\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nExamine trace plots/posterior:\n\nplot(diatones_m81)\n\n\n\n\n\n\n\n\nThese look OK, though we might wonder whether the \\(\\sigma\\) posterior needs more iterations. Running the model for twice as long (left as an exercise) gives a similar-looking posterior, so we’ll stick with this model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\hat{R}\\) and ESS for the parameters shown look satisfactory.\nThe model summary shows the parameters \\(\\beta_0\\) and \\(\\sigma\\), whose estimates and 95% CIs are similar (but not identical) to the frequentist model (diatones_freq_m81):\n\nsummary(diatones_m81)\n##  Family: binomial \n##   Links: mu = logit \n## Formula: stress_shifted | trials(1) ~ 1 + (1 | prefix) \n##    Data: diatones (Number of observations: 130) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~prefix (Number of levels: 13) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     1.13      0.66     0.08     2.61 1.00      935     1412\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    -1.40      0.45    -2.25    -0.41 1.00     1458     1736\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe model also fits each \\(\\alpha_j\\), the random intercepts, but these are not shown by default.\n\n### list variables in the model\nvariables(diatones_m81)\n##  [1] \"b_Intercept\"              \"sd_prefix__Intercept\"    \n##  [3] \"Intercept\"                \"r_prefix[aC,Intercept]\"  \n##  [5] \"r_prefix[comn,Intercept]\" \"r_prefix[de,Intercept]\"  \n##  [7] \"r_prefix[dis,Intercept]\"  \"r_prefix[eC,Intercept]\"  \n##  [9] \"r_prefix[ex,Intercept]\"   \"r_prefix[iN,Intercept]\"  \n## [11] \"r_prefix[mis,Intercept]\"  \"r_prefix[pre,Intercept]\" \n## [13] \"r_prefix[pro,Intercept]\"  \"r_prefix[re,Intercept]\"  \n## [15] \"r_prefix[sup,Intercept]\"  \"r_prefix[sur,Intercept]\" \n## [17] \"lprior\"                   \"lp__\"\n\nThese also have posteriors, 95% CIs, trace plots, Rhat values, and so on:\n\n### posterior summary for *all* parameters, with 95% CIs:\nposterior_summary(diatones_m81)\n##                             Estimate Est.Error         Q2.5       Q97.5\n## b_Intercept               -1.3992381 0.4547829  -2.24559522  -0.4074337\n## sd_prefix__Intercept       1.1252789 0.6643871   0.08220781   2.6101612\n## Intercept                 -1.3992381 0.4547829  -2.24559522  -0.4074337\n## r_prefix[aC,Intercept]    -0.6072764 0.7253027  -2.29208968   0.5082547\n## r_prefix[comn,Intercept]  -0.0227525 0.7372341  -1.67179589   1.3967454\n## r_prefix[de,Intercept]    -0.4273092 0.7174684  -2.05782585   0.8149659\n## r_prefix[dis,Intercept]   -0.4820494 0.7081390  -2.11648816   0.6998360\n## r_prefix[eC,Intercept]    -1.0446985 1.1742041  -3.93323094   0.5604813\n## r_prefix[ex,Intercept]    -0.1219485 0.8701621  -2.05991353   1.5294766\n## r_prefix[iN,Intercept]     1.5638417 1.3224835  -0.19889598   4.6590338\n## r_prefix[mis,Intercept]   -0.5337148 1.1617605  -3.45925037   1.2941277\n## r_prefix[pre,Intercept]   -0.3174477 1.1580880  -3.14138694   1.6713924\n## r_prefix[pro,Intercept]    1.2681752 1.2762139  -0.36068743   4.3544651\n## r_prefix[re,Intercept]    -0.5189332 0.6055688  -1.84715869   0.4890578\n## r_prefix[sup,Intercept]   -0.5119821 1.1756984  -3.36394599   1.3591041\n## r_prefix[sur,Intercept]    0.8458484 0.9662156  -0.65081485   3.0776287\n## lprior                    -2.9307143 0.6756104  -4.48891174  -1.9313278\n## lp__                     -74.2178808 4.2487196 -83.57989256 -67.2042423\n\n### show just random effect posteriors using \n## regex to choose just parameters starting with \n## r_\npost_ranef &lt;- as_draws_df(diatones_m81, regex = \"^r_\")\n\n\n### posterior densities\nmcmc_dens(post_ranef, regex = \"^r_\")\n\n### trace plot\nmcmc_trace(post_ranef, regex = \"^r_\")\n\n## for all parameters in the model:\n\n## Rhat plot\nmcmc_plot(diatones_m81, type = \"rhat\") +\n## adds parameter names on y-axis\nyaxis_text(hjust = 1)\n\n## N_eff plot\nmcmc_plot(diatones_m81, type = \"neff\") + yaxis_text(hjust = 1)  \n#neff_ratio(diatones_m81) %&gt;% mcmc_neff(size = 2, regex = \"^r_\") + yaxis_text(hjust = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese plots, together with posterior + trace plots above, suggest our model is decent:\n\n\\(\\hat{R}\\) always very near 1\nNo posteriors look bizarre – all at least roughly unimodal, smooth.\nTrace plots look fine.\n(And: Sampling not very inefficient for any parameter.)\n\nIn this model there is only one random-effect term, with few levels. In a more realistic model there are hundreds or thousands of random effect terms. Typically it is infeasible to actually examine posterior plots, etc., for all terms, so we rely on diagnostics like checking if any parameter has \\(\\hat{R}&gt;&gt;1\\).\nWe’d do this with plots like the following:\n\n\nCode\np1 &lt;- mcmc_plot(diatones_m81, type = \"rhat\")\np2 &lt;- mcmc_plot(diatones_m81, type = \"neff\")\n\np1 / p2\n\n\n\n\n\n\n\n\n\nNote that the posteriors for individual random effects look different from what we’re used to—many don’t look normal, and several look highly skewed. This is fine, and in fact makes sense: there is no reason these parameters need to have normal posterior distributions, and it is expected that some posteriors will be skewed, which is an effect of “partial pooling”.\n\nExercise 5.1 This code gets draws from the posterior of the predicted log-odds of shifted_stress for words with prefix de, as column pred_logit:\n\ndiatones_m81 %&gt;%\n  spread_draws(`r_prefix[de,Intercept]`, b_Intercept) %&gt;%\n  mutate(pred_logit = `r_prefix[de,Intercept]` + b_Intercept)\n## # A tibble: 4,000 × 6\n##    .chain .iteration .draw `r_prefix[de,Intercept]` b_Intercept pred_logit\n##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;                    &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n##  1      1          1     1                  -0.129       -1.46      -1.59 \n##  2      1          2     2                  -0.749       -1.59      -2.34 \n##  3      1          3     3                   0.0717      -1.49      -1.42 \n##  4      1          4     4                  -0.528       -1.31      -1.84 \n##  5      1          5     5                   0.380       -1.18      -0.800\n##  6      1          6     6                  -0.590       -1.77      -2.36 \n##  7      1          7     7                  -3.71        -1.00      -4.72 \n##  8      1          8     8                  -0.353       -0.800     -1.15 \n##  9      1          9     9                  -1.14        -0.909     -2.05 \n## 10      1         10    10                  -0.385       -1.12      -1.50 \n## # ℹ 3,990 more rows\n\n\nExplain how this works: what two parameters are added to make up pred_logit?\nUse these draws to plot the posterior of the probability of shifted_stress for words with prefix de.\nExtra: Make a similar plot, with one panel per prefix. That is, first each facet/panel should show the posterior of the probability of shifted_stress for words with a different prefix (levels of prefix). (Hint: read more about tidybayes notation.)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bayesian Hierarchical Models 1</span>"
    ]
  },
  {
    "objectID": "week9.html#footnotes",
    "href": "week9.html#footnotes",
    "title": "5  Bayesian Hierarchical Models 1",
    "section": "",
    "text": "With the important caveat that “convergence” may just mean “result more influenced by the prior”, in cases where glmer() is less likely to converge.↩︎\n\\(\\beta_0\\) has most probability mass in log-odds of -4 to 4 = 0.02-0.98 probability. See the end of Kurz (2023) Sec. 13.1 for a plot of the \\(\\sigma\\) prior.↩︎\nHint: examples in ?conditional_effects.↩︎\nHint: Kurz 13.5.1↩︎\nHint: examine the posterior for iN’s random effect, and look how common this prefix is.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bayesian Hierarchical Models 1</span>"
    ]
  },
  {
    "objectID": "week9.html#adding-predictors",
    "href": "week9.html#adding-predictors",
    "title": "7  Bayesian Hierarchical Models 1",
    "section": "7.3 Adding predictors",
    "text": "7.3 Adding predictors\nLet’s now fit a realistic model of the diatones data, including predictors—this is the frequentist model from RMLD Sec. 9.2.1. There are now several `fixed-effect’ predictors.\nFor priors we will use:\n\nIntercept: \\(N(0,5)\\)\n\nsame as the diatones_m41 model from Section 3.4.1.\n\nEach \\(\\beta_i\\): \\(N(0,3)\\):\n\nsame\n\nRandom intercept variance: \\(\\text{Exponential}(1)\\), as above.\n\nHere we are using essentially “flat” priors, just to keep things simpler for the moment (we don’t have to decide on “weakly informative” values).\n\ndiatones_m82 &lt;- brm(\n  data = diatones,\n  stress_shifted | trials(1) ~ syll2_coda + syll2_td + frequency + syll1_coda + syll2_td:frequency + syll1_coda:frequency + (1 | prefix),\n  family = binomial,\n  prior = c(\n    prior(normal(0, 5), class = Intercept), # beta_0\n    prior(normal(0, 3), class = b), # beta_i\n    prior(exponential(1), class = sd) # sigma\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  file = \"models/diatones_m82.brm\"\n)\n## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n\nModel summary:\n\ndiatones_m82\n## Warning: There were 1 divergent transitions after warmup. Increasing\n## adapt_delta above 0.8 may help. See\n## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n##  Family: binomial \n##   Links: mu = logit \n## Formula: stress_shifted | trials(1) ~ syll2_coda + syll2_td + frequency + syll1_coda + syll2_td:frequency + syll1_coda:frequency + (1 | prefix) \n##    Data: diatones (Number of observations: 130) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~prefix (Number of levels: 13) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     1.39      0.86     0.07     3.32 1.00      724     1142\n## \n## Regression Coefficients:\n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## Intercept               -2.16      0.62    -3.47    -0.89 1.00     1426\n## syll2_coda              -1.73      0.75    -3.29    -0.32 1.00     3104\n## syll2_td                 1.48      0.71     0.12     2.91 1.00     2630\n## frequency               -0.80      0.60    -2.05     0.34 1.00     2917\n## syll1_coda               1.99      0.98     0.16     4.14 1.00     1724\n## syll2_td:frequency       2.43      1.17     0.28     4.90 1.00     3225\n## frequency:syll1_coda     3.21      1.51     0.27     6.25 1.00     3333\n##                      Tail_ESS\n## Intercept                1101\n## syll2_coda               2850\n## syll2_td                 2922\n## frequency                2574\n## syll1_coda               2329\n## syll2_td:frequency       2610\n## frequency:syll1_coda     2839\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nExercise 7.2 Examine some diagnostics for this model, just for the parameters shown by summary(). Do you see any sign that the model hasn’t “converged” (good posterior sample for all parameters)? If so, re-fit the model with more iterations.\n\nSolution:\n\n\nCode\n## yes, you need more iterations\ndiatones_m82 &lt;- brm(\n  data = diatones,\n  stress_shifted | trials(1) ~ syll2_coda + syll2_td + frequency + syll1_coda + syll2_td:frequency + syll1_coda:frequency + (1 | prefix),\n  family = binomial,\n  prior = c(\n    prior(normal(0, 5), class = Intercept), # beta_0\n    prior(normal(0, 3), class = b), # beta_i\n    prior(exponential(1), class = sd) # sigma\n  ),\n  iter = 5000, warmup = 2500, chains = 4, cores = 4,\n  file = \"models/diatones_m82.brm\"\n)\n## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n\n\n\n\nCode\nsummary(diatones_m82)\n\n## (We'll return to the \"divergent transitions\" message below.)\n\n\n\n7.3.1 Model comparison\nWe can ask whether adding the random intercept term is justified by comparing to a model without this term, using PSIS (“LOO”), from Section 4.2 :\n\ndiatones_m83 &lt;- brm(\n  data = diatones,\n  stress_shifted | trials(1) ~ syll2_coda + syll2_td + frequency + syll1_coda + syll2_td:frequency + syll1_coda:frequency,\n  family = binomial,\n  prior = c(\n    prior(normal(0, 5), class = Intercept), # beta_0\n    prior(normal(0, 3), class = b) # beta_i\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  file = \"models/diatones_m83.brm\"\n)\n\n\ndiatones_m82 &lt;- add_criterion(diatones_m82, c(\"loo\"))\ndiatones_m83 &lt;- add_criterion(diatones_m83, c(\"loo\"))\nloo_compare(diatones_m82, diatones_m83)\n##              elpd_diff se_diff\n## diatones_m82  0.0       0.0   \n## diatones_m83 -1.7       2.2\n\nDepending on the model selection criterion we use—lowest PSIS, versus lowest PSIS up to 95% CredI—either model could be preferred. (See the end of Section 4.2 for discussion.)\n\n\n\n7.3.2 Plotting effects\nIt’s always an option to:\n\nDefine your own dataframe, of values to be predicted at\nGet predictions with 95% CIs or PI%s, using predict or fitted from brms.\n\nSome decisions need to be made to make marginal effect plots, especially about what uncertainty to show,and this workflow lets you make the decisions. You can also use pre-made functions. There is excellent functionality in brms::conditional_effects and bayesplot—see the vignette for the latter (“fit/prediction curves”).\nExample: marginal effect of frequency, averaging over other predictors:\n\nconditional_effects(diatones_m82, effects = \"frequency\")\n\n\n\n\n\n\n\n\nPredictions here are for an “average prefix”, with CIs for the expected value of the PDF – see posterior_epred. (Analogous to brms fitted.) These are on the probability scale; to get predictions on the log-odds scale, you’d need to use posterior_linpred.\nYou can alternatively get PIs from the posterior predictive distribution, which will give a larger interval. (Analogous to brms predict, see ?posterior_predict.) This won’t do something sensible for the current example:\n\nconditional_effects(diatones_m82, effects = \"frequency\", method = \"posterior_predict\")\n\n\n\n\n\n\n\n\n(An exercise below is to try this for a linear regression model)\nExercises\n\nMake plots visualizing the syll1:frequency and syll2_td:frequency interactions, like RMLD Fig. 9.4. (It’s OK if your figures don’t treat syll1_coda and syll2_td as factors,or maybe you can figure out how to do this.) Hint: examples in ?conditional_effects.\n\n\n\n\n\nYou should have found that the plot looks similar to the plot for the frequentist model. Let’s use the bayesian model to make a prediction plot that would be much harder, if possible at all, using the frequentist model: the frequency effect just for words with prefix = iN.\n\nMake a dataframe of new values to predict at:\n\nprefix=iN\nsyll2_coda=syll2_td=0 (held at average values)\nsyll1_coda=-0.22 (recall: syll1_coda is a prefix-level predictor; this is its value for prefix iN)\nLet frequency range between -1.5 and 1.5 (meaning +-3 SD of frequency before it was standarized).\n\nUse fitted to get predictions with 95% CIs.\nPlot these predictions, to make a plot like the one above with frequency on the \\(x\\)-axis. (Hint: Kurz 13.5.1)\nYou should find that the plot looks different from the frequency plot above, both in terms of its mean and the width of the CI. Why is this? (Hint: examine the posterior for iN’s random effect, and look how common this prefix is.)\n\n\n\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data analysis using regression and multilevel/hierarchical models. Cambridge: Cambridge University Press.\n\n\nKurz, S. 2021. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Hierarchical Models 1</span>"
    ]
  },
  {
    "objectID": "week5.html#sec-model-quality",
    "href": "week5.html#sec-model-quality",
    "title": "4  Bayesian Regression Models 2",
    "section": "4.2 Model quality metrics",
    "text": "4.2 Model quality metrics\nSummarizing McElreath (2020) Sec. 7.4:\nThe ideal for model quality metrics is out-of-sample deviance. This can never be computed, so we approximate using leave-one-out-cross-validation. This is usually impractical to compute—it would require refitting the model \\(n\\) times, where \\(n\\) = number of observations. So we approximate, in one of two ways\n\nPSIS-LOO-CV (“Pareto-smoothed importance-sampling leave-one-out cross-validation”), a.k.a. PSIS\nImplemented as loo() in the loo package\nWAIC (“Widely-applicable information criterion”)\nImplemented as waic()\n\nIn principle, these are different approaches, based on cross-validation versus calculating an information criterion, analagous to AIC/BIC for frequentist models. In practice, both PSIS and WAIC measure the same thing (out-of-sample deviance), and the larger the dataset, the more similar they will be.\nIf they give different qualitative results (with no errors in WAIC/PSIS calculation) you should be circumspect.\n\n\n\n\n\n\nPractical note: PSIS or WAIC?\n\n\n\n\n\nWhich of PSIS or WAIC should actually be used for model comparison in a concrete case? McElreath notes that PSIS and WAIC may each be better for different model types (Sec. 7.4.3), but seems to recommend defaulting to PSIS, because it “has a distinct advantage in warning the user about when it is unreliable” via the \\(k\\)-values it computes for each observation. However, WAIC is faster to compute—much faster, for large datasets or complex models—and the current WAIC implementation in loo also reports when it’s probably unreliable (and recommends using PSIS instead).\nMy usual workflow for model comparison is:\n\nFirst use WAIC\nIf there are warnings, switch to PSIS (a.k.a. “loo”, in the loo package)\nIf there are warnings about Pareto \\(k\\) values being too large, follow the package’s recommendation to compute PSIS with moment matching instead (see ?loo_moment_match).\n\nMy understanding is that Options 1–3 are (usually) progressively more accurate and slower.\n\n\n\n\n4.2.1 Example: nonlinear effect of WrittenFrequency\nThis section assumes as background the introduction to non-linear effects of predictors in Sec. 7.5 of RMLD, especially Sec. 7.5.3, where a similar example for frequentist linear regression is given.\nThe empirical effect of frequency (WrittenFrequency) on reaction time (RTlexdec) for the english_250 data is:\n\n\nCode\nenglish_250 %&gt;% ggplot(aes(x = WrittenFrequency, y = RTlexdec)) +\n  geom_point() +\n  geom_smooth(aes(color = AgeSubject))\n## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIt’s not immediately clear here whether the effect is linear or non-linear, and what degree the non-linear effect would be.\nWe assess this by comparing models fit with AgeSubject and different effects of WrittenFrequency:\n\nLinear effect of WrittenFrequency (equivalent to english_m43 from Section 3.3)\nPolynomial effect of WrittenFrequency, degree=2 (quadratic)\nPolynomial effect of WrittenFrequency, degree=3 (cubic)\nPolynomial effect of WrittenFrequency, degree=4 (quartic)\n\nFit these models, using the same priors as in Section 3.3:\n\nprior_1 &lt;- c(\n  prior(normal(0, 100), class = Intercept),\n  prior(normal(0, 5), class = b),\n  prior(exponential(1), class = sigma)\n)\n\nenglish_m51 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian,\n    prior = prior_1,\n    file = \"models/english_m51.brm\"\n  )\n\nenglish_m52 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + poly(WrittenFrequency_c, 2) + SubjectYoung,\n    family = gaussian,\n    prior = prior_1,\n    file = \"models/english_m52.brm\"\n  )\n\n\nenglish_m53 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + poly(WrittenFrequency_c, 3) + SubjectYoung,\n    family = gaussian,\n    prior = prior_1,\n    file = \"models/english_m53.brm\"\n  )\n\nenglish_m54 &lt;-\n  brm(\n    data = english_250,\n    RTlexdec ~ 1 + poly(WrittenFrequency_c, 4) + SubjectYoung,\n    family = gaussian,\n    prior = prior_1,\n    file = \"models/english_m54.brm\"\n  )\n\nWhile the loo() and waic() functions can be applied to a fitted model to calculate PSIS or WAIC, the recommended workflow is instead to add them to the fitted model using add_criterion(). This saves the specified criterion to the model file (indicated by the file argument when you fit the model) so it is only calculated once per fitted model. This is useful because PSIS/WAIC take a lot of time to compute, especially for more complex models.\n\n## add both WAIC and LOO\nenglish_m51 &lt;- add_criterion(english_m51, c(\"waic\", \"loo\"))\nenglish_m52 &lt;- add_criterion(english_m52, c(\"waic\", \"loo\"))\nenglish_m53 &lt;- add_criterion(english_m53, c(\"waic\", \"loo\"))\nenglish_m54 &lt;- add_criterion(english_m54, c(\"waic\", \"loo\"))\n\nExample WAIC and LOO output for one model:\n\nwaic(english_m53)\n## \n## Computed from 4000 by 250 log-likelihood matrix.\n## \n##           Estimate   SE\n## elpd_waic    263.9 12.3\n## p_waic         5.4  0.7\n## waic        -527.8 24.5\nloo(english_m53)\n## \n## Computed from 4000 by 250 log-likelihood matrix.\n## \n##          Estimate   SE\n## elpd_loo    263.9 12.3\n## p_loo         5.4  0.7\n## looic      -527.8 24.5\n## ------\n## MCSE of elpd_loo is 0.0.\n## MCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.7]).\n## \n## All Pareto k estimates are good (k &lt; 0.7).\n## See help('pareto-k-diagnostic') for details.\n\nThe most important output here is:\n\nThe Estimate and SE of PSIS (a.k.a. “LOO”) and WAIC, in the waic and looic rows.\n\nNote that a rough 95% CredI of WAIC or PSIS would be Estimate +- 1.96*SE.\n\nPareto \\(k\\) estimates\n\nThese values, one per observation, go into the calculation of PSIS-LOO.\nWhen \\(k&gt;\\) some threshold, by default 0.7, LOO is unreliable.\nObservations with \\(k&gt;\\) threshold are influential/potential “outliers”.\n\n\nOther output is less important.1\nNote how similar LOO and WAIC are for this dataset, as expected for large enough \\(n\\) (here, \\(n=250\\)).\nThese metrics can be used for model comparison via the loo_compare() function from loo:\n\nloo_compare(english_m51, english_m52, english_m53, english_m54, criterion = \"waic\")\n##             elpd_diff se_diff\n## english_m53  0.0       0.0   \n## english_m52 -0.6       1.4   \n## english_m54 -0.7       0.2   \n## english_m51 -1.2       1.8\nloo_compare(english_m51, english_m52, english_m53, english_m54, criterion = \"loo\")\n##             elpd_diff se_diff\n## english_m53  0.0       0.0   \n## english_m52 -0.6       1.4   \n## english_m54 -0.8       0.3   \n## english_m51 -1.2       1.8\n\nFor example: the difference in deviance (ELPD) based on WAIC between the cubic (english_m53) and quadratic (english_m52) models is 0.6, with standard error of 1.4. Thus, a rough 95% CredI for this difference in deviance is \\([0.6 - 1.96*1.4, 0.6 + 1.96*1.4]\\) = \\([-2.1, 3.3]\\).\nIn terms of LOO or WAIC, the cubic model (english_m53) wins: it has lower LOO than model english_52 by 0.6, which is lower than english_m54 by 0.2 (difference between 0.8 and 0.6), and so on. So we’d choose the cubic model.\nThe SE values suggest a slightly more complex picture: the 95% CIs for the difference in deviance with the next-best model (quadratic) or the least-good model (linear) overlap, but the 95% CI with the next-next-best model (quartic) does not:\n\n# 95% CI of diff: overlaps 0\nloo_compare(english_m53, english_m52)\n##             elpd_diff se_diff\n## english_m53  0.0       0.0   \n## english_m52 -0.6       1.4\n# 95% CI of diff: doesn't overlap 0\nloo_compare(english_m53, english_m54)\n##             elpd_diff se_diff\n## english_m53  0.0       0.0   \n## english_m54 -0.8       0.3\n# 95% CI of diff: overlaps 0\nloo_compare(english_m53, english_m51)\n##             elpd_diff se_diff\n## english_m53  0.0       0.0   \n## english_m51 -1.2       1.8\n\nThe most conservative option would be to just choose the linear model (english_m51), which doesn’t differ from any nonlinear model by the 95% CredI method:\n\n## 95% CredI all overlap 0\nloo_compare(english_m53, english_m51)\n##             elpd_diff se_diff\n## english_m53  0.0       0.0   \n## english_m51 -1.2       1.8\nloo_compare(english_m52, english_m51)\n##             elpd_diff se_diff\n## english_m52  0.0       0.0   \n## english_m51 -0.6       1.6\nloo_compare(english_m54, english_m51)\n##             elpd_diff se_diff\n## english_m54  0.0       0.0   \n## english_m51 -0.4       1.8\n\nWhen it is of interest to choose a single “best” model”, both methods are used in current practice:\n\nChoose model with lowest WAIC/PSIS\nChoose the simplest model using 95% CredI on WAIC/PSIS differences.\n\nThe second method is more conservative.\n\n\n\n\n\n\nBroader context: Using PSIS/WAIC for model selection\n\n\n\n\n\nThe two methods above—choosing the model with lowest PSIS (or WAIC) versus choosing the model which beats others by at least 2 SE—can be thought of as two options on a continuum, where you choose the best model depending on which one beats others by at least X SE (where X = 0 or 2). There is no right answer here, and no reason you need to be restricted to X = 0 or 2. These are just conventional choices for how conservative you want to be, like the commonly used AIC and BIC for frequentist models just correspond to different penalty terms in “data likelihood minus penalty”.\nIn fact, there is no reason you need to choose a “best model”—McElreath advises against it. Flego and Forrest (2021) is a nice example from phonetics where different X are used to differentiate between models which are more and less likely for the shape of vowel formant trajectories (linear, quadratic, an interpolation between two points, etc.).\nAn interesting option is model averaging, where instead of choosing a best model, you ask, “what combination of a set of models best predicts the data”, to get a weight for each model (where the weights add up to one). This is implemented in the loo package (see ?loo_model_weights).\nFor example, for the four models considered above:\n\nloo_model_weights(english_m51, english_m52, english_m53, english_m54)\n## Method: stacking\n## ------\n##             weight\n## english_m51 0.062 \n## english_m52 0.171 \n## english_m53 0.767 \n## english_m54 0.000\n\nThe data is best described as 77% the cubic model, 17% the quadratic model, and 6% the linear model.\n\n\n\nFor comparison, shown in Section 4.5: when we compare frequentist linear regression models for the same case, we get that the linear model or cubic model are best, depending on the method used for model comparison (e.g. AIC vs. BIC).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.1  \n\nFit the same four models as above, but now to english_25, and recalculate LOO/WAIC.\nWhich model is best, in terms of LOO?\nWhat conclusion do the 95% CIs of LOO differences suggest?\nDoes this conclusion fit your intuition from plotting the data geom_smooth() of WrittenFrequency vs RTlexdec)? If you get different answers for (b) and (c), which better fits this plot?\nYour “best model” from (b) should be different from the english_250 case. Why is this?\nExtra: Try (a)–(d) for a model fit to the entire english dataset. (You should now find clear evidence for a nonlinear effect.)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Regression Models 2</span>"
    ]
  },
  {
    "objectID": "week9.html#complexifying-and-interpreting-the-model",
    "href": "week9.html#complexifying-and-interpreting-the-model",
    "title": "5  Bayesian Hierarchical Models 1",
    "section": "5.3 Complexifying and interpreting the model",
    "text": "5.3 Complexifying and interpreting the model\nLet’s now fit a realistic model of the diatones data, including predictors—this is the frequentist model from RMLD Sec. 9.2.1. There are now several `fixed-effect’ predictors.\nFor priors we will use:\n\nIntercept: \\(N(0,5)\\)\n\nsame as the diatones_m41 model from Section 3.4.1.\n\nEach \\(\\beta_i\\): \\(N(0,3)\\):\n\nsame\n\nRandom intercept variance: \\(\\text{Exponential}(1)\\), as above.\n\nHere we are using essentially “flat” priors, just to keep things simpler for the moment (we don’t have to decide on “weakly informative” values).\n\ndiatones_m82 &lt;- brm(\n  data = diatones,\n  stress_shifted | trials(1) ~ syll2_coda + syll2_td + frequency + syll1_coda + syll2_td:frequency + syll1_coda:frequency + (1 | prefix),\n  family = binomial,\n  prior = c(\n    prior(normal(0, 5), class = Intercept), # beta_0\n    prior(normal(0, 3), class = b), # beta_i\n    prior(exponential(1), class = sd) # sigma\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  file = \"models/diatones_m82.brm\"\n)\n## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n\nModel summary:\n\ndiatones_m82\n## Warning: There were 1 divergent transitions after warmup. Increasing\n## adapt_delta above 0.8 may help. See\n## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n##  Family: binomial \n##   Links: mu = logit \n## Formula: stress_shifted | trials(1) ~ syll2_coda + syll2_td + frequency + syll1_coda + syll2_td:frequency + syll1_coda:frequency + (1 | prefix) \n##    Data: diatones (Number of observations: 130) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~prefix (Number of levels: 13) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     1.39      0.86     0.07     3.32 1.00      724     1142\n## \n## Regression Coefficients:\n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## Intercept               -2.16      0.62    -3.47    -0.89 1.00     1426\n## syll2_coda              -1.73      0.75    -3.29    -0.32 1.00     3104\n## syll2_td                 1.48      0.71     0.12     2.91 1.00     2630\n## frequency               -0.80      0.60    -2.05     0.34 1.00     2917\n## syll1_coda               1.99      0.98     0.16     4.14 1.00     1724\n## syll2_td:frequency       2.43      1.17     0.28     4.90 1.00     3225\n## frequency:syll1_coda     3.21      1.51     0.27     6.25 1.00     3333\n##                      Tail_ESS\n## Intercept                1101\n## syll2_coda               2850\n## syll2_td                 2922\n## frequency                2574\n## syll1_coda               2329\n## syll2_td:frequency       2610\n## frequency:syll1_coda     2839\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nExercise 5.2 Examine some diagnostics for this model, just for the parameters shown by summary(). Do you see any sign that the model hasn’t “converged” (good posterior sample for all parameters)? If so, re-fit the model with more iterations.\n\nSolution:\n\n\nCode\n## yes, you need more iterations\ndiatones_m82 &lt;- brm(\n  data = diatones,\n  stress_shifted | trials(1) ~ syll2_coda + syll2_td + frequency + syll1_coda + syll2_td:frequency + syll1_coda:frequency + (1 | prefix),\n  family = binomial,\n  prior = c(\n    prior(normal(0, 5), class = Intercept), # beta_0\n    prior(normal(0, 3), class = b), # beta_i\n    prior(exponential(1), class = sd) # sigma\n  ),\n  iter = 5000, warmup = 2500, chains = 4, cores = 4,\n  file = \"models/diatones_m82.brm\"\n)\n## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n## Only 2 levels detected so that family 'bernoulli' might be a more efficient choice.\n\n\n\n\nCode\nsummary(diatones_m82)\n\n## (We'll return to the \"divergent transitions\" message below.)\n\n\n\n5.3.1 Model comparison\nWe can ask whether adding the random intercept term is justified by comparing to a model without this term, using PSIS (“LOO”), from Section 4.2 :\n\ndiatones_m83 &lt;- brm(\n  data = diatones,\n  stress_shifted | trials(1) ~ syll2_coda + syll2_td + frequency + syll1_coda + syll2_td:frequency + syll1_coda:frequency,\n  family = binomial,\n  prior = c(\n    prior(normal(0, 5), class = Intercept), # beta_0\n    prior(normal(0, 3), class = b) # beta_i\n  ),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  file = \"models/diatones_m83.brm\"\n)\n\n\ndiatones_m82 &lt;- add_criterion(diatones_m82, c(\"loo\"))\ndiatones_m83 &lt;- add_criterion(diatones_m83, c(\"loo\"))\nloo_compare(diatones_m82, diatones_m83)\n##              elpd_diff se_diff\n## diatones_m82  0.0       0.0   \n## diatones_m83 -1.7       2.2\n\nDepending on the model selection criterion we use—lowest PSIS, versus lowest PSIS up to 95% CredI—either model could be preferred. (See the end of Section 4.2 for discussion.)\n\n\n\n5.3.2 Plotting effects\nAs for non-hierarchical models (Section 3.3.1, Section 3.2.2), there are various ways to plot effects:\n\nMaking your own plots (using predict() or fitted() from brms) or using pre-existing functions.\nConfidence intervals vs. prediction intervals\n\n\n\n\n\n\nFor example: plot the marginal effect of frequency, averaging over other predictors, with 95% CIs:\n\nconditional_effects(diatones_m82, effects = \"frequency\")\n\n\n\n\n\n\n\n\nPredictions here are “fitted vlaues”: the expected value of the probability of stress shifting for an “average prefix”.  These are on the probability scale; to get predictions on the log-odds scale, you’d need to use posterior_linpred(). You can alternatively get PIs from the posterior predictive distribution, which will give a larger interval.\n\nThis won’t do something sensible for the current example:\n\nconditional_effects(diatones_m82, effects = \"frequency\", method = \"posterior_predict\")\n\n\n\n\n\n\n\n\n(An exercise next week will be to try this for a linear regression model)\nWe discussed fitted values vs. model predictions in Section 3.2.2.\n\nExercise 5.3  \n\nMake plots visualizing the syll1:frequency and syll2_td:frequency interactions, like RMLD Fig. 9.4. (It’s OK if your figures don’t treat syll1_coda and syll2_td as factors,or maybe you can figure out how to do this.)3\n\n\n\n\n\n\nExtra: You should have found that the plot looks similar to the plot for the frequentist model. Let’s use the Bayesian model to make a prediction plot that would be much harder, if possible at all, using the frequentist model: the frequency effect just for words with prefix = iN.\n\n\nMake a dataframe of new values to predict at:\n\nprefix=iN\nsyll2_coda=syll2_td=0 (held at average values)\nsyll1_coda=-0.22 (recall: syll1_coda is a prefix-level predictor; this is its value for prefix iN)\nLet frequency range between -1.5 and 1.5 (meaning +-3 SD of frequency before it was standarized).\n\nUse fitted() to get predictions with 95% CIs.\nPlot these predictions, to make a plot like the one above with frequency on the \\(x\\)-axis.4\nYou should find that the plot looks different from the frequency plot above, both in terms of its mean and the width of the CI. Why is this?5",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bayesian Hierarchical Models 1</span>"
    ]
  },
  {
    "objectID": "week10.html",
    "href": "week10.html",
    "title": "6  Bayesian Hierarchical Models 2",
    "section": "",
    "text": "6.1 Preliminaries\nLoad libraries we will need:\nlibrary(brms)\nlibrary(lme4)\nlibrary(arm)\nlibrary(tidyverse)\n\nlibrary(tidybayes)\nlibrary(bayestestR)\n\nlibrary(bayesplot)\nlibrary(loo)\n\nlibrary(broom) ## for tidy model summaries\nlibrary(broom.mixed) ## for tidy model summaries for lme4 models\n\nlibrary(patchwork)\nMake numbers be printed only to 3 digits, for neater output:\noptions(digits = 3)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hierarchical Models 2</span>"
    ]
  },
  {
    "objectID": "week10.html#preliminaries",
    "href": "week10.html#preliminaries",
    "title": "6  Bayesian Hierarchical Models 2",
    "section": "",
    "text": "Practical notes\n\n\n\n\n\n\nIf you have loaded rethinking, you need to detach it before using brms. See Kurz (2023) Sec. 4.3.1.\nI use the file argument when fitting brms models to make compiling this document easier (so the models don’t refit every time I compile). You may or may not want to do this for your own models. See file and file_refit arguments in ?brm.\nHere I set the file_refit option so “brms will refit the model if model, data or algorithm as passed to Stan differ from what is stored in the file.”\n\n\noptions(brms.file_refit = \"on_change\")\n\n\nI use chains = 4, cores = 4 when fitting brm models below—this means 4 chains, each to be run on one core on my laptop. cores = 4 may need to be adjusted for your computer. (You may have fewer or more cores; I have 8 cores, so this leaves 50% free.) You should figure out how to use multiple cores on your machine.\n\n\n\n\n\n\n\n6.1.1 Data\nLoad the neutralization data from RMLD (Sonderegger 2023)—where this dataset is described in more detail (Sec. 3.3.1)—and perform some preprocessing (described in Sec. 10.1).\n\nneutralization &lt;- read.csv(\"https://osf.io/qg5fc/download\", stringsAsFactors = TRUE) %&gt;%\n  mutate(voicing_fact = fct_relevel(voicing, \"voiceless\")) %&gt;%\n  filter(!is.na(prosodic_boundary)) %&gt;%\n  mutate(\n    prosodic_boundary = rescale(prosodic_boundary),\n    voicing = rescale(voicing_fact),\n    item_pair = as.factor(item_pair),\n    subject = as.factor(subject)\n  )\n\n## Code multi-level factors with Helmert contrasts\n## so that all predictors are centered\ncontrasts(neutralization$vowel) &lt;- contr.helmert\ncontrasts(neutralization$place) &lt;- contr.helmert\n\nRecall that for this data:\n\nvoicing is of primary interest\nThe response is vowel_dur\nThere are two grouping factors, item_pair and subject\nvoicing varies within both item and subject\nprosodic_boundary, vowel, place are controls.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hierarchical Models 2</span>"
    ]
  },
  {
    "objectID": "week10.html#multiple-grouping-factors",
    "href": "week10.html#multiple-grouping-factors",
    "title": "6  Bayesian Hierarchical Models 2",
    "section": "6.2 Multiple grouping factors",
    "text": "6.2 Multiple grouping factors\nLet’s fit a first model of the neutralization data, with:\n\nA realistic set of fixed effects: voicing (of primary interest), plus all controls: vowel, place, prosodic_boundary.\nRandom intercepts for item_pair and subject\n\nThis is not a good model (we’d need random slopes for voicing), but it will do for now.\nLet’s first fit a frequentist lmer() model, to compare to what we’ll get with the Bayesian model:\n\nneut_m1 &lt;- lmer(vowel_dur ~ voicing + vowel + place + prosodic_boundary + (1 | item_pair) + (1 | subject), data = neutralization)\n\n\nsummary(neut_m1, correlation = FALSE)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: vowel_dur ~ voicing + vowel + place + prosodic_boundary + (1 |  \n##     item_pair) + (1 | subject)\n##    Data: neutralization\n## \n## REML criterion at convergence: 6693\n## \n## Scaled residuals: \n##    Min     1Q Median     3Q    Max \n## -4.562 -0.604 -0.021  0.601  3.019 \n## \n## Random effects:\n##  Groups    Name        Variance Std.Dev.\n##  item_pair (Intercept)  71.9     8.48   \n##  subject   (Intercept) 669.3    25.87   \n##  Residual              402.5    20.06   \n## Number of obs: 749, groups:  item_pair, 24; subject, 16\n## \n## Fixed effects:\n##                   Estimate Std. Error t value\n## (Intercept)        157.175      6.741   23.32\n## voicing              9.599      1.469    6.53\n## vowel1              -0.288      3.125   -0.09\n## vowel2             -18.239      1.722  -10.59\n## vowel3              -3.724      1.160   -3.21\n## vowel4              -6.757      0.897   -7.53\n## place1              -9.373      2.428   -3.86\n## place2               1.604      1.321    1.21\n## prosodic_boundary   12.392      2.612    4.74\n\nTo fit a Bayesian model, we need to set priors. We will determine weakly informative priors, for practice.\nTo choose a prior for the intercept, note that the response variable vowel_dur, has range of about 50–300:\n\nggplot(aes(x = vowel_dur), data = neutralization) +\n  geom_density(fill = \"blue\", alpha = 0.25)\n\n\n\n\n\n\n\n\nLet’s use the following priors:\n\nIntercept: \\(N(150, 50)\\)\n\nWhatever the intercept corresponds to in this model, reasonable values must be positive (vowel duration can only be positive).1\n\n\\(\\beta_i\\): \\(N(0, 50)\\) — a change of 50 (msec) in vowel_duration is huge.\nRandom intercept variances: \\(\\text{Exponential}(0.02)\\)\n\nWhy is \\(\\lambda = 0.02\\) for the exponential prior, \\(\\text{Exponential}(\\lambda)\\)? \\(\\lambda\\) should be on the order of 1/\\(SD_y\\), the standard deviation of the response variable, to be “weakly informative”.2 The \\(\\text{Exponential(1)}\\) prior we’ve seen in other models was based on the assumption that \\(y\\) has been standardized. In this case, it has not: sd(neutralization$vowel_dur) is 43.5, so \\(\\lambda\\) should be around 0.02.\n\n\n\n\n\n\nPractical note: Weakly-informative vs. default priors\n\n\n\n\n\nRemember: if you are not comfortable determining weakly-informative priors, it’s always an option to just use brms’ default priors, by not specifying the prior argument of your model. brms is particularly good for weakly-informative default priors for random effects (see ?prior() Sec. 2). To use these, we’d remove the class = sd line from our prior statement when fitting the model. Using weakly-informative priors is best practice, but it is better to use default/flat priors you don’t understand than to risk fitting a model with priors that don’t make sense.\nIt’s also a good idea to use brms default priors whenever you just don’t understand what a model parameter is doing. (Well, the second-best idea, after actually understanding it.). This can happen even if you’re pretty comfortable with Bayesian models, when you move to a new model type. For example: if you fit a negative binomial model, everything might make sense except the shape parameter, which in fact requires substantial digging in documentation to find a good definition. It’s fine to just use brms’s default prior for it.\n\n\n\n\nneutralization_m10_1 &lt;- brm(\n  data = neutralization,\n  family = gaussian,\n  vowel_dur ~ voicing + place + vowel + prosodic_boundary + (1 | subject) + (1 | item_pair),\n  prior = c(\n    prior(normal(150, 50), class = Intercept), # beta_0\n    prior(normal(0, 50), class = b),\n    prior(exponential(0.02), class = sd) # sigma\n  ),\n  iter = 4000, warmup = 2000, chains = 4, cores = 4,\n  file = \"models/neutralization_m10_1.brm\"\n)\n\n\nsummary(neutralization_m10_1)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: vowel_dur ~ voicing + place + vowel + prosodic_boundary + (1 | subject) + (1 | item_pair) \n##    Data: neutralization (Number of observations: 749) \n##   Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n##          total post-warmup draws = 8000\n## \n## Multilevel Hyperparameters:\n## ~item_pair (Number of levels: 24) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     9.10      1.98     5.99    13.71 1.00     3047     4798\n## \n## ~subject (Number of levels: 16) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)    27.80      5.70    19.16    41.02 1.00     2312     3711\n## \n## Regression Coefficients:\n##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept           156.99      7.09   143.28   171.16 1.00     1834     3038\n## voicing               9.60      1.51     6.61    12.57 1.00    11511     5544\n## place1               -9.36      2.56   -14.42    -4.26 1.00     5323     5081\n## place2                1.56      1.45    -1.29     4.51 1.00     5502     4747\n## vowel1               -0.23      3.37    -6.69     6.51 1.00     5919     4941\n## vowel2              -18.20      1.87   -21.95   -14.56 1.00     4969     4896\n## vowel3               -3.74      1.25    -6.26    -1.27 1.00     5544     4502\n## vowel4               -6.69      0.97    -8.52    -4.71 1.00     5548     5520\n## prosodic_boundary    12.35      2.63     7.25    17.51 1.00     9918     6003\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma    20.10      0.54    19.07    21.17 1.00    11169     4645\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nHere is one possible visualization of the model’s coefficients:\n\n# One possible posterior plot, showing all non-random effect parameters except intercept:\n# regexp means \"anything starting with b_X, where X doen't begin with I, and anything starting with s\".\nmcmc_plot(neutralization_m10_1, variable = c(\"^b_[^I]\", \"^s\"), regex = TRUE)\n\n\n\n\n\n\n\n\n\nExercise 6.1  \n\nMatch every fitted parameter shown in the Bayesian model output (there are 12) to the frequentist model output and verify that their values are (practically) the same.\n\n\n\nMake plots of the marginal effect of vowel for model neutralization_m81 (as in Section 5.3.2): one plot for an “average item/speaker” (CIs), and another for a new observation (PIs).\n\n\n\n\n\nRecall that the model contains many more fitted parameters than shown in the summary, where none of the random effects are shown. To see all parameters:\n\nposterior_summary(neutralization_m10_1)\n##                            Estimate Est.Error     Q2.5     Q97.5\n## b_Intercept                1.57e+02     7.095   143.28   171.164\n## b_voicing                  9.60e+00     1.510     6.61    12.569\n## b_place1                  -9.36e+00     2.564   -14.42    -4.259\n## b_place2                   1.56e+00     1.446    -1.29     4.510\n## b_vowel1                  -2.29e-01     3.370    -6.69     6.510\n## b_vowel2                  -1.82e+01     1.868   -21.95   -14.557\n## b_vowel3                  -3.74e+00     1.252    -6.26    -1.268\n## b_vowel4                  -6.69e+00     0.967    -8.52    -4.715\n## b_prosodic_boundary        1.24e+01     2.633     7.25    17.511\n## sd_item_pair__Intercept    9.10e+00     1.983     5.99    13.712\n## sd_subject__Intercept      2.78e+01     5.699    19.16    41.017\n## sigma                      2.01e+01     0.537    19.07    21.171\n## Intercept                  1.56e+02     7.096   141.81   169.675\n## r_item_pair[1,Intercept]   3.87e+00     5.878    -7.53    16.024\n## r_item_pair[2,Intercept]   1.81e+00     5.913   -10.04    13.362\n## r_item_pair[3,Intercept]  -7.10e+00     5.706   -18.71     4.011\n## r_item_pair[4,Intercept]  -6.95e+00     5.440   -17.22     3.702\n## r_item_pair[5,Intercept]   1.72e+01     6.078     6.09    29.742\n## r_item_pair[6,Intercept]   4.96e+00     5.470    -5.48    15.713\n## r_item_pair[7,Intercept]  -1.96e+00     5.539   -13.20     8.793\n## r_item_pair[8,Intercept]  -1.81e-02     5.590   -11.06    11.100\n## r_item_pair[9,Intercept]   7.04e+00     5.878    -4.24    18.692\n## r_item_pair[10,Intercept] -1.25e+01     6.242   -25.10    -0.622\n## r_item_pair[11,Intercept]  1.14e+00     5.554    -9.76    12.399\n## r_item_pair[12,Intercept]  6.88e+00     6.000    -4.72    18.763\n## r_item_pair[13,Intercept] -6.70e+00     5.583   -17.84     4.181\n## r_item_pair[14,Intercept]  5.86e-01     5.503   -10.41    11.221\n## r_item_pair[15,Intercept]  9.94e+00     6.283    -2.39    22.866\n## r_item_pair[16,Intercept] -6.58e+00     5.539   -17.70     4.332\n## r_item_pair[17,Intercept]  4.98e+00     5.445    -6.05    15.538\n## r_item_pair[18,Intercept]  5.13e-01     5.605   -10.40    11.993\n## r_item_pair[19,Intercept] -7.22e+00     5.584   -18.39     3.626\n## r_item_pair[20,Intercept]  2.18e-01     5.631   -10.80    11.244\n## r_item_pair[21,Intercept]  1.43e+00     5.456    -9.31    12.317\n## r_item_pair[22,Intercept]  1.98e+00     5.436    -8.57    12.995\n## r_item_pair[23,Intercept] -8.99e+00     5.552   -20.54     1.619\n## r_item_pair[24,Intercept] -4.11e+00     5.867   -15.62     7.216\n## r_subject[1,Intercept]     5.87e+01     7.473    44.10    73.399\n## r_subject[2,Intercept]    -2.84e+01     7.394   -43.27   -13.967\n## r_subject[3,Intercept]    -3.15e+01     7.421   -46.29   -16.872\n## r_subject[4,Intercept]    -4.32e+00     7.401   -19.15    10.203\n## r_subject[5,Intercept]    -3.57e-01     7.477   -15.16    14.374\n## r_subject[6,Intercept]    -2.68e+01     7.360   -41.44   -12.377\n## r_subject[7,Intercept]    -3.05e+00     7.422   -18.04    11.295\n## r_subject[8,Intercept]     3.20e+00     7.572   -11.90    18.068\n## r_subject[9,Intercept]    -2.08e+00     7.449   -16.80    12.308\n## r_subject[10,Intercept]   -1.44e+00     7.374   -16.22    13.181\n## r_subject[11,Intercept]   -1.80e+01     7.440   -32.88    -3.690\n## r_subject[12,Intercept]    2.64e+00     7.388   -12.13    17.021\n## r_subject[13,Intercept]    3.12e+01     7.439    16.42    45.626\n## r_subject[14,Intercept]    4.72e+01     7.494    32.32    62.069\n## r_subject[15,Intercept]   -6.02e+00     7.355   -20.66     8.392\n## r_subject[16,Intercept]   -1.82e+01     7.458   -33.17    -3.662\n## lprior                    -5.64e+01     0.128   -56.68   -56.178\n## lp__                      -3.41e+03     6.583 -3427.23 -3401.640\n\nBecause the posterior distribution is over all terms, we can examine the posterior for any parameter(s), using tools to manipulate the posterior from previous models (Chapter 3).\nFor example, to examine just the posterior for the by-item and by-subject random intercept variances:\n\n\nCode\nneutralization_m10_1 %&gt;%\n  spread_draws(sd_item_pair__Intercept, sd_subject__Intercept) %&gt;%\n  ggplot(aes(x = sd_item_pair__Intercept, y = sd_subject__Intercept)) +\n  geom_hex() +\n  xlab(\"by-item variability\") +\n  ylab(\"by-subject variability\") +\n  geom_vline(aes(xintercept = 0), lty = 2) +\n  geom_hline(aes(yintercept = 0), lty = 2) +\n  geom_abline(lty = 1, alpha = 0.2)\n\n\n\n\n\n\n\n\n\nA \\(y = x\\) line has been added here to aid in an exercise.\n\nExercise 6.2  \n\nBased on the plot just above and/or the model’s output, do you think that subjects or items vary more in vowel duration? Explain.\nThis question can be formally tested using this command:\n\n\nhypothesis(neutralization_m10_1, \"sd_subject__Intercept &gt; sd_item_pair__Intercept\", class = NULL)\n\nRun this command to evaluate the hypothesis: Post.Prob is \\(p_d\\) and Evid.Ratio is a Bayes Factor.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hierarchical Models 2</span>"
    ]
  },
  {
    "objectID": "week10.html#footnotes",
    "href": "week10.html#footnotes",
    "title": "6  Bayesian Hierarchical Models 2",
    "section": "",
    "text": "The intercept here actually corresponds to “average value of vowel_dur”, which must be positive. From ?prior(): “Note that technically, this prior is set on an intercept that results when internally centering all population-level predictors around zero to improve sampling efficiency. On this centered intercept, specifying a prior is actually much easier and intuitive than on the original intercept, since the former represents the expected response value when all predictors are at their means.”↩︎\nThis is because \\(\\lambda\\) describes how fast the exponential prior decreases: a larger value means a faster decrease.↩︎\nSee discussion in RMLD Sec. 3.2.3.2 and Kirby and Sonderegger (2018).↩︎\nOnly the lower half of the correlation matrix is shown because the matrix is symmetric.↩︎\nEven \\(\\eta = 1.02\\) can help the model avoid extreme correlations, though more iterations may be needed for convergence.↩︎\nThe discussion in RMLD Chapter 10 on the benefits and risks of more/less complex random effect structure still holds: more and less complex random effect structure both run risks, and rather than applying a recipe it is best to decide on a random effect structure with research questions in mind.↩︎\nI think! TODO: check that coefficients() output incoroprates both sources of uncertainty.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hierarchical Models 2</span>"
    ]
  },
  {
    "objectID": "week9.html#extra",
    "href": "week9.html#extra",
    "title": "5  Bayesian Hierarchical Models 1",
    "section": "5.4 Extra",
    "text": "5.4 Extra\nModel diatones_m82 has some divergent transitions:\n\ndiatones_m82\n## Warning: There were 1 divergent transitions after warmup. Increasing\n## adapt_delta above 0.8 may help. See\n## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n##  Family: binomial \n##   Links: mu = logit \n## Formula: stress_shifted | trials(1) ~ syll2_coda + syll2_td + frequency + syll1_coda + syll2_td:frequency + syll1_coda:frequency + (1 | prefix) \n##    Data: diatones (Number of observations: 130) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~prefix (Number of levels: 13) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     1.39      0.86     0.07     3.32 1.00      724     1142\n## \n## Regression Coefficients:\n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## Intercept               -2.16      0.62    -3.47    -0.89 1.00     1426\n## syll2_coda              -1.73      0.75    -3.29    -0.32 1.00     3104\n## syll2_td                 1.48      0.71     0.12     2.91 1.00     2630\n## frequency               -0.80      0.60    -2.05     0.34 1.00     2917\n## syll1_coda               1.99      0.98     0.16     4.14 1.00     1724\n## syll2_td:frequency       2.43      1.17     0.28     4.90 1.00     3225\n## frequency:syll1_coda     3.21      1.51     0.27     6.25 1.00     3333\n##                      Tail_ESS\n## Intercept                1101\n## syll2_coda               2850\n## syll2_td                 2922\n## frequency                2574\n## syll1_coda               2329\n## syll2_td:frequency       2610\n## frequency:syll1_coda     2839\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n(You may have a different number of divergent transitions, as expected given that each MCMC fit is different.)\nIn general this is not a problem to be ignored, as it indicates poor sampling of the posterior. However, a very small number of divergent transitions can be fine (e.g. 2 out of 10000). Nonetheless, let’s take care of this by increasing adapt_delta, as suggested:\n\ndiatones_m83_2 &lt;- update(diatones_m82,control=list(adapt_delta=0.9))\ndiatones_m83_2\n\nNow there is no warning.\nYou can check that the resulting model is very similar to diatones_m82, but in general models before and after fixing divergent transitions issues can be very different.\nNote that adapt_delta must be smaller than 1. The brms default is 0.8, while McElreath’s default (in rethinking::ulam) is 0.95. (These are just defaults, one isn’t better than the other.)\n\n\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data analysis using regression and multilevel/hierarchical models. Cambridge: Cambridge University Press.\n\n\nKurz, S. 2023. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bayesian Hierarchical Models 1</span>"
    ]
  },
  {
    "objectID": "week10.html#random-slopes",
    "href": "week10.html#random-slopes",
    "title": "6  Bayesian Hierarchical Models 2",
    "section": "6.3 Random slopes",
    "text": "6.3 Random slopes\nOur model of the neutralization data above included by-subject and by-item random intercepts, with a set of control predictors. We now allow the voicing effect, which is of key interest, to vary by-subject and by-item, i.e. random slopes.\n\n6.3.1 Research questions\nOur research questions will be:\n\nIs there an overall effect of voicing?\nWhat by-subject variability is there in the voicing effect?\n\nIt will be important background knowledge that:\n\nThe voicing effect is expected to be small.\nA small enough voicing effect is functionally zero (humans can’t perceive the duration difference).\n\nWe will assume below that “small enough” is 5 msec, but other values might be reasonable (~5-10 msec).3\n\n\n\n\n6.3.2 Frequentist model\nThis model updates neut_m1 to add random slopes:\n\nneut_m2 &lt;- lmer(vowel_dur ~ voicing + place + vowel + prosodic_boundary +\n  (1 + voicing | subject) + (1 + voicing | item_pair), data = neutralization)\n## boundary (singular) fit: see help('isSingular')\n\n\nsummary(neut_m2, correlation = FALSE)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: vowel_dur ~ voicing + place + vowel + prosodic_boundary + (1 +  \n##     voicing | subject) + (1 + voicing | item_pair)\n##    Data: neutralization\n## \n## REML criterion at convergence: 6683\n## \n## Scaled residuals: \n##    Min     1Q Median     3Q    Max \n## -4.460 -0.601  0.002  0.599  2.876 \n## \n## Random effects:\n##  Groups    Name        Variance Std.Dev. Corr \n##  item_pair (Intercept)  76.57    8.75         \n##            voicing       8.35    2.89    -1.00\n##  subject   (Intercept) 674.27   25.97         \n##            voicing      18.86    4.34    1.00 \n##  Residual              395.35   19.88         \n## Number of obs: 749, groups:  item_pair, 24; subject, 16\n## \n## Fixed effects:\n##                   Estimate Std. Error t value\n## (Intercept)        157.085      6.777   23.18\n## voicing              9.604      1.910    5.03\n## place1             -10.592      2.336   -4.53\n## place2               2.120      1.269    1.67\n## vowel1              -0.400      3.013   -0.13\n## vowel2             -17.542      1.659  -10.58\n## vowel3              -3.185      1.115   -2.86\n## vowel4              -6.207      0.868   -7.15\n## prosodic_boundary   11.768      2.579    4.56\n## optimizer (nloptwrap) convergence code: 0 (OK)\n## boundary (singular) fit: see help('isSingular')\n\nThis simple model, which includes the minimal random intercepts and slopes needed for the research questions, is singular. This is due to “perfect” random effect correlations, which are likely pathological. Fitting issues like singular models or non-convergence are common in mixed-effects models with linguistic data and often require substantial effort in model selection. Chapter 10 of RMLD covers these issues in detail. But as we’ll see below, using Bayesian models goes a long way to solving them.\nIgnoring the perfect correlations: the model predicts a clear voicing effect, with some variability by subject and item.\n\n\n6.3.3 Priors for random-effect terms\nNote that the random-effect terms in this model—describing covariance among random effects—are decomposed into:\n\n\\(k\\) random effect variances\nA matrix of \\(k(k-1)/2\\) correlations\n\nFor this model:\n\nBy-subject random effects: 2 variances, 1 correlation (=2x2 matrix)\nBy-item random effects: 2 variances, 1 correlation (=2x2 matrix)\n\nThis is standard for frequentist mixed-effects models, though we usually don’t think about the parametrization, especially when focused on fixed effects.\nIn a Bayesian model, we must understand this decomposition (at a high level) because both the random-effect variances and the correlation matrix need priors.\nThe priors are typically set as:\n\nOne prior for each variance\nOne prior for each correlation matrix\n\nInstead of setting a prior for the entire covariance matrix, we decompose it into these components, just like in the lmer() output.4\nFor variances, we can use the same priors as for random intercepts or residual \\(\\sigma\\) variance.\nFor correlation matrices, we use the LKJ prior, \\(LKJ(\\eta)\\):\n\n\\(\\eta = 1\\): flat\n\\(0 &lt; \\eta &lt; 1\\): extreme correlations (closer to 1/-1) favored\n\\(\\eta &gt; 1\\): extreme correlations disfavored.\n\nSee Kurz (2023) 14.1.3 for intuition on LKJ priors.\nSince random-effect correlations near 1/-1 are unlikely, it is common to fit Bayesian mixed-effects models using LKJ priors with \\(\\eta &gt; 1\\). A common choice is \\(\\eta = 2\\), which we’ll use below, though this is somewhat arbitrary.5\n\n\n6.3.4 A first model\nWe’ll fit the Bayesian equivalent of model neut_m2, using similar priors to neutralization_m10_1.\n\nFixed effects\n\nIntercept: \\(N(150, 75)\\)\nEach coefficient \\(\\beta_i\\): \\(N(0, 50)\\)\n\nRandom effects\n\nResidual variance: \\(\\sigma \\sim \\text{Exponential}(0.02)\\)\nRandom effect variances: SDs \\(\\sim \\text{Exponential}(0.02)\\)\nRandom effect correlation matrices: \\(LKJ(2)\\)\n\n\nExcept for the LKJ prior, the justifications are the same as for neutralization_m10_1.\n\nprior_1 &lt;- c(\n  prior(normal(150, 75), class = Intercept), # beta_0\n  prior(normal(0, 50), class = b),\n  prior(exponential(0.02), class = sd), # random-effect SDs\n  prior(lkj(2), class = cor), # random-effect correlation matrices\n  prior(exponential(0.02), class = sigma) # residual variance\n)\n\nFit model:\n\nneutralization_m10_2 &lt;- brm(\n  data = neutralization,\n  family = gaussian,\n  vowel_dur ~ voicing + place + vowel + prosodic_boundary +\n    (1 + voicing | subject) + (1 + voicing | item_pair),\n  prior = prior_1,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  file = \"models/neutralization_m10_2.brm\"\n)\n\n\nsummary(neutralization_m10_2)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: vowel_dur ~ voicing + place + vowel + prosodic_boundary + (1 + voicing | subject) + (1 + voicing | item_pair) \n##    Data: neutralization (Number of observations: 749) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~item_pair (Number of levels: 24) \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)              9.34      2.04     6.20    14.12 1.00     1737\n## sd(voicing)                3.05      1.99     0.14     7.34 1.00     1546\n## cor(Intercept,voicing)    -0.25      0.40    -0.87     0.63 1.00     4471\n##                        Tail_ESS\n## sd(Intercept)              2292\n## sd(voicing)                1729\n## cor(Intercept,voicing)     3012\n## \n## ~subject (Number of levels: 16) \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)             27.37      5.18    19.37    39.71 1.00     1451\n## sd(voicing)                5.02      2.14     1.13     9.54 1.00     1638\n## cor(Intercept,voicing)     0.53      0.27    -0.12     0.92 1.00     4566\n##                        Tail_ESS\n## sd(Intercept)              2565\n## sd(voicing)                1772\n## cor(Intercept,voicing)     2631\n## \n## Regression Coefficients:\n##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept           157.24      7.16   142.69   171.47 1.00      944     1293\n## voicing               9.68      2.17     5.44    13.95 1.00     3285     3028\n## place1               -9.75      2.77   -15.40    -4.23 1.00     2848     2702\n## place2                1.73      1.49    -1.23     4.70 1.00     3346     2485\n## vowel1               -0.23      3.32    -6.75     6.42 1.00     3062     2538\n## vowel2              -17.98      1.88   -21.66   -14.25 1.00     2859     3030\n## vowel3               -3.63      1.28    -6.25    -1.08 1.00     3106     2762\n## vowel4               -6.46      1.04    -8.45    -4.31 1.00     2778     2776\n## prosodic_boundary    11.93      2.56     6.76    16.93 1.00     6359     2906\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma    19.90      0.53    18.92    20.98 1.00     6183     2951\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nPosterior for the voicing term (RQ 1):\n\nplot(neutralization_m10_1, variable = \"b_voicing\")\n\n\n\n\n\n\n\n\nThere is also substantial by-subject and by-item variability in the voicing effect. We can see from the 95% CIs on those random-slope terms that it is unclear whether items or subjects vary more.\nTo address individual variability, the posterior estimate for sd(voicing) tells us the model is confident there is some variability, but not how much: the 95% CredI is [1.13, 9,54].\nJust for fun, here is a visualization of all effects whose parameters end with voicing—which includes the two effects discussed just above:\n\nmcmc_intervals(neutralization_m10_2, pars = vars(ends_with(\"voicing\")))\n\n\n\n\n\n\n\n\nThis illustrates the useful tidy parameter selection functionality, which bayesplot functions (like mcmc_intervals()) can use to pick out variable names of interest.\n\n\n6.3.5 A “Maximal” model\n\n\nRegularizing correlations tends to change fixed-effect estimates little, while allowing complex random effect structures which are important for accurately estimating fixed effects.\nRecall, from Chap 8 and 10 of RMLD:\n\nRandom effect correlations were a big issue for frequentist mixed models.\nDifferent approaches to building up random effect structure (“maximal” vs. “data-driven” vs. “uncorrelated first”) all agree on the importance of adding many random slope terms.\nThe simplest recipe was to fit a “maximal” model (RMLD Chap. 8) , with all possible random slopes.\n\nThis often wasn’t feasible, because the random effect structure is too complex for the data (especially given the number of correlations), causing the model to be singular or not converge.\n\nThe basic issue was overfitted random effect structure.\n\n\n\nWith a Bayesian model, we can just set a regularizing prior on random effect terms, and fit a random effect structure with a large number of terms. In other words, “maximal” models are always possible. This is not magic – using a very complex random-effect structure for a small dataset just means you will get random-effect parameter estimates which reflect the prior more, and may lower power on fixed-effect estimates.6 But the practical fitting issues which bedevil us for frequentist mixed-effects models are gone.\n\nLet’s fit a maximal model to this data, using an \\(LKJ(2)\\) prior for correlations:\n\nprior_2 &lt;- c(\n  prior(normal(150, 75), class = Intercept), # beta_0\n  prior(normal(0, 50), class = b),\n  prior(exponential(0.02), class = sd), # random-effect SDs\n  prior(lkj(2), class = cor), # random-effect correlation matrices\n  prior(exponential(0.02), class = sigma) # residual variance\n)\n\n## by-speaker random slopes for all predictors\n## by-item random slopes for all predictors which vary within item.\nneutralization_m10_3 &lt;- brm(\n  data = neutralization,\n  family = gaussian,\n  vowel_dur ~ voicing + place + vowel + prosodic_boundary +\n    (1 + voicing + place + vowel + prosodic_boundary | subject) +\n    (1 + voicing + prosodic_boundary | item_pair),\n  prior = prior_2,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  file = \"models/neutralization_m10_3.brm\"\n)\n\n\nsummary(neutralization_m10_3)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: vowel_dur ~ voicing + place + vowel + prosodic_boundary + (1 + voicing + place + vowel + prosodic_boundary | subject) + (1 + voicing + prosodic_boundary | item_pair) \n##    Data: neutralization (Number of observations: 749) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~item_pair (Number of levels: 24) \n##                                  Estimate Est.Error l-95% CI u-95% CI Rhat\n## sd(Intercept)                        9.51      1.99     6.34    14.04 1.00\n## sd(voicing)                          3.52      2.04     0.20     7.81 1.00\n## sd(prosodic_boundary)                2.28      1.63     0.08     5.99 1.00\n## cor(Intercept,voicing)              -0.29      0.35    -0.85     0.50 1.00\n## cor(Intercept,prosodic_boundary)    -0.18      0.38    -0.82     0.61 1.00\n## cor(voicing,prosodic_boundary)       0.07      0.40    -0.71     0.78 1.00\n##                                  Bulk_ESS Tail_ESS\n## sd(Intercept)                        1361     2213\n## sd(voicing)                          1471     1667\n## sd(prosodic_boundary)                2231     2030\n## cor(Intercept,voicing)               3407     2837\n## cor(Intercept,prosodic_boundary)     5026     3226\n## cor(voicing,prosodic_boundary)       4730     3394\n## \n## ~subject (Number of levels: 16) \n##                                  Estimate Est.Error l-95% CI u-95% CI Rhat\n## sd(Intercept)                       25.43      4.77    18.22    37.02 1.00\n## sd(voicing)                          5.54      2.18     1.61    10.28 1.00\n## sd(place1)                           3.96      1.57     0.92     7.28 1.00\n## sd(place2)                           1.43      0.74     0.15     3.05 1.00\n## sd(vowel1)                           9.12      2.34     5.42    14.53 1.00\n## sd(vowel2)                           4.66      1.06     2.91     7.14 1.00\n## sd(vowel3)                           0.68      0.52     0.03     1.94 1.00\n## sd(vowel4)                           1.34      0.54     0.31     2.52 1.00\n## sd(prosodic_boundary)                3.48      2.85     0.16    10.64 1.00\n## cor(Intercept,voicing)               0.33      0.22    -0.14     0.70 1.00\n## cor(Intercept,place1)               -0.21      0.23    -0.63     0.24 1.00\n## cor(voicing,place1)                 -0.23      0.25    -0.69     0.27 1.00\n## cor(Intercept,place2)                0.22      0.24    -0.27     0.64 1.00\n## cor(voicing,place2)                  0.11      0.27    -0.42     0.61 1.00\n## cor(place1,place2)                  -0.02      0.27    -0.53     0.51 1.00\n## cor(Intercept,vowel1)               -0.21      0.20    -0.58     0.19 1.00\n## cor(voicing,vowel1)                 -0.09      0.24    -0.54     0.40 1.00\n## cor(place1,vowel1)                   0.10      0.24    -0.37     0.54 1.00\n## cor(place2,vowel1)                   0.20      0.25    -0.32     0.66 1.00\n## cor(Intercept,vowel2)               -0.47      0.18    -0.78    -0.08 1.00\n## cor(voicing,vowel2)                 -0.03      0.24    -0.49     0.44 1.00\n## cor(place1,vowel2)                   0.10      0.24    -0.38     0.55 1.00\n## cor(place2,vowel2)                  -0.13      0.25    -0.59     0.38 1.00\n## cor(vowel1,vowel2)                   0.22      0.22    -0.23     0.62 1.00\n## cor(Intercept,vowel3)                0.00      0.27    -0.53     0.52 1.00\n## cor(voicing,vowel3)                  0.04      0.29    -0.50     0.58 1.00\n## cor(place1,vowel3)                  -0.03      0.28    -0.56     0.52 1.00\n## cor(place2,vowel3)                  -0.01      0.29    -0.56     0.56 1.00\n## cor(vowel1,vowel3)                  -0.03      0.29    -0.59     0.53 1.00\n## cor(vowel2,vowel3)                   0.10      0.28    -0.47     0.63 1.00\n## cor(Intercept,vowel4)               -0.16      0.23    -0.58     0.31 1.00\n## cor(voicing,vowel4)                  0.05      0.26    -0.46     0.55 1.00\n## cor(place1,vowel4)                   0.01      0.26    -0.51     0.52 1.00\n## cor(place2,vowel4)                  -0.10      0.26    -0.59     0.42 1.00\n## cor(vowel1,vowel4)                   0.04      0.24    -0.45     0.50 1.00\n## cor(vowel2,vowel4)                   0.39      0.23    -0.10     0.77 1.00\n## cor(vowel3,vowel4)                   0.09      0.28    -0.49     0.61 1.00\n## cor(Intercept,prosodic_boundary)     0.04      0.27    -0.51     0.56 1.00\n## cor(voicing,prosodic_boundary)       0.02      0.28    -0.53     0.56 1.00\n## cor(place1,prosodic_boundary)       -0.01      0.28    -0.55     0.53 1.00\n## cor(place2,prosodic_boundary)        0.06      0.28    -0.48     0.58 1.00\n## cor(vowel1,prosodic_boundary)        0.11      0.29    -0.48     0.63 1.00\n## cor(vowel2,prosodic_boundary)       -0.01      0.28    -0.55     0.53 1.00\n## cor(vowel3,prosodic_boundary)       -0.03      0.29    -0.57     0.54 1.00\n## cor(vowel4,prosodic_boundary)        0.00      0.28    -0.54     0.54 1.00\n##                                  Bulk_ESS Tail_ESS\n## sd(Intercept)                        1401     1751\n## sd(voicing)                          1633     1228\n## sd(place1)                           1269      954\n## sd(place2)                           1585     1432\n## sd(vowel1)                           2752     2983\n## sd(vowel2)                           2823     2874\n## sd(vowel3)                           2226     2278\n## sd(vowel4)                           1970     1376\n## sd(prosodic_boundary)                2486     2703\n## cor(Intercept,voicing)               4072     3055\n## cor(Intercept,place1)                4551     2958\n## cor(voicing,place1)                  3349     2872\n## cor(Intercept,place2)                4510     3135\n## cor(voicing,place2)                  4744     3169\n## cor(place1,place2)                   3759     3027\n## cor(Intercept,vowel1)                3911     2960\n## cor(voicing,vowel1)                  2119     2629\n## cor(place1,vowel1)                   2731     2891\n## cor(place2,vowel1)                   1892     2108\n## cor(Intercept,vowel2)                3959     3317\n## cor(voicing,vowel2)                  3400     2979\n## cor(place1,vowel2)                   2729     2682\n## cor(place2,vowel2)                   2708     2857\n## cor(vowel1,vowel2)                   4156     3519\n## cor(Intercept,vowel3)                6677     3109\n## cor(voicing,vowel3)                  5953     2826\n## cor(place1,vowel3)                   5178     2725\n## cor(place2,vowel3)                   3959     2857\n## cor(vowel1,vowel3)                   5850     3567\n## cor(vowel2,vowel3)                   4756     3049\n## cor(Intercept,vowel4)                5059     3348\n## cor(voicing,vowel4)                  4432     3514\n## cor(place1,vowel4)                   3982     3338\n## cor(place2,vowel4)                   3238     3125\n## cor(vowel1,vowel4)                   4307     3383\n## cor(vowel2,vowel4)                   3596     3457\n## cor(vowel3,vowel4)                   3126     3493\n## cor(Intercept,prosodic_boundary)     6602     3114\n## cor(voicing,prosodic_boundary)       6357     3276\n## cor(place1,prosodic_boundary)        5217     3160\n## cor(place2,prosodic_boundary)        4202     3272\n## cor(vowel1,prosodic_boundary)        4926     3252\n## cor(vowel2,prosodic_boundary)        4648     3258\n## cor(vowel3,prosodic_boundary)        2986     2818\n## cor(vowel4,prosodic_boundary)        3670     3552\n## \n## Regression Coefficients:\n##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept           157.26      6.62   144.18   170.64 1.00      662      857\n## voicing               9.57      2.14     5.34    13.73 1.00     2410     2755\n## place1              -10.00      2.89   -15.66    -4.22 1.00     2249     2617\n## place2                1.85      1.47    -1.17     4.74 1.00     2209     2637\n## vowel1               -0.24      4.24    -8.56     8.17 1.00     2801     2609\n## vowel2              -17.77      2.25   -22.13   -13.20 1.00     2139     2422\n## vowel3               -3.54      1.28    -6.06    -1.07 1.00     2422     2473\n## vowel4               -6.42      1.07    -8.44    -4.28 1.00     2112     2349\n## prosodic_boundary    14.14      2.96     8.39    19.94 1.00     4224     3003\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma    18.03      0.52    17.06    19.07 1.00     3790     2607\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nExercise 6.3 Verify that the estimates for the terms of interest for the RQs have barely changed between models neutralization_m10_3 and neutralization_m10_2:\n\nvoicing fixed effect\nBy-speaker voicing random slope SD\n\nNB:\n\nThis requires first determining the relevant row of ## Multilevel Hyperparameters: in the model output.\nThis does not require refitting the models, which might take a while on your computer (~6 minutes for student laptops in 2024). The model summaries are shown above.\n\n\n\n\nExercise 6.4 Refit neutralization_m10_3 as a frequentist model. What issue is there?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hierarchical Models 2</span>"
    ]
  },
  {
    "objectID": "week10.html#a-maximal-model",
    "href": "week10.html#a-maximal-model",
    "title": "8  Bayesian Hierarchical Models 2",
    "section": "8.4 A “Maximal” model",
    "text": "8.4 A “Maximal” model\n\n\nRegularizing correlations tends to change fixed-effect estimates little, while allowing complex random effect structures which are important for accurately estimating fixed effects.\nRecall, from Chap 8 and 10 of RMLD:\n\nRandom effect correlations were a big issue for frequentist mixed models.\nDifferent approaches to building up random effect structure (“maximal” vs. “data-driven” vs. “uncorrelated first”) all agree on the importance of adding many random slope terms.\nThe simplest recipe was to fit a “maximal” model (RMLD Chap. 8) , with all possible random slopes.\n\nThis often wasn’t feasible, because the random effect structure is too complex for the data (especially given the number of correlations), causing the model to be singular or not converge.\n\nThe basic issue was overfitted random effect structure.\n\n\n\nWith a Bayesian model, we can just set a regularizing prior on random effect terms, and fit a random effect structure with a large number of terms. In other words, “maximal” models are always possible. This is not magic – using a very complex random-effect structure for a small dataset just means you will get random-effect parameter estimates which reflect the prior more, and may lower power on fixed-effect estimates.6 But the practical fitting issues which bedevil us for frequentist mixed-effects models are gone.\n\nLet’s fit a maximal model to this data, using an \\(LKJ(2)\\) prior for correlations:\n\nprior_2 &lt;- c(\n  prior(normal(150, 75), class = Intercept), # beta_0\n  prior(normal(0, 50), class = b),\n  prior(exponential(0.02), class = sd), # random-effect SDs\n  prior(lkj(2), class = cor), # random-effect correlation matrices\n  prior(exponential(0.02), class = sigma) # residual variance\n)\n\n## by-speaker random slopes for all predictors\n## by-item random slopes for all predictors which vary within item.\nneutralization_m10_3 &lt;- brm(\n  data = neutralization,\n  family = gaussian,\n  vowel_dur ~ voicing + place + vowel + prosodic_boundary +\n    (1 + voicing + place + vowel + prosodic_boundary | subject) +\n    (1 + voicing + prosodic_boundary | item_pair),\n  prior = prior_2,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  file = \"models/neutralization_m10_3.brm\"\n)\n\n\nsummary(neutralization_m10_3)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: vowel_dur ~ voicing + place + vowel + prosodic_boundary + (1 + voicing + place + vowel + prosodic_boundary | subject) + (1 + voicing + prosodic_boundary | item_pair) \n##    Data: neutralization (Number of observations: 749) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~item_pair (Number of levels: 24) \n##                                  Estimate Est.Error l-95% CI u-95% CI Rhat\n## sd(Intercept)                        9.51      1.99     6.34    14.04 1.00\n## sd(voicing)                          3.52      2.04     0.20     7.81 1.00\n## sd(prosodic_boundary)                2.28      1.63     0.08     5.99 1.00\n## cor(Intercept,voicing)              -0.29      0.35    -0.85     0.50 1.00\n## cor(Intercept,prosodic_boundary)    -0.18      0.38    -0.82     0.61 1.00\n## cor(voicing,prosodic_boundary)       0.07      0.40    -0.71     0.78 1.00\n##                                  Bulk_ESS Tail_ESS\n## sd(Intercept)                        1361     2213\n## sd(voicing)                          1471     1667\n## sd(prosodic_boundary)                2231     2030\n## cor(Intercept,voicing)               3407     2837\n## cor(Intercept,prosodic_boundary)     5026     3226\n## cor(voicing,prosodic_boundary)       4730     3394\n## \n## ~subject (Number of levels: 16) \n##                                  Estimate Est.Error l-95% CI u-95% CI Rhat\n## sd(Intercept)                       25.43      4.77    18.22    37.02 1.00\n## sd(voicing)                          5.54      2.18     1.61    10.28 1.00\n## sd(place1)                           3.96      1.57     0.92     7.28 1.00\n## sd(place2)                           1.43      0.74     0.15     3.05 1.00\n## sd(vowel1)                           9.12      2.34     5.42    14.53 1.00\n## sd(vowel2)                           4.66      1.06     2.91     7.14 1.00\n## sd(vowel3)                           0.68      0.52     0.03     1.94 1.00\n## sd(vowel4)                           1.34      0.54     0.31     2.52 1.00\n## sd(prosodic_boundary)                3.48      2.85     0.16    10.64 1.00\n## cor(Intercept,voicing)               0.33      0.22    -0.14     0.70 1.00\n## cor(Intercept,place1)               -0.21      0.23    -0.63     0.24 1.00\n## cor(voicing,place1)                 -0.23      0.25    -0.69     0.27 1.00\n## cor(Intercept,place2)                0.22      0.24    -0.27     0.64 1.00\n## cor(voicing,place2)                  0.11      0.27    -0.42     0.61 1.00\n## cor(place1,place2)                  -0.02      0.27    -0.53     0.51 1.00\n## cor(Intercept,vowel1)               -0.21      0.20    -0.58     0.19 1.00\n## cor(voicing,vowel1)                 -0.09      0.24    -0.54     0.40 1.00\n## cor(place1,vowel1)                   0.10      0.24    -0.37     0.54 1.00\n## cor(place2,vowel1)                   0.20      0.25    -0.32     0.66 1.00\n## cor(Intercept,vowel2)               -0.47      0.18    -0.78    -0.08 1.00\n## cor(voicing,vowel2)                 -0.03      0.24    -0.49     0.44 1.00\n## cor(place1,vowel2)                   0.10      0.24    -0.38     0.55 1.00\n## cor(place2,vowel2)                  -0.13      0.25    -0.59     0.38 1.00\n## cor(vowel1,vowel2)                   0.22      0.22    -0.23     0.62 1.00\n## cor(Intercept,vowel3)                0.00      0.27    -0.53     0.52 1.00\n## cor(voicing,vowel3)                  0.04      0.29    -0.50     0.58 1.00\n## cor(place1,vowel3)                  -0.03      0.28    -0.56     0.52 1.00\n## cor(place2,vowel3)                  -0.01      0.29    -0.56     0.56 1.00\n## cor(vowel1,vowel3)                  -0.03      0.29    -0.59     0.53 1.00\n## cor(vowel2,vowel3)                   0.10      0.28    -0.47     0.63 1.00\n## cor(Intercept,vowel4)               -0.16      0.23    -0.58     0.31 1.00\n## cor(voicing,vowel4)                  0.05      0.26    -0.46     0.55 1.00\n## cor(place1,vowel4)                   0.01      0.26    -0.51     0.52 1.00\n## cor(place2,vowel4)                  -0.10      0.26    -0.59     0.42 1.00\n## cor(vowel1,vowel4)                   0.04      0.24    -0.45     0.50 1.00\n## cor(vowel2,vowel4)                   0.39      0.23    -0.10     0.77 1.00\n## cor(vowel3,vowel4)                   0.09      0.28    -0.49     0.61 1.00\n## cor(Intercept,prosodic_boundary)     0.04      0.27    -0.51     0.56 1.00\n## cor(voicing,prosodic_boundary)       0.02      0.28    -0.53     0.56 1.00\n## cor(place1,prosodic_boundary)       -0.01      0.28    -0.55     0.53 1.00\n## cor(place2,prosodic_boundary)        0.06      0.28    -0.48     0.58 1.00\n## cor(vowel1,prosodic_boundary)        0.11      0.29    -0.48     0.63 1.00\n## cor(vowel2,prosodic_boundary)       -0.01      0.28    -0.55     0.53 1.00\n## cor(vowel3,prosodic_boundary)       -0.03      0.29    -0.57     0.54 1.00\n## cor(vowel4,prosodic_boundary)        0.00      0.28    -0.54     0.54 1.00\n##                                  Bulk_ESS Tail_ESS\n## sd(Intercept)                        1401     1751\n## sd(voicing)                          1633     1228\n## sd(place1)                           1269      954\n## sd(place2)                           1585     1432\n## sd(vowel1)                           2752     2983\n## sd(vowel2)                           2823     2874\n## sd(vowel3)                           2226     2278\n## sd(vowel4)                           1970     1376\n## sd(prosodic_boundary)                2486     2703\n## cor(Intercept,voicing)               4072     3055\n## cor(Intercept,place1)                4551     2958\n## cor(voicing,place1)                  3349     2872\n## cor(Intercept,place2)                4510     3135\n## cor(voicing,place2)                  4744     3169\n## cor(place1,place2)                   3759     3027\n## cor(Intercept,vowel1)                3911     2960\n## cor(voicing,vowel1)                  2119     2629\n## cor(place1,vowel1)                   2731     2891\n## cor(place2,vowel1)                   1892     2108\n## cor(Intercept,vowel2)                3959     3317\n## cor(voicing,vowel2)                  3400     2979\n## cor(place1,vowel2)                   2729     2682\n## cor(place2,vowel2)                   2708     2857\n## cor(vowel1,vowel2)                   4156     3519\n## cor(Intercept,vowel3)                6677     3109\n## cor(voicing,vowel3)                  5953     2826\n## cor(place1,vowel3)                   5178     2725\n## cor(place2,vowel3)                   3959     2857\n## cor(vowel1,vowel3)                   5850     3567\n## cor(vowel2,vowel3)                   4756     3049\n## cor(Intercept,vowel4)                5059     3348\n## cor(voicing,vowel4)                  4432     3514\n## cor(place1,vowel4)                   3982     3338\n## cor(place2,vowel4)                   3238     3125\n## cor(vowel1,vowel4)                   4307     3383\n## cor(vowel2,vowel4)                   3596     3457\n## cor(vowel3,vowel4)                   3126     3493\n## cor(Intercept,prosodic_boundary)     6602     3114\n## cor(voicing,prosodic_boundary)       6357     3276\n## cor(place1,prosodic_boundary)        5217     3160\n## cor(place2,prosodic_boundary)        4202     3272\n## cor(vowel1,prosodic_boundary)        4926     3252\n## cor(vowel2,prosodic_boundary)        4648     3258\n## cor(vowel3,prosodic_boundary)        2986     2818\n## cor(vowel4,prosodic_boundary)        3670     3552\n## \n## Regression Coefficients:\n##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept           157.26      6.62   144.18   170.64 1.00      662      857\n## voicing               9.57      2.14     5.34    13.73 1.00     2410     2755\n## place1              -10.00      2.89   -15.66    -4.22 1.00     2249     2617\n## place2                1.85      1.47    -1.17     4.74 1.00     2209     2637\n## vowel1               -0.24      4.24    -8.56     8.17 1.00     2801     2609\n## vowel2              -17.77      2.25   -22.13   -13.20 1.00     2139     2422\n## vowel3               -3.54      1.28    -6.06    -1.07 1.00     2422     2473\n## vowel4               -6.42      1.07    -8.44    -4.28 1.00     2112     2349\n## prosodic_boundary    14.14      2.96     8.39    19.94 1.00     4224     3003\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma    18.03      0.52    17.06    19.07 1.00     3790     2607\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nExercise 8.3 Verify that the estimates for the terms of interest for the RQs have barely changed between models neutralization_m10_3 and neutralization_m10_2:\n\nvoicing fixed effect\nBy-speaker voicing random slope SD\n\n(This requires first determining the relevant rows of ## Multilevel Hyperparameters: in the model output.) \n\n\nExercise 8.4 Refit neutralization_m10_3 as a frequentist model. What happens?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Hierarchical Models 2</span>"
    ]
  },
  {
    "objectID": "week10.html#model-predictions-random-effects",
    "href": "week10.html#model-predictions-random-effects",
    "title": "6  Bayesian Hierarchical Models 2",
    "section": "6.4 Model predictions: random effects",
    "text": "6.4 Model predictions: random effects\nOur second RQ relates to by-subject differences in the effect of voicing. How can we examine these?\nIn the frequentist model, we could just look at the distribution of estimated voicing effects for each subject, using the coefficients() function:\n\ncoefficients(neut_m2)$subject\n##    (Intercept)   voicing    place1   place2     vowel1    vowel2    vowel3\n## 1     215.8745 19.435870 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 2     128.5247  4.827369 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 3     124.9758  4.233839 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 4     152.0437  8.760712 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 5     157.2912  9.638314 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 6     129.9029  5.057855 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 7     153.5643  9.015020 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 8     160.6845 10.205820 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 9     155.1717  9.283853 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 10    155.4794  9.335308 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 11    138.5167  6.498433 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 12    159.4236  9.994944 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 13    187.5889 14.705350 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 14    204.2916 17.498730 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 15    151.1975  8.619187 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 16    138.8313  6.551059 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n##       vowel4 prosodic_boundary\n## 1  -6.206707          11.76825\n## 2  -6.206707          11.76825\n## 3  -6.206707          11.76825\n## 4  -6.206707          11.76825\n## 5  -6.206707          11.76825\n## 6  -6.206707          11.76825\n## 7  -6.206707          11.76825\n## 8  -6.206707          11.76825\n## 9  -6.206707          11.76825\n## 10 -6.206707          11.76825\n## 11 -6.206707          11.76825\n## 12 -6.206707          11.76825\n## 13 -6.206707          11.76825\n## 14 -6.206707          11.76825\n## 15 -6.206707          11.76825\n## 16 -6.206707          11.76825\n\nEach subject’s predicted voicing slope is the seecond column.\nTo do the same for the Bayesian model, we can apply the same coefficients() function, but the syntax to access by-subject effects is different:\n\n## For the \"maximal\" model:\n# model predicted voicing effect for each speaker: estimated random effect + fixed-effect\ncoefficients(neutralization_m10_3)$subject[, , \"voicing\"]\n##     Estimate Est.Error       Q2.5    Q97.5\n## 1  18.349845  4.821197  9.3384185 27.84298\n## 2   8.295398  3.700706  1.2027796 15.77378\n## 3   2.060514  4.528705 -7.2423731 10.26555\n## 4   5.424699  4.110228 -3.2951036 12.72515\n## 5  13.966328  4.101631  6.6968663 22.96792\n## 6   6.940101  3.745876 -0.8041782 13.98368\n## 7   8.716751  3.650439  1.1978122 15.64319\n## 8  11.384075  3.534655  4.5988210 18.68192\n## 9  11.259319  3.875665  3.5712874 19.09196\n## 10  9.228546  3.788430  1.6255860 16.67072\n## 11  5.120790  3.883003 -3.2873839 12.04578\n## 12  8.022669  3.798239  0.1814080 15.38926\n## 13  9.947688  3.851994  2.1833171 17.59587\n## 14 12.149368  4.075520  4.5862079 20.48364\n## 15 11.847352  3.844718  4.7049233 19.66762\n## 16 10.997811  3.770996  3.8324372 18.78784\n\nNote the errors/CredI’s here, where frequentist model only outputs point estimates.\nPlot the distribution of these values:\n\n\nCode\ncoefficients(neutralization_m10_3)$subject[, , \"voicing\"] %&gt;% data.frame() %&gt;%\n  ggplot(aes(x = Estimate)) +\n  geom_density() +\n  geom_rug() +\n  geom_vline(aes(xintercept = 0), lty = 2) +\n  xlab(\"Speaker voicing effect estimates\")\n\n\n\n\n\n\n\n\n\nThe model predicts that all subjects have positive voicing effects.\nThese are the by-subject offsets we would see in the posterior summary (posterior_summary(neutralization_m10_3)), added to the model’s estimate for b_voicing.\n\n\n\n\nWhat’s shown in the coefficients() output is one of two ways to capture uncertainty in speaker voicing effects—incorporating  uncertainty in the fixed effect coefficient and the by-speaker random effects:7\n\nCompute predictions for observed subjects, with PIs.\nCompute predictions for new subjects, with PIs.\n\nThese two ways, illustrated in McElreath (2020) Sec. 13.5.1-13.5.2, are subtly but importantly different. (1) is shown above. It would would be appropriate if we are interested in these particular subjects. (2) is appropriate if we are interested in what the model predicts about individual differences, but not these particular subjects (they are just a random sample). (2) incorporates an extra source of uncertainty, in the degree of by-subject variability in the voicing effect.\nWe can implement (2) as follows:\n\nDraw values for (a) \\(\\beta_{voicing}\\) and (b) the by-subject voicing slope SD\nUse these to simulate the voicing effect for one new subject.\nDo this 2000 times.\nPlot the distribution.\n\nThis can be thought of as the posterior predictive distribution of the voicing effect for a random new speaker. Workflow adapted from Kurz (2021), Sec. 13.5.2:\n\n# nbumber of simulated subjects\nn_sim &lt;- 2000\n\nsub_voicing_sim &lt;- neutralization_m10_3 %&gt;%\n  # draw (a) and (b)\n  spread_draws(b_voicing, sd_subject__voicing, ndraws = n_sim) %&gt;%\n  ## for each draw, simulate a single new speaker's voicing effect\n  mutate(sub_voicing = rnorm(n(), mean = b_voicing, sd = sd_subject__voicing))\n\nThese estimates look like:\n\nsub_voicing_sim %&gt;% head()\n## # A tibble: 6 × 6\n##   .chain .iteration .draw b_voicing sd_subject__voicing sub_voicing\n##    &lt;int&gt;      &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;               &lt;dbl&gt;       &lt;dbl&gt;\n## 1      2        295  1295      7.25                4.24      6.25  \n## 2      1        137   137     10.0                 5.99      3.39  \n## 3      1        262   262     11.1                 8.58     12.5   \n## 4      2        643  1643      7.89                6.64      9.29  \n## 5      1        970   970      6.34                5.62      1.56  \n## 6      4        370  3370     11.5                 7.76     -0.0381\n\nPlot the distribution:\n\n\nCode\nsub_voicing_sim %&gt;% ggplot(aes(x = sub_voicing)) +\n  geom_density(fill = \"grey\") +\n  xlab(\"By-speaker voicing effect\") +\n  geom_vline(aes(xintercept = 0), lty = 2) +\n  ylab(\"Posterior predictive distribution density\")\n\n\n\n\n\n\n\n\n\n\nExercise 6.5 Both density plots immediately above show a distribution of model-predicted voicing effects, across subjects, calculated in two different ways.\n\nHow are they different? (width? mode?)\nWhy do these plots look different?\nWhat would the answer to RQ2 be using the first plot? Using the second plot? Which one makes more sense / why?\n\n\n\nExercise 6.6 (Extra) Before drawing any firm conclusions about what the model predicts about individual differences, it would be good to examine uncertainty in these predictions.\n\nMake a spaghetti plot showing uncertainty in the distribution of by-speaker voicing effects.\n\n\nThat is: draw 50 new values of b_voicing and sd_subject__voicing. For each draw, simulate 100 new subjects, and plot the distribution of their voicing effects. The resulting plot should show 50 densities/distributions, overplotted.\n\n\nHow, if at all, has the answer to “what does this model say about RQ2” changed relative to the plot showing just a single distribution of by-speaker voicing effects?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKirby, James, and Morgan Sonderegger. 2018. “Mixed-Effects Design Analysis for Experimental Phonetics.” Journal of Phonetics 70: 70–85.\n\n\nKurz, S. 2021. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hierarchical Models 2</span>"
    ]
  },
  {
    "objectID": "hw5.html",
    "href": "hw5.html",
    "title": "16  Homework 5",
    "section": "",
    "text": "16.1 Preliminaries\nLoad libraries we’ll need:\nlibrary(tidyverse)\nlibrary(arm)\n\nlibrary(GGally)\nYou should submit a single .zip folder (see Piazza for deadline/format).\nYou are welcome to work together , but you must write up your code and (prose) results independently. See the syllabus for the collaboration and Generative AI policy.\nMake sure that your plots and prose are polished and legible.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "hw5.html#preliminaries",
    "href": "hw5.html#preliminaries",
    "title": "16  Homework 5",
    "section": "",
    "text": "If you’re using rmd, the folder should contain:\n\nThe final .rmd file\nThe compiled .html file\n\nIf you’re using qmd, it should contain:\n\nThe final .qmd file\nThe compiled .html file\nThe _files folder, needed to render the html file.\n\n\n\n\n\n16.1.1 Data\nLoad data:\n\n### load diatones data\ndiatones &lt;- read.csv(\"https://osf.io/tqjm8/download\", stringsAsFactors = TRUE)\n\n## make numeric versions of all categorical predictors, while saving original versions\ndiatones &lt;- diatones %&gt;% mutate(\n  syll1_coda_orig = syll1_coda,\n  syll2_coda_orig = syll2_coda,\n  syll2_td_orig = syll2_td,\n### turns no/yes -&gt; 0/1\nsyll1_coda = ifelse(syll1_coda=='no', 0, 1),\n### turns '0'/'C'/'CC'/'CCC' -&gt; 0/1/2/3\nsyll2_coda = str_count(syll2_coda_orig, \"C\"), \nsyll2_td = ifelse(syll2_td=='no', 0, 1)\n)\n\n### standardize all predictors using arm::rescale \ndiatones &lt;- diatones %&gt;% mutate(syll1_coda = arm::rescale(syll1_coda_orig),\n                                syll2_td = arm::rescale(syll2_td_orig),\n                                syll2_coda = arm::rescale(syll2_coda),\n                                frequency = arm::rescale(frequency)\n)\n\n\n## get Roettger et al. German incomplete neutralization data\nneutralization &lt;- read.csv('https://osf.io/qg5fc/download', stringsAsFactors=TRUE) %&gt;%\n  mutate(voicing_fact = fct_relevel(voicing, 'voiceless')) %&gt;% filter(!is.na(prosodic_boundary)) %&gt;%\n  mutate(prosodic_boundary = rescale(prosodic_boundary),\n         voicing = rescale(voicing_fact),\n         item_pair = as.factor(item_pair),\n         subject=as.factor(subject)\n  )\n\n## helmert coding for multi-level factors, so all predictors are \"centered\"\ncontrasts(neutralization$vowel) &lt;- contr.helmert\ncontrasts(neutralization$place) &lt;- contr.helmert\n\n## get Grawunder incomplete neutralization data\n\ngrawunder_df &lt;- read.delim(\"https://osf.io/7vbsy/download\", sep='\\t', stringsAsFactors = TRUE) %&gt;%\n  mutate(voicing_fact = fct_relevel(voicing, 'voiceless'))  %&gt;%\n  mutate(voicing = rescale(voicing_fact)) %&gt;%\n  rename(item_pair=pair, vowel_dur=dur) ### rename some predictors to have the same names as neutralization data\n\n\n## helmert coding for multi-level factors, so all predictors are \"centered\"\ncontrasts(grawunder_df$vowel) &lt;- contr.helmert\ncontrasts(grawunder_df$place) &lt;- contr.helmert\ncontrasts(grawunder_df$population) &lt;- contr.helmert\ncontrasts(grawunder_df$stimtype) &lt;- contr.helmert\n\n## stimtype = German / Tirol (subject-level)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "hw5.html#question-1-40",
    "href": "hw5.html#question-1-40",
    "title": "9  Homework 5",
    "section": "9.2 Question 1 (40%)",
    "text": "9.2 Question 1 (40%)\nThis question involves finding and summarizing a research paper that uses bayesian methods. See Piazza for details and deadline.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "hw5.html#question-2-30",
    "href": "hw5.html#question-2-30",
    "title": "9  Homework 5",
    "section": "9.3 Question 2 (30%)",
    "text": "9.3 Question 2 (30%)\nDo the following exercises from the two Hierarchical Bayesian Models labs, using objects fitted in those labs. (You may need to copy some code here to create those objects.)\n(a). From HBM 1, end of section 2:\n\nExplain how this works: what two parameters are added to make up pred_logit?\nUse these draws to plot the posterior of the probability of shifted_stress for words with prefix de.\n\n(b). From HBM 1, end of section 4:\n\nUsing the bayesian model neutralization_m81, test the hypothesis that subjects vary more than items, using the hypothesis function discussed in Week 4. What does the “posterior probability” that it reports mean?\nMake two plots of the marginal effect of vowel for model neutralization_m81: one plot for an “average item/speaker” (CIs), and another for a new observation (PIs: method = posterior_predict).\n\nc). From HBM 2, end of section 3:\n\nDo parts (1)-(3) of this exercise.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "hw5.html#question-3-30",
    "href": "hw5.html#question-3-30",
    "title": "10  Homework 5",
    "section": "10.3 Question 3 (30%)",
    "text": "10.3 Question 3 (30%)\nThe dataframe grawunder_df contains data from Grawunder (2014), another study of incomplete neutralization in German, with a similar (but not identical) design to the Roettger et al. study which the neutralization data is from (described in RMLD).1 These predictors have the same meanings as in neutralization:\n\nvowel\nplace\nvoicing / voicing_fact (of primary interest)\nitem_pair: 28 items\nsubject: 18 subjects\n\nThe main difference between the studies is that Grawunder’s subjects are 19 Austrian German speakers. Standard German (whether in German or Austria) has “neutralized” voicing in word-final stops, but the Tirol dialect of Austrian German maintains a voiced/voiceless distinction; all subjects have exposure to this dialect and the standard. All subjects heard the same words, but for some subjects words were produced in the Austrian dialect, and for others words were produced in the Standard. This subject-level variable is indicated by population:\n\npopulation: levels = Austrian German, German (9 subjects each)\n\nThat is, all subjects are performing the same task—repeating words, whose vowel durations are then measured—but with the prompts spoken in different accents. Since all subjects have exposure to both dialects (in their daily life), subjects in the  Austrian and German groups may differ depending on population, in one of several ways:\n\nIn the effect of voicing\nIn vowel duration (across all data)\nIn the degree of individual variability in the voicing effect\nIn the degree of individual variability in vowel durations (across all data)\n\nHere is an empirical plot, from which you can assess all four:\n\n### mean vowel duration by voicing, per subject, for Austrian and German subjects\ngrawunder_df %&gt;% ggplot(aes(x=voicing_fact, y=vowel_dur)) + \n  stat_summary(fun.data='mean_cl_boot', geom='line', aes(group=subject)) + facet_wrap(~population)\n\n\n\n\n\n\n\n\n(a). Just from this plot, how do you think Austrian and German groups might differ (from 1-4)?\n(b). Fit a first model to the Grawunder data, using the same structure as neutralization_m10_1 from Week 10’s lab, with two added terms:\n\nFixed effect of population\nFixed effect for population:voicing interaction\n\nYou can use the same prior for these terms as for other \\(\\beta\\) terms in neutralization_m10_1.\n(For your interpretation of the results, note the processing steps that were done above—all factors have been “centered”, and voicing is a numeric, centered predictor.)\nCall this model grawunder_m1.\n(c). Make an appropriate marginal effect plot to understand what this model says about the most basic question here—how large is the voicing effect for subjects in the Austrian and German groups? What is the 95% CredI for the voicing effect for an average population=Austrian subject? For an average population=German subject?\n(d). What does the model say about (1)?\n(e). Which of (1)-(4) are not captured by this model? Explain why the model cannot capture this/these kind(s) of variability.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "hw5.html#question-4",
    "href": "hw5.html#question-4",
    "title": "10  Homework 5",
    "section": "10.4 Question 4",
    "text": "10.4 Question 4\nThis question is optional. It is worth up to 25% extra credit, which will count towards your course homework grade.\n(a). Make a plot of the model’s predicted vowel_dur, by voicing, for each speaker, divided up by population. That is, it should look like the empirical plot above (except the lines come from model predictions). Comparing to the empirical plot, how does the model prediction plot reflect your answer to the previous question?\n\nWe consider a second model, which fits separate by-speaker random effects for population=Austrian and German subjects. That is:\n\nThere are by-speaker intercepts, and `voicing slopes for German speakers, with one set of variances (+ one correlation)\nThere are by-speaker intercepts, and `voicing slopes for Austrian speakers, with another set of variances (+ one correlation)\n\n(b). You can fit this model, which should be called grawunder_m2, by replacing the (1+voicing|subject) term with (1+voicing|gr(subject, by=population)). Do so.\n(c). Which terms in the model address (3) and (4)? Explain.\n(d). Carry out hypothesis tests (using hypothesis) assessing (3) and (4) for this model.\n\n\n\n\nGrawunder, Sven. 2014. “Wie Schaukt a Pruag Aos?-Stabile Phonetische Unterschiede in Wortformen Nach Auslautverhärtung in Tirol.” In Sprechwissenschaft: Bestand, Prognose, Perspektive, 209–20. Peter Lang.\n\n\nNicenboim, Bruno, Timo B Roettger, and Shravan Vasishth. 2018. “Using Meta-Analysis for Evidence Synthesis: The Case of Incomplete Neutralization in German.” Journal of Phonetics 70: 39–55.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "hw5.html#footnotes",
    "href": "hw5.html#footnotes",
    "title": "16  Homework 5",
    "section": "",
    "text": "I got this data from the OSF page for Nicenboim, Roettger, and Vasishth (2018), a meta-analysis of German incomplete neutralization across many studies.↩︎\nHint: this involves a small change to the (1+voicing|subject) term, which would not be possible in an lme4 model. You’ll need to look beyond the lecture notes.↩︎",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "hw5.html#question-1-50",
    "href": "hw5.html#question-1-50",
    "title": "16  Homework 5",
    "section": "16.2 Question 1 (50%)",
    "text": "16.2 Question 1 (50%)\nDo three of the following exercises from the Hierarchical Bayesian Models chapters (Chapters 5, 6, 7) using objects there. (You may need to copy some code here to create those objects.) You may do 1-2 more of the exercises for extra credit.\n\nExercise 5.1, parts (a)-(b).\nExercise 5.3, part (a), with a small change: your figures should treat syll1_coda and syll2_td as factors.\n\n\n\nFor example, for the first plot, there should be two curves, each labeled with one level of syll1_coda, and the legend should show no/yes, not numeric values.\n\n\nExercise 6.1, part (b)\nExercise 7.1\nExercise 7.2, parts (a)-(c)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "week11.html",
    "href": "week11.html",
    "title": "7  Bayesian Hierarchical Models 3",
    "section": "",
    "text": "7.1 Preliminaries\nLoad libraries we will need:\nlibrary(brms)\nlibrary(lme4)\nlibrary(arm)\nlibrary(tidyverse)\n\nlibrary(tidybayes)\nlibrary(bayestestR)\n\nlibrary(bayesplot)\nlibrary(loo)\n\nlibrary(broom) ## for tidy model summaries\nlibrary(broom.mixed) ## for tidy model summaries for lme4 models\n\nlibrary(patchwork)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Hierarchical Models 3</span>"
    ]
  },
  {
    "objectID": "week11.html#preliminaries",
    "href": "week11.html#preliminaries",
    "title": "7  Bayesian Hierarchical Models 3",
    "section": "",
    "text": "Practical notes\n\n\n\n\n\n\nIf you have loaded rethinking, you need to detach it before using brms. See Kurz (2023) Sec. 4.3.1.\nI use the file argument when fitting brms models to make compiling this document easier (so the models don’t refit every time I compile). You may or may not want to do this for your own models. See file and file_refit arguments in ?brm.\nHere I set the file_refit option so “brms will refit the model if model, data or algorithm as passed to Stan differ from what is stored in the file.”\n\n\noptions(brms.file_refit = \"on_change\")\n\n\nI use chains = 4, cores = 4 when fitting brm models below—this means 4 chains, each to be run on one core on my laptop. cores = 4 may need to be adjusted for your computer. (You may have fewer or more cores; I have 8 cores, so this leaves 50% free.) You should figure out how to use multiple cores on your machine.\nMake numbers be printed only to 3 digits, for neater output:\n\n\noptions(digits = 3)\n\n\n\n\n\n7.1.1 Data\nLoad and preprocess the neutralization data, as in Section 6.1.1:\n\nneutralization &lt;- read.csv(\"https://osf.io/qg5fc/download\", stringsAsFactors = TRUE) %&gt;%\n  mutate(voicing_fact = fct_relevel(voicing, \"voiceless\")) %&gt;%\n  filter(!is.na(prosodic_boundary)) %&gt;%\n  mutate(\n    prosodic_boundary = rescale(prosodic_boundary),\n    voicing = rescale(voicing_fact),\n    item_pair = as.factor(item_pair),\n    subject = as.factor(subject)\n  )\n\n## Code multi-level factors with Helmert contrasts\n## so that all predictors are centered\ncontrasts(neutralization$vowel) &lt;- contr.helmert\ncontrasts(neutralization$place) &lt;- contr.helmert",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Hierarchical Models 3</span>"
    ]
  },
  {
    "objectID": "week11.html#hypothesis-testing",
    "href": "week11.html#hypothesis-testing",
    "title": "7  Bayesian Hierarchical Models 3",
    "section": "7.2 Hypothesis testing",
    "text": "7.2 Hypothesis testing\nThe neutralization data is a good case to illustrate aspects of hypothesis testing for Bayesian models:\n\nThe distinction between “existence” and “significance” measures\nModel comparison using prediction (LOO, WAIC) vs. likelihood (Bayes factor).\nHow tests differ for fixed and random effect terms.\n\n(a) and (b) were introduced for non-hierarchical models in Section 3.7, while (c) is new for hierarchical models.\nWe will use the final model of the neutralization data fitted in the last chapter, which we refit as neutralization_m11_1. This is the same model as neutralization_m10_3, but with this argument added, to save all parameter draws:\n\nsave_pars = save_pars(all = TRUE)\n\nis necessary for calculating Bayes Factors in general (see here). We’ll fit all models in this chapter using this argument, just to be consistent.\n\n## same prior as prior_2 in previous chapter\nprior_11_1 &lt;- c(\n  prior(normal(150, 75), class = Intercept), # beta_0\n  prior(normal(0, 50), class = b),\n  prior(exponential(0.02), class = sd), # random-effect SDs\n  prior(lkj(2), class = cor), # random-effect correlation matrices\n  prior(exponential(0.02), class = sigma) # residual variance\n)\n\nneutralization_m11_1 &lt;- brm(\n  data = neutralization,\n  family = gaussian,\n  vowel_dur ~ voicing + place + vowel + prosodic_boundary +\n    (1 + voicing + place + vowel + prosodic_boundary | subject) +\n    (1 + voicing + prosodic_boundary | item_pair),\n  prior = prior_11_1,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  save_pars = save_pars(all = TRUE),\n  file = \"models/neutralization_m11_1.brm\"\n)\n\n\n7.2.1 Fixed effects\n\n7.2.1.1 Existence and significance\nThe existence of the voicing fixed effect is clear from the posterior of neutralization_m11_1. We can confirm this by calculating \\(p_d\\) (see Section 3.7.1):\n\npd(neutralization_m11_1)\n## Probability of Direction\n## \n## Parameter         |     pd\n## --------------------------\n## (Intercept)       |   100%\n## voicing           |   100%\n## place1            | 99.98%\n## place2            | 89.92%\n## vowel1            | 54.10%\n## vowel2            |   100%\n## vowel3            | 99.25%\n## vowel4            |   100%\n## prosodic_boundary |   100%\n\nFor significance, we calculate \\(p_{\\text{ROPE}}\\) (see Section 3.7.2), assuming a rope of [-5 msec, 5 msec]:\n\nrope(neutralization_m11_1, range = c(-5, 5), ci = 1)\n## # Proportion of samples inside the ROPE [-5.00, 5.00]:\n## \n## Parameter         | inside ROPE\n## -------------------------------\n## Intercept         |      0.00 %\n## voicing           |      1.47 %\n## place1            |      4.88 %\n## place2            |     97.52 %\n## vowel1            |     78.70 %\n## vowel2            |      0.00 %\n## vowel3            |     89.25 %\n## vowel4            |      9.53 %\n## prosodic_boundary |      0.18 %\n\nWe can be fairly confident that the voicing effect is practically significant (because \\(p_{\\text{ROPE}}\\) is near 0).\n\nExercise 7.1 Recall that it’s not actually clear what the ROPE should be for the neutralization data: anywhere between 5 and 10 msec could be argued to be functionally equivalent to 0 (Section 6.3.1).\nHow would our conclusion change, if at all, if the ROPE were [-8, 8]? [-10, 10]?\n\n\n\n7.2.1.2 Bayes factor vs. LOO comparison\nRecall that to calculate Bayes Factors, we should re-fit the model with more iterations (Section 3.7.3), at least 40,000 posterior samples. This is left as an exercise for all Bayes Factor calculations in this chapter—since re-fitting each model may take a long time on your computer (each one takes a couple of minutes on mine), we’ll just compute Bayes Factors using our current models (fitted with fewer iterations) and assume they are accurate enough.1 For a “real” analysis, make sure to use enough iterations.\nNow, calculate the Bayes Factor for the voicing fixed effect using our current model:\n\nbf_pointnull(neutralization_m11_1)\n## Sampling priors, please wait...\n## Warning: Bayes factors might not be precise.\n##   For precise Bayes factors, sampling at least 40,000 posterior samples is\n##   recommended.\n## Bayes Factor (Savage-Dickey density ratio)\n## \n## Parameter         |       BF\n## ----------------------------\n## (Intercept)       | 1.51e+23\n## voicing           |    56.66\n## place1            |     6.45\n## place2            |    0.067\n## vowel1            |    0.090\n## vowel2            | 4.01e+05\n## vowel3            |    0.939\n## vowel4            |   146.80\n## prosodic_boundary |   266.52\n## \n## * Evidence Against The Null: 0\n\nThe voicing row assesses: how significant is the voicing fixed effect?\nBF = 10-30 is “strong evidence” and 30-100 is “very strong”, so there is substantial evidence that the effect is meaningful.\nAnother way to assess the importance of voicing would be to use LOO (Section 4.2) to compare two models, with and without the voicing term:\n\nneutralization_m11_1\nThe same model, with the voicing fixed effect excluded.\n\n(Note that we don’t need extra iterations for LOO comparison.)\nFit the second model:\n\n## same as neutralization_m11_1, with the 'voicing'\n## fixed effect removed\nneutralization_m11_2 &lt;- brm(\n  data = neutralization,\n  family = gaussian,\n  vowel_dur ~  place + vowel + prosodic_boundary +\n    (1 + voicing + place + vowel + prosodic_boundary | subject) +\n    (1 + voicing + prosodic_boundary | item_pair),\n  prior = prior_11_1,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  save_pars = save_pars(all = TRUE),\n  file = \"models/neutralization_m11_2.brm\"\n)\n\n\n\n\n\n\n\nPractical note: updating a model\n\n\n\n\n\nThe update() command is an elegant way to re-fit models with terms added or removed. For example, instead of the brm() call just above, we could do:\n\nneutralization_m11_2 &lt;- update(neutralization_m11_1, . ~ . - voicing, chains = 4, cores = 4, iter = 2000)\n\nI am not using update() commands in these notes, just for clarity.\n\n\n\nTo do the model comparison, we first compute LOO for both models:\n\n## Compute LOO for both models\nneutralization_m11_1 &lt;- add_criterion(neutralization_m11_1, c(\"loo\"), cores = 4)\nneutralization_m11_2 &lt;- add_criterion(neutralization_m11_2, c(\"loo\"), cores = 4)\n\n\n\n\n\nModel comparison:\n\nloo_compare(neutralization_m11_1, neutralization_m11_2)\n##                      elpd_diff se_diff\n## neutralization_m11_1  0.0       0.0   \n## neutralization_m11_2 -1.5       2.3\n\nSuppose we are using the “more conservative” method from Section 4.2.1: choose the simplest model that doesn’t differ from others by more than two standard errors.\nThe models are not clearly different in terms of predictive accuracy. This is because the voicing term improves predictive accuracy little—it is a small effect, as expected. But it is clearly not zero, or in the ROPE, which is what matters for our research questions.\nThis illustrates how “improves predictive accuracy” does not necessarily line up with “scientifically important”, a point emphasized by McElreath (2020) (e.g. Sec. 7.5.1): a model with better predictive performance isn’t necessarily the one that more accurately reflects reality (= has the correct causal structure).2  \n\nExercise 7.2 One application of model comparison is checking whether a factor (= predictor with multiple levels) significantly contributes to a model.\n\nConsider the contribution of place to model neutralization_m11_1. Why can’t we just example a \\(p_d\\) value to assess this predictor’s contribution to the model?\nCarry out model comparisons using BF and LOO to assess whether place significantly contributes.\n\nNote that calculating the BF here works differently from above—you need to fit two models (say: m1 and m2) with and without the term(s) of interest, then compare them using bayes_factor(m1, m2). An example is shown just below.\n\n\nExtra: Re-do (b), now operationalizing “does place significantly contribute?” as “do the fixed and random effects involving place matter?” (rather than just the fixed effects).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Random effects\nFor a random-effect SD, there is no \\(p_{d}\\) to calculate—the SD must be positive.\nLet’s consider the by-subject voicing slope SD, relevant for research question 2 for the neutralization data (Section 6.3.1).\nTo address “existence” of the by-speaker random slopes, we could compare models with and without this term, via Bayes Factor or LOO. Let’s again use LOO for model comparison.\nWe’ll need to fit a subset model, dropping the the by-speaker random slope of voicing and related correlation terms:\n\n\n## 'voicing' removed from by-subject random effects\nneutralization_m11_3 &lt;- brm(\n  data = neutralization,\n  family = gaussian,\n  vowel_dur ~ voicing + place + vowel + prosodic_boundary +\n    (1 + place + vowel + prosodic_boundary | subject) +\n    (1 + voicing + prosodic_boundary | item_pair),\n  prior = prior_11_1,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  save_pars = save_pars(all = TRUE),\n  file = \"models/neutralization_m11_3.brm\"\n)\n\n\nModel comparison:\n\nneutralization_m11_3 &lt;- add_criterion(neutralization_m11_3, c(\"loo\"), cores = 4, moment_match = TRUE) # moment_match used to avoid a warning you get otherwise\nloo_compare(neutralization_m11_1, neutralization_m11_3)\n##                      elpd_diff se_diff\n## neutralization_m11_1  0.0       0.0   \n## neutralization_m11_3 -2.0       3.6\n\nThe interpretation is similar to the model comparison above, assuming we’re again using the “simplest model that doesn’t differ by more than 2 SE” rule: allowing speakers to vary in the voicing effect doesn’t improve predictive performance.\nFor “significance” of this random effect, we’ll consider ways to calculate \\(p_{rope}\\) for the by-speaker random slope of voicing. There is no standard ROPE width, i.e. an answer to “how much by-speaker variability is effectively the same as no by-speaker variabilty”? We’ll consider two intuitive options.\nFirst, we could make use of the same domain-specific knowledge as when defining ROPE for a fixed-effect coeffficient above: that 5 msec is a very small effect. We could say that the ROPE is [0, 1.25], where 1.25 is 5/4, since this would mean that 95% of speakers differ in the voicing effect by 5 msec (= \\(\\pm\\) 2 SD).\nUse hypothesis() to compute: what percentage of the posterior for the by-speaker random-slope SD for voicing, which we’ll call \\(\\sigma_{voicing}\\), lies below 1.25?\n\n# this illustrates one way to write random effect terms in brms::hypothesis\nhypothesis(neutralization_m11_1, \"voicing &lt; 1.25\", class = \"sd\", group = \"subject\")\n## Hypothesis Tests for class sd_subject:\n##             Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n## 1 (voicing)-(1.25) &lt; 0     4.33       2.2     1.09     8.24       0.02\n##   Post.Prob Star\n## 1      0.02     \n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\nAnother possibility, which doesn’t require domain-specific knowledge, would be to define “speakers vary very little” relative to the voicing fixed effect: perhaps 1/10 of its magnitude.\nThis hypothesis() call asks what percentage of the time \\(\\sigma_voicing\\) is less than 10% of the magnitude of \\(\\beta_{voicing}\\) (the fixed effect).\n\n# this illustrates another way to write random effect terms in brms::hypothesis\nhypothesis(neutralization_m11_1, \"sd_subject__voicing &lt; abs(b_voicing)/10\", class = NULL)\n## Hypothesis Tests for class :\n##                 Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n## 1 (sd_subject__voic... &lt; 0     4.62       2.2     1.31     8.57       0.01\n##   Post.Prob Star\n## 1      0.01     \n## ---\n## 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n## '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n## for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n## Posterior probabilities of point hypotheses assume equal prior probabilities.\n\nNote that this call takes into account uncertainty in the fixed effect (which is good), by calculating in what percentage of posterior draws \\(\\sigma_{voicing} &lt; \\beta_{voicing}/10\\).\nBy either option, \\(p_{rope}\\) is about 0.02, suggesting that speakers differ meaningfully in the voicing effect.\n\nExercise 7.3  \n\nWe might also define “significance” (effect size) in terms of individual subjects. Let X = “percentage of subjects with voicing effects outside the ROPE”. This measures the proportion of subjects who have functionally zero voicing effects. Suppose the ROPE is [-5,5].\n\n\nThe sub_voicing_sim dataframe, defined in Section 6.4, contains posterior draws of by-subject voicing effects. Use this to calculate X.\n\n\n\n\n\n\nInterpret the results of the LOO model comparison, on the one hand, and a “signficance” calculation done above (this could be either your answer to (b), or a \\(p_{\\text{ROPE}}\\) calculated above). What does the model say, qualitatively, about interspeaker variability?\nExtra Calculate a Bayes Factor comparing neutralization_m11_1 and neutralization_m11_3.\n\n\n\nDo you need to increase the number of iterations used when fitting these models? (How can you check?)\n\n\nPart (b) of this exercise is an example of how a Bayesian model can be interpreted intuitively, using a quantity of interest. We define a quantity \\(X\\) that addresses our question, then calculate the posterior distribution of \\(X\\). Here, \\(X\\) is “percentage of subjects with voicing effects larger than 5 msec [the just-noticeable difference for vowel duration]”.\n\n\n7.2.3 Fixed and random effects\nTo assess the question “does \\(X\\) matter?”, it often makes sense to ask a fitted model, “how much do all fixed and random-effect terms involving \\(X\\) contribute?”\nFor our example, we could assess “whether voicing matters”—which combines RQs 1 and 2—by fitting a new version of neutralization_m11_1 without any voicing information. Fit this model:\n\nprior_2 &lt;- c(\n  prior(normal(150, 75), class = Intercept), # beta_0\n  prior(normal(0, 50), class = b),\n  prior(exponential(0.02), class = sd), # random-effect SDs\n  prior(lkj(2), class = cor), # random-effect correlation matrices\n  prior(exponential(0.02), class = sigma) # residual variance\n)\n\nneutralization_m11_4 &lt;- brm(\n  data = neutralization,\n  family = gaussian,\n  vowel_dur ~ place + vowel + prosodic_boundary +\n    (1 + place + vowel + prosodic_boundary | subject) +\n    (1 +  prosodic_boundary | item_pair),\n  prior = prior_2,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  save_pars = save_pars(all = TRUE),\n  file = \"models/neutralization_m11_4.brm\"\n)\n\nModel comparison using LOO:\n\n## first, calculate LOO for the subset\n## model\nneutralization_m11_4 &lt;- add_criterion(neutralization_m11_4, c(\"loo\"), cores = 4)\n\n\nloo_compare(neutralization_m11_1, neutralization_m11_4)\n##                      elpd_diff se_diff\n## neutralization_m11_1   0.0       0.0  \n## neutralization_m11_4 -28.7       8.3\n\nLet’s also compute a Bayes Factor, to show an example of comparing two models differing in multiple terms.3\n\nbayesfactor_models(neutralization_m11_1, neutralization_m11_4, verbose = FALSE)\n## Warning: logml could not be estimated within maxiter, rerunning with adjusted starting value. \n## Estimate might be more variable than usual.\n## Bayes Factors for Model Comparison\n## \n##     Model                                                                                                                             BF\n## [2] place + vowel + prosodic_boundary + (1 + place + vowel + prosodic_boundary | subject) + (1 + prosodic_boundary | item_pair) 5.10e-09\n## \n## * Against Denominator: [1] voicing + place + vowel + prosodic_boundary + (1 + voicing + place + vowel + prosodic_boundary | subject) + (1 + voicing + prosodic_boundary | item_pair)\n## *   Bayes Factor Type: marginal likelihoods (bridgesampling)\n\nThe Bayes Factor here is ~0.\n\nExercise 7.4 Interpret the BF and LOO results, from just above, qualitatively: what do they say about the question “does voicing matter”?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Hierarchical Models 3</span>"
    ]
  },
  {
    "objectID": "week4.html#sec-brm1-ess",
    "href": "week4.html#sec-brm1-ess",
    "title": "3  Bayesian Regression Models 1",
    "section": "3.7 Effect size and significance",
    "text": "3.7 Effect size and significance\nTo test a point hypothesis for a Bayesian model (or an interval hypothesis: \\(\\beta\\) is v. small), different methods measure effect existence of parameter \\(\\beta\\) (is it zero?) and significance (or size)—the latter meaning “important enough to care about”. Note that “significance” here is different from frequentist “significance” from \\(p\\)-values.\nMakowski, Ben-Shachar, and Lüdecke (2019) discuss four kinds of indices, assuming frequentist \\(p\\)-values as a starting point (their Table 3 summarizes):\n\nprobability of direction (\\(pd\\))\nMAP probability (we won’t consider this)\nBayes factor (BF)\n\nMust define a null model:\nPoint-null (most common)\nVersus the ROPE\n\nROPE (region of practical equivalence)\n\nPercentage of 95% (or 89%) CI for \\(\\beta\\) that contains ROPE\nPercentage of whole posterior for \\(\\beta\\) that contains ROPE (most common?)\n\n\n\n\n\n\n\n(From Makowski, Ben-Shachar, and Lüdecke (2019)) “Bayesian indices of effect existence and significance. (A) The probability of Direction (pd) is defined as the proportion of the posterior distribution that is of the median’s sign (the size of the yellow area relative to the whole distribution). (B) The MAP-based p-value is defined as the density value at 0 – the height of the red lollipop, divided by the density at the Maximum A Posteriori (MAP) – the height of the blue lollipop. (C) The percentage in ROPE corresponds to the red area relative to the distribution [with or without tails for ROPE (full) and ROPE (95%), respectively]. (D) The Bayes factor (vs. 0) corresponds to the point-null density of the prior (the blue lollipop on the dotted distribution) divided by that of the posterior (the red lollipop on the yellow distribution), and the Bayes factor (vs. ROPE) is calculated as the odds of the prior falling within vs. outside the ROPE (the blue area on the dotted distribution) divided by that of the posterior (the red area on the yellow distribution).”\n\n\n\n\n\nFor futher understanding, the bayestestR vignettes are helpful, e.g. on probability of direction or Bayes factors.\n\\(pd\\) and MAP probability measure existence, and don’t provide useful info about evidence for the null. BF and ROPE methods measure significance (mostly), and can be used for evidence for the null.\nWe will show examples with functions from bayestestR.\n\n3.7.1 Indices of existence\nMost commonly reported is \\(p_d\\), which is conceptually similar to a \\(p\\)-value (but subtracted from 1).\nCalculate \\(pd\\) for the two multicollinearity models of english_25 from above:\n\np_direction(english_collin_m1)\n## Probability of Direction\n## \n## Parameter          |     pd\n## ---------------------------\n## (Intercept)        |   100%\n## WrittenFrequency_c | 83.03%\n## Familiarity_c      | 95.58%\n## SubjectYoung       |   100%\np_direction(english_collin_m2)\n## Probability of Direction\n## \n## Parameter          |     pd\n## ---------------------------\n## (Intercept)        |   100%\n## WrittenFrequency_c | 98.22%\n## Familiarity_c      | 99.12%\n## SubjectYoung       |   100%\n\nThis is what we’d expect from examining the posteriors/CIs—similar interpretation to \\(p\\)-values. With a stronger prior, there is more evidence for the existence of both WrittenFrequency_c and Familiarity_c effects.\nFor the diatones_m41 model:\n\np_direction(diatones_m41)\n## Probability of Direction\n## \n## Parameter   |     pd\n## --------------------\n## (Intercept) |   100%\n## syll2_coda  | 98.05%\n## syll2_td    | 91.50%\n## frequency   | 89.62%\n## syll1_coda  | 98.65%\n\nThere is progressively more evidence for the existence of the frequency, syll2_td, syll2_coda, and syll1_coda effects. If we impose a cutoff analagous to \\(p &lt; 0.05\\), we’d say that only the syll1_coda and syll2_coda effects “exist”.\n\n\n3.7.2 Indices of significance: ROPE\nTo use a ROPE method (including BF-ROPE), we must define a region of values of the parameter of interest (e.g. \\(\\beta_1\\)) that we consider practically equivalent to 0. Kruschke (2018) suggests -0.1/0.1 and -0.18/0.18 as defaults for linear and logistic regression, on a standardized parameter scale. This means a change of 0.1 SD (for a linear regression) in \\(y\\) is considered the same as 0, for a standardized predictor (it’s been \\(z\\)-scored). For example, for RTlexdec for english_25, 0.1 SD of \\(y\\) is:\n\nsd(english_25$RTlexdec) * 0.1\n## [1] 0.01550091\n\nSo the ROPE for \\(\\beta_1\\) is \\([-0.016, 0.016]\\). This -0.1/0.1 interval is based on Cohen’s “negligible” effect size, and should be changed based on domain knowledge when available (see e.g. here). You also need to adjust the ROPE when the predictor is not standardized (so that a change of 1 SD in \\(x\\) corresponds to a change of 0.1 SD in \\(y\\)).\nThe ROPE is not appropriate when predictors are highly correlated (see here), so we don’t apply it to our collinearity examples.\n\n3.7.2.1 Example\nLet’s instead calculate \\(p_{ROPE}\\) for the diatones_m41 model, assuming the default is OK (-0.18/0.18, as this is a logistic regression):11\n\nrope(diatones_m41, ci = 1)\n## # Proportion of samples inside the ROPE [-0.18, 0.18]:\n## \n## Parameter  | inside ROPE\n## ------------------------\n## Intercept  |      0.00 %\n## syll2_coda |      2.67 %\n## syll2_td   |      9.10 %\n## frequency  |     14.10 %\n## syll1_coda |      2.33 %\n\nFor each coefficient, \\(p_{ROPE}\\) is the percentage of the posterior that lies inside the ROPE. These values are fairly similar to \\(pd\\) (e.g. compare 1-0.26 to 98.28% for syll2_coda), with some differences.\nTo see the differences in interpretation better, let’s consider \\(p_{ROPE}\\) for the 95%HDI: the percentage of the 95% HDI of each parameter that lies inside the ROPE.\n\nrope(diatones_m41, ci = 0.95)\n## # Proportion of samples inside the ROPE [-0.18, 0.18]:\n## \n## Parameter  | inside ROPE\n## ------------------------\n## Intercept  |      0.00 %\n## syll2_coda |      1.08 %\n## syll2_td   |      9.58 %\n## frequency  |     14.84 %\n## syll1_coda |      0.26 %\n\nNote that syll1_coda now has \\(p_{rope} &lt; 0.005\\), but \\(pd\\) around 0.99. The model is very certain that the effect is important, and slightly less sure of its direction/existence. This is weird, but possible. More common is a situation where \\(pd\\) indicates strong evidence but \\(p_{rope}\\) does not—the model is sure the effect exists, but not that it is large enough to be meaningful.\n\n\n\n3.7.3 Indices of significance: Bayes Factors\nBayes factors are both widely used and controversial. The central issues are sensitivity to the prior, and proper interpretation. Schad et al. (2023) (= Nicenboim, Schad, and Vaishth (2024) Chap. 16 is a nice but very detailed discussion. This page is shorter, with references. These sources make clear that Bayes Factors are not a method where you should just use a package’s defaults without thinking. We’ll show a couple examples.\nBayes factors can be used to compare any two models. In practice they are often used to assess evidence in favor of each effect in a model, either using: - A null interval (\\(\\beta\\) is in the ROPE) - A point-null hypothesis (\\(\\beta = 0\\)).\nThe point-null version is most common. In either case, the interpretation is “how much more credible has the absence of an effect become, given the observed data?”\nThere are different scales for interpreting a BF; see “Interpretation” on the Wikipedia page. Most common is Jeffrey’s scale:\n\nvalues of 1–3.16 are evidence “barely worth mentioning” in favor of the model; thus, values of 0.31–1 are evidence “barely worth mentioning” in favor of the null model.\nvalues of 3.16–10 are “substantial” evidence for the model.\nvalues of 10–31: “strong” evidence\n\nAs a first example, the ROPE-based BF for our diatones_m31 model, with very weak priors, is:\n\nbf_rope(diatones_m41)\n## Sampling priors, please wait...\n## Bayes Factor (Null-Interval)\n## \n## Parameter   |       BF\n## ----------------------\n## (Intercept) | 9.53e+04\n## syll2_coda  |     1.80\n## syll2_td    |    0.476\n## frequency   |    0.292\n## syll1_coda  |     2.52\n## \n## * Evidence Against The Null: [-0.181, 0.181]\n\nThus, we would say there is:\n\n“Barely” evidence for effects of syll2_coda and syll1_coda\nLittle evidence either for or against the effects of syll2_td\nWeak evidence against an effect of frequency.\n\nA nice property of Bayes Factors is that they allow us to assess evidence for or against an effect, with sufficiently a sufficiently small BF providing evidence “for the null”. This is an important contrast to frequentist \\(p\\)-values, which only allow us to conclude “significant” (effect “exists”) versus “not significant” (effect could exist or not). High \\(p\\)-values are frequently misinterpreted as providing evdience “for the null”.12\n\n\n\n\n\n\nPractical note\n\n\n\n\n\nIt is common when interpreting Bayes Factors to use a scale like the one given above to reach a decision—most commonly that an effect “exists” if BF &gt; 3 or 10, “doesn’t exist” if BF &lt; 1/3 or 1/10, and “we can’t say” if 1/3 &lt; BF &lt; 3 or 1/10 &lt; BF &lt; 10. Schad et al. (2023) argues this is not a good idea, in the absence of a utility function that tells us the consequences of correct and incorrect decisions:\n\n“In the cognitive sciences, because it is often unclear how to define good utility functions, we argue that Bayesian decision making is premature: Inferences based on continuous Bayes factors should be reported instead of decisions.”\n\n(They then show a complicated procedure by which you could still define a decision rule, given your data/model/priors.)\nI think the upshot is similar to other applications of Bayesian methods: we should ideally report continuous measures summarizing our models (like \\(pd\\), a Bayes Factor, a 95% CredI) and try to resist applying discrete cutoffs in interpretation. So for example, while we probably shouldn’t say for the diatones_m41 model that there is “no effect” of frequency, we could say there is less evidence for a frequency effect than for a syll1_coda effect (smaller BF).\n\n\n\n\nAs examples of computing point-null BFs, we’ll consider the two english models fit above with weaker priors, with \\(n=250\\) and \\(n=25\\) observations: these are english_m43 and english_m46 above.\nCalculating point-null BFs uses “bridge sampling”, which requires a much larger number of posterior samples than usual to give a precise answer. It is recommended to use at least 40k posterior samples, or as many as needed to give the same result when you calculate Bayes Factors several times. (I have used 80k posterior samples below because 40k wasn’t enough for this criterion.)\nSo we first refit the models with more samples:\n\n## Note: 40k samples per chain x 4 chains, \n## of which 50% are discarded as warm-up = \n## 80k samples total\nenglish_m43_1 &lt;- brm(\n    data = english_250,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n        prior(normal(0, 100), class = Intercept),\n        prior(normal(0, 5), class = b),\n        prior(exponential(1), class = sigma)\n    ), chains = 4, iter = 40000, cores = 4,\n    file = \"models/english_m43_1.brm\"\n)\n\nenglish_m46_1 &lt;- brm(\n    data = english_25,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n        prior(normal(0, 100), class = Intercept),\n        prior(normal(0, 5), class = b),\n        prior(exponential(1), class = sigma)\n    ), chains = 4, iter = 40000, cores = 4,\n    file = \"models/english_m46_1.brm\"\n)\n\nenglish_m47_1 &lt;-\n  brm(\n    data = english_25,\n    RTlexdec ~ 1 + WrittenFrequency_c + SubjectYoung,\n    family = gaussian,\n    prior = c(\n      prior(normal(6.5, 0.5), class = Intercept),\n      prior(normal(-0.05, 0.025), class = b, coef = WrittenFrequency_c),\n      prior(normal(-0.5, 0.75), class = b, coef = SubjectYoung),\n      prior(exponential(1), class = sigma)\n    ), chains = 4, iter = 40000, cores = 4,\n    file = \"models/english_m47_1.brm\"\n  )\n\nThen calculate point-null Bayes Factors:\n\nbf_pointnull(english_m43_1)\n## Sampling priors, please wait...\n## Bayes Factor (Savage-Dickey density ratio)\n## \n## Parameter          |       BF\n## -----------------------------\n## (Intercept)        |      Inf\n## WrittenFrequency_c | 3.76e+14\n## SubjectYoung       | 4.39e+25\n## \n## * Evidence Against The Null: 0\nbf_pointnull(english_m46_1) \n## Sampling priors, please wait...\n## Bayes Factor (Savage-Dickey density ratio)\n## \n## Parameter          |       BF\n## -----------------------------\n## (Intercept)        |      Inf\n## WrittenFrequency_c |     4.73\n## SubjectYoung       | 1.50e+03\n## \n## * Evidence Against The Null: 0\n\nCompare to \\(pd\\) for each model:\n\np_direction(english_m43_1)\n## Probability of Direction\n## \n## Parameter          |   pd\n## -------------------------\n## (Intercept)        | 100%\n## WrittenFrequency_c | 100%\n## SubjectYoung       | 100%\np_direction(english_m46_1)\n## Probability of Direction\n## \n## Parameter          |     pd\n## ---------------------------\n## (Intercept)        |   100%\n## WrittenFrequency_c | 99.99%\n## SubjectYoung       |   100%\n\nIn both cases, the model is very sure the effect exists. But the model fitted to just 25 observations gives only moderate evidence for the WrittenFrequency_c effect’s significance versus the null, while the model fitted to 250 observations gives very strong evidence. This is counterintuitive at first, but follows from how \\(p_d\\) (and \\(p\\)-values) are defined:\n\n\\(pd\\): the observed data is unlikely if \\(\\beta_1=0\\) (for \\(n=25\\))\nBF: the observed data is about as likely if \\(\\beta_1=0\\) (under this prior) as when \\(\\beta_1 \\ne 0\\)\n\nWe see another interesting pattern comparing \\(pd\\) and BF for the models fit to the english_25 data, with the weaker versus stronger priors:\n\np_direction(english_m46_1)\n## Probability of Direction\n## \n## Parameter          |     pd\n## ---------------------------\n## (Intercept)        |   100%\n## WrittenFrequency_c | 99.99%\n## SubjectYoung       |   100%\np_direction(english_m47_1)\n## Probability of Direction\n## \n## Parameter          |      pd\n## ----------------------------\n## (Intercept)        |    100%\n## WrittenFrequency_c | 100.00%\n## SubjectYoung       |    100%\n\n\nbf_pointnull(english_m46_1)\n## Sampling priors, please wait...\n## Bayes Factor (Savage-Dickey density ratio)\n## \n## Parameter          |       BF\n## -----------------------------\n## (Intercept)        |      Inf\n## WrittenFrequency_c |     4.77\n## SubjectYoung       | 1.46e+03\n## \n## * Evidence Against The Null: 0\nbf_pointnull(english_m47_1)\n## Sampling priors, please wait...\n## Bayes Factor (Savage-Dickey density ratio)\n## \n## Parameter          |       BF\n## -----------------------------\n## (Intercept)        |      Inf\n## WrittenFrequency_c | 1.17e+03\n## SubjectYoung       | 8.39e+03\n## \n## * Evidence Against The Null: 0\n\nAgain, the \\(pd\\) values are the same: the data is unlikely if \\(\\beta_0\\) in each case, for the WrittenFrequency_c and SubjectYoung parameters. The prior does not matter for this calculation.\nBut the BF values are much higher for the model with the strong prior. This is because the BF considers how likely the data is relative to the prior. Under the “strong” prior, \\(\\beta\\) values of 0 for WrittenFrequency_c and SubjectYoung are unlikely—we defined the prior to capture that these coefficients are probably negative. So the model with a strong prior thinks there is strong evidence for both effects, relative to the prior; the model with a weak prior thinks there is much weaker evidence.\n\nExercise 3.8  \n\nFor the dyads model fitted in Exercise 3.6: assume the default ROPE, and calculate \\(pd\\) and either \\(p_{rope}\\) or BF (point-null). What do these indices say about the (two) effects of interest?\nWrite up these results in a paragraph, following the model from Makowski, Ben-Shachar, and Lüdecke (2019).\n\n\n\n\n3.7.4 Recommendations\nSince they give different information, it seems like a good idea to use one index of each type (existence, significance) when reporting results, if you are summarizing effects using indices.\nCommon practice (for linguistic data) is currently to report in the regression table, for each effect (one row):\n\nEstimate\nError\n95% CredI\n\\(pd\\)\n\nwhich basically replicates a frequentist regression model table (replacing \\(p\\) by \\(pd\\)). This doesn’t give any information about importance/significance of effects, and leaves deciding what an “important” value is up to the reader. A better option would be to include a column including BF or \\(p_{ROPE}\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Regression Models 1</span>"
    ]
  },
  {
    "objectID": "week11.html#footnotes",
    "href": "week11.html#footnotes",
    "title": "7  Bayesian Hierarchical Models 3",
    "section": "",
    "text": "To confirm this is OK, you can re-run each Bayes Factor-computing command (e.g. bf_pointnull(), bayes_factor()) a few times and check that the BF doesn’t change much.↩︎\n“This result just echoes the core fact about WAIC (and CV and PSIS [a.k.a LOO]): It guesses predictive accuracy, not causal truth. A variable can be causally related to an outcome, but have little relative im pact on it, and WAIC [or LOO] will tell you that. That is what is happening in this case. We can use WAIC/CV/PSIS to measure how big a difference some variable makes in prediction. But we cannot use these criteria to decide whether or not some effect exists.”↩︎\nHere we’ve used the bayesfactor_models() function from the easytestR package. This is just a wrapper to functionality from the bridgesampling package Gronau, Singmann, and Wagenmakers (2020), which is doing the real work.↩︎\nThe mismatch in shape the right end of the distribution turns out to be because a log-transformation of vowel_dur is appropriate. You can try fitting a model of log(vowel_dur), or a model of vowel_dur with family=lognormal, to check.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Hierarchical Models 3</span>"
    ]
  },
  {
    "objectID": "week10.html#sec-hbm2-mp",
    "href": "week10.html#sec-hbm2-mp",
    "title": "6  Bayesian Hierarchical Models 2",
    "section": "6.4 Model predictions: random effects",
    "text": "6.4 Model predictions: random effects\nOur second RQ relates to by-subject differences in the effect of voicing. How can we examine these?\nIn the frequentist model, we could just look at the distribution of estimated voicing effects for each subject, using the coefficients() function:\n\ncoefficients(neut_m2)$subject\n##    (Intercept)   voicing    place1   place2     vowel1    vowel2    vowel3\n## 1     215.8745 19.435870 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 2     128.5247  4.827369 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 3     124.9758  4.233839 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 4     152.0437  8.760712 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 5     157.2912  9.638314 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 6     129.9029  5.057855 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 7     153.5643  9.015020 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 8     160.6845 10.205820 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 9     155.1717  9.283853 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 10    155.4794  9.335308 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 11    138.5167  6.498433 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 12    159.4236  9.994944 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 13    187.5889 14.705350 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 14    204.2916 17.498730 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 15    151.1975  8.619187 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n## 16    138.8313  6.551059 -10.59214 2.120172 -0.3999683 -17.54189 -3.184979\n##       vowel4 prosodic_boundary\n## 1  -6.206707          11.76825\n## 2  -6.206707          11.76825\n## 3  -6.206707          11.76825\n## 4  -6.206707          11.76825\n## 5  -6.206707          11.76825\n## 6  -6.206707          11.76825\n## 7  -6.206707          11.76825\n## 8  -6.206707          11.76825\n## 9  -6.206707          11.76825\n## 10 -6.206707          11.76825\n## 11 -6.206707          11.76825\n## 12 -6.206707          11.76825\n## 13 -6.206707          11.76825\n## 14 -6.206707          11.76825\n## 15 -6.206707          11.76825\n## 16 -6.206707          11.76825\n\nEach subject’s predicted voicing slope is the seecond column.\nTo do the same for the Bayesian model, we can apply the same coefficients() function, but the syntax to access by-subject effects is different:\n\n## For the \"maximal\" model:\n# model predicted voicing effect for each speaker: estimated random effect + fixed-effect\ncoefficients(neutralization_m10_3)$subject[, , \"voicing\"]\n##     Estimate Est.Error       Q2.5    Q97.5\n## 1  18.349845  4.821197  9.3384185 27.84298\n## 2   8.295398  3.700706  1.2027796 15.77378\n## 3   2.060514  4.528705 -7.2423731 10.26555\n## 4   5.424699  4.110228 -3.2951036 12.72515\n## 5  13.966328  4.101631  6.6968663 22.96792\n## 6   6.940101  3.745876 -0.8041782 13.98368\n## 7   8.716751  3.650439  1.1978122 15.64319\n## 8  11.384075  3.534655  4.5988210 18.68192\n## 9  11.259319  3.875665  3.5712874 19.09196\n## 10  9.228546  3.788430  1.6255860 16.67072\n## 11  5.120790  3.883003 -3.2873839 12.04578\n## 12  8.022669  3.798239  0.1814080 15.38926\n## 13  9.947688  3.851994  2.1833171 17.59587\n## 14 12.149368  4.075520  4.5862079 20.48364\n## 15 11.847352  3.844718  4.7049233 19.66762\n## 16 10.997811  3.770996  3.8324372 18.78784\n\nNote the errors/CredI’s here, where frequentist model only outputs point estimates.\nPlot the distribution of these values:\n\n\nCode\ncoefficients(neutralization_m10_3)$subject[, , \"voicing\"] %&gt;% data.frame() %&gt;%\n  ggplot(aes(x = Estimate)) +\n  geom_density() +\n  geom_rug() +\n  geom_vline(aes(xintercept = 0), lty = 2) +\n  xlab(\"Speaker voicing effect estimates\")\n\n\n\n\n\n\n\n\n\nThe model predicts that all subjects have positive voicing effects.\nThese are the by-subject offsets we would see in the posterior summary (posterior_summary(neutralization_m10_3)), added to the model’s estimate for b_voicing.\n\n\n\n\nWhat’s shown in the coefficients() output is one of two ways to capture uncertainty in speaker voicing effects—incorporating  uncertainty in the fixed effect coefficient and the by-speaker random effects:7\n\nCompute predictions for observed subjects, with PIs.\nCompute predictions for new subjects, with PIs.\n\nThese two ways, illustrated in McElreath (2020) Sec. 13.5.1-13.5.2, are subtly but importantly different. (1) is shown above. It would would be appropriate if we are interested in these particular subjects. (2) is appropriate if we are interested in what the model predicts about individual differences, but not these particular subjects (they are just a random sample). (2) incorporates an extra source of uncertainty, in the degree of by-subject variability in the voicing effect.\nWe can implement (2) as follows:\n\nDraw values for (a) \\(\\beta_{voicing}\\) and (b) the by-subject voicing slope SD\nUse these to simulate the voicing effect for one new subject.\nDo this 2000 times.\nPlot the distribution.\n\nThis can be thought of as the posterior predictive distribution of the voicing effect for a random new speaker. Workflow adapted from Kurz (2023), Sec. 13.5.2:\n\n# nbumber of simulated subjects\nn_sim &lt;- 2000\n\nsub_voicing_sim &lt;- neutralization_m10_3 %&gt;%\n  # draw (a) and (b)\n  spread_draws(b_voicing, sd_subject__voicing, ndraws = n_sim) %&gt;%\n  ## for each draw, simulate a single new speaker's voicing effect\n  mutate(sub_voicing = rnorm(n(), mean = b_voicing, sd = sd_subject__voicing))\n\nThese estimates look like:\n\nsub_voicing_sim %&gt;% head()\n## # A tibble: 6 × 6\n##   .chain .iteration .draw b_voicing sd_subject__voicing sub_voicing\n##    &lt;int&gt;      &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;               &lt;dbl&gt;       &lt;dbl&gt;\n## 1      2        295  1295      7.25                4.24      6.25  \n## 2      1        137   137     10.0                 5.99      3.39  \n## 3      1        262   262     11.1                 8.58     12.5   \n## 4      2        643  1643      7.89                6.64      9.29  \n## 5      1        970   970      6.34                5.62      1.56  \n## 6      4        370  3370     11.5                 7.76     -0.0381\n\nPlot the distribution:\n\n\nCode\nsub_voicing_sim %&gt;% ggplot(aes(x = sub_voicing)) +\n  geom_density(fill = \"grey\") +\n  xlab(\"By-speaker voicing effect\") +\n  geom_vline(aes(xintercept = 0), lty = 2) +\n  ylab(\"Posterior predictive distribution density\")\n\n\n\n\n\n\n\n\n\n\nExercise 6.5 Both density plots immediately above show a distribution of model-predicted voicing effects, across subjects, calculated in two different ways.\n\nHow are they different? (width? mode?)\nWhy do these plots look different?\nWhat would the answer to RQ2 be using the first plot? Using the second plot? Which one makes more sense / why?\n\n\n\nExercise 6.6 (Extra) Before drawing any firm conclusions about what the model predicts about individual differences, it would be good to examine uncertainty in these predictions.\n\nMake a spaghetti plot showing uncertainty in the distribution of by-speaker voicing effects.\n\n\nThat is: draw 50 new values of b_voicing and sd_subject__voicing. For each draw, simulate 100 new subjects, and plot the distribution of their voicing effects. The resulting plot should show 50 densities/distributions, overplotted.\n\n\nHow, if at all, has the answer to “what does this model say about RQ2” changed relative to the plot showing just a single distribution of by-speaker voicing effects?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKirby, James, and Morgan Sonderegger. 2018. “Mixed-Effects Design Analysis for Experimental Phonetics.” Journal of Phonetics 70: 70–85.\n\n\nKurz, S. 2023. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hierarchical Models 2</span>"
    ]
  },
  {
    "objectID": "hw5.html#question-2-50",
    "href": "hw5.html#question-2-50",
    "title": "16  Homework 5",
    "section": "16.3 Question 2 (50%)",
    "text": "16.3 Question 2 (50%)\nThe dataframe grawunder_df contains data from Grawunder (2014), another study of incomplete neutralization in German, with a similar (but not identical) design to the Roettger et al. study which the neutralization data is from (described in RMLD).1 These predictors have the same meanings as in neutralization:\n\nvowel\nplace\nvoicing / voicing_fact (of primary interest)\nitem_pair: 28 items\nsubject: 18 subjects\n\nThe main difference between the studies is that Grawunder’s subjects are 19 Austrian German speakers. Standard German (whether in German or Austria) has “neutralized” voicing in word-final stops, but the Tirol dialect of Austrian German maintains a voiced/voiceless distinction; all subjects have exposure to this dialect and the standard. All subjects heard the same words, but for some subjects words were produced in the Austrian dialect, and for others words were produced in the Standard. This subject-level variable is indicated by population:\n\npopulation: levels = Austrian German, German (9 subjects each)\n\nThat is, all subjects are performing the same task—repeating words, whose vowel durations are then measured—but with the prompts spoken in different accents. Since all subjects have exposure to both dialects (in their daily life), subjects in the  Austrian and German groups may differ depending on population, in one of several ways:\n\nIn the effect of voicing\nIn vowel duration (across all data)\nIn the degree of individual variability in the voicing effect\nIn the degree of individual variability in vowel durations (across all data)\n\nHere is an empirical plot, from which you can assess all four:\n\n### mean vowel duration by voicing, per subject, for Austrian and German subjects\ngrawunder_df %&gt;% ggplot(aes(x=voicing_fact, y=vowel_dur)) + \n  stat_summary(fun.data='mean_cl_boot', geom='line', aes(group=subject)) + facet_wrap(~population)\n\n\n\n\n\n\n\n\n(a). Just from this plot, how do you think Austrian and German groups might differ (from 1-4)?\n(b). Fit a first model to the Grawunder data, using the same structure as neutralization_10_2 from Section 6.3.4, except without prosodic_boundary (which doesn’t exist for this dataset) and with two added terms:\n\nFixed effect of population\nFixed effect for population:voicing interaction\n\nYou can use the same prior for these terms as for other \\(\\beta\\) terms in neutralization_m10_2.\n(For your interpretation of the results, note the processing steps that were done above—all factors have been “centered”, and voicing is a numeric, centered predictor.)\nCall this model grawunder_m1.\n(c). Make an appropriate marginal effect plot to understand what this model says about the most basic question here—how large is the voicing effect for subjects in the Austrian and German groups? What is the 95% CredI for the voicing effect for an average population=Austrian subject? For an average population=German subject?\n(d). What does the model say about (1)?\n(e). Which of (1)-(4) are not captured by this model? Explain why the model cannot capture this/these kind(s) of variability.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "hw5.html#question-4-optional",
    "href": "hw5.html#question-4-optional",
    "title": "12  Homework 5",
    "section": "12.4 Question 4 (Optional)",
    "text": "12.4 Question 4 (Optional)\nThis question is worth up to 20% extra credit.\n(a). Make a plot of the model’s predicted vowel_dur, by voicing, for each speaker, divided up by population. That is, it should look like the empirical plot above (except the lines come from model predictions). Comparing to the empirical plot, how does the model prediction plot reflect your answer to the previous question?\n\nWe consider a second model, which fits separate by-speaker random effects for population=Austrian and German subjects. That is:\n\nThere are by-speaker intercepts, and `voicing slopes for German speakers, with one set of variances (+ one correlation)\nThere are by-speaker intercepts, and `voicing slopes for Austrian speakers, with another set of variances (+ one correlation)\n\n(b). Figure out how to fit this model, which should be called grawunder_m2. Do so.2\n(c). Which terms in the model address (3) and (4)? Explain.\n(d). Carry out hypothesis tests (using hypothesis) assessing (3) and (4) for this model.\n\n\n\n\nGrawunder, Sven. 2014. “Wie Schaukt a Pruag Aos?-Stabile Phonetische Unterschiede in Wortformen Nach Auslautverhärtung in Tirol.” In Sprechwissenschaft: Bestand, Prognose, Perspektive, 209–20. Peter Lang.\n\n\nNicenboim, Bruno, Timo B Roettger, and Shravan Vasishth. 2018. “Using Meta-Analysis for Evidence Synthesis: The Case of Incomplete Neutralization in German.” Journal of Phonetics 70: 39–55.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "week11.html#distributional-regression",
    "href": "week11.html#distributional-regression",
    "title": "7  Bayesian Hierarchical Models 3",
    "section": "7.3 Distributional regression",
    "text": "7.3 Distributional regression\nModels so far in this course always only involve the mean of a distribution, which is modeled as a function of predictors, by-subject variability, etc. But other parameters could vary as well; such distributional regression models are easily fitted using brms/Stan (vignette). We’ll consider models where the residual variability, \\(\\sigma\\) (“degree of noise”) is modeled, for linear regression.\nThis kind of model could let us address research questions involving (by-observation) variability. (Example: for VOT data, are voiced stops “more variable” than voiceless stops?)\nWe will fit a distributional extension of our neutralization model, following the steps shown in Nicenboim, Schad, and Vaishth (2024) Sec. 5.1.6.\nOne situation where we’d need such a model is when the research questions are about “noise”/variability—as in Ciaccio and Verı́ssimo (2022) (who also give a tutorial on these models using brms). Another example, from phonetics, is Sonderegger, Stuart-Smith, and Mielke (2023) (code here).\nBut, most of the time, our research questions will not be about this kind of variability. How might we decide we need such a model, even if the RQs don’t involve noise?\nAcross all data, a posterior predictive check of our neutralization model looks decent:4\n\npp_check(neutralization_m11_1, ndraws = 50, type = \"dens_overlay\")\n\n\n\n\n\n\n\n\nHowever, we know that subjects differ a lot in how they speak during a production experiment. Some will speak formally, others casually; some may have had more sleep than others; and so on. This could lead to the degree of “noise” differing by subject. To see if this could be happening, consider the same plot, by-subject:\n\nppc_dens_overlay_grouped(neutralization$vowel_dur,\n  yrep =\n    posterior_predict(neutralization_m11_1,\n      ndraws = 100\n    ),\n  group = neutralization$subject\n)\n\n\n\n\n\n\n\n\nIt does look like the degree of noise may vary by subject, e.g. 13, 14, 15. We can check this more directly by doing a PP check for the standard deviation, by subject:\n\npp_check(neutralization_m11_1,\n  type = \"stat_grouped\",\n  ndraws = 1000,\n  group = \"subject\",\n  stat = \"sd\"\n)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFor some subjects, the observed value of SD lies outside the model’s predicted values, suggesting allowing residual SD to differ by-subject could make sense.\nFor the sake of this example, we might also wonder whether the amount of noise differs by voicing:\n\nppc_dens_overlay_grouped(neutralization$vowel_dur,\n  yrep =\n    posterior_predict(neutralization_m11_1,\n      ndraws = 100\n    ),\n  group = neutralization$voicing_fact\n)\n\n\n\n\n\n\n\n\n(This could be part of the search for any difference in vowel_duration by voicing.) It seems unlikely from this plot, but we will include a term in the model to confirm.\nThis model will allow residual SD is allowed to differ by-subject, and by voicing. The SD for subject \\(j\\) is modeled as:\n\\[\\begin{align}\n\\sigma_j & = \\exp(\\sigma + \\beta^*_{voicing} + u_j) \\\\\nu_j & \\sim N(0, \\tau) \\\\\n\\end{align}\\]\nHere, \\(\\exp(\\sigma)\\) is the “average” residual SD; \\(\\beta^*_{voicing}\\) is the difference in residual SD between voicing=-0.5 and 0.5 (in log space), and \\(u_j\\) is the offset for subject \\(j\\).\nThe exponential parametrization is used so that \\(\\sigma_j\\) stays positive for all subjects. For priors, we use:\n\n\\(\\sigma \\sim N(0, \\log(50))\\) : because \\(\\sigma\\) is now inside an exponential, and its old prior had width 50.\n\\(\\beta^*_{voicing} \\sim N(0, \\log(50))\\): similar\n\\(\\tau \\sim \\exp(0.25)\\) : because log(50) is the scale, and 1/log(50) \\(\\approx\\) 0.25.\n\nReminder: if you are not comfortable determining weakly-informative priors, it’s always an option to just use brms’ default priors (see Section 6.2). This would correspond to just omitting the prior argument in the brm() call below.\n\nprior_6 &lt;- c(\n  prior(normal(150, 75), class = Intercept), # beta_0\n  prior(normal(0, 50), class = b),\n  prior(exponential(0.02), class = sd), # random-effect SDs\n  prior(lkj(1.5), class = cor), # random-effect correlation matrices\n  prior(normal(0, log(50)), class = Intercept, dpar = sigma),\n  prior(normal(0, log(50)), class = b, dpar = sigma),\n  prior(exponential(0.25),\n    class = sd, group = subject,\n    dpar = sigma\n  )\n)\n\nneutralization_m11_4 &lt;- brm(\n  data = neutralization,\n  brmsformula(\n    vowel_dur ~ voicing + place + vowel + prosodic_boundary +\n      (1 + voicing | subject) + (1 + voicing | item_pair),\n    sigma ~ 1 + voicing + (1 | subject)\n  ),\n  prior = prior_6,\n  save_pars = save_pars(all = TRUE),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  file = \"models/neutralization_m11_4.brm\"\n)\n\n\nsummary(neutralization_m11_4)\n##  Family: gaussian \n##   Links: mu = identity; sigma = log \n## Formula: vowel_dur ~ voicing + place + vowel + prosodic_boundary + (1 + voicing | subject) + (1 + voicing | item_pair) \n##          sigma ~ 1 + voicing + (1 | subject)\n##    Data: neutralization (Number of observations: 749) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~item_pair (Number of levels: 24) \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)              9.85      2.09     6.59    14.48 1.00     1699\n## sd(voicing)                2.34      1.65     0.09     6.09 1.00     1798\n## cor(Intercept,voicing)    -0.16      0.46    -0.91     0.76 1.00     4335\n##                        Tail_ESS\n## sd(Intercept)              2709\n## sd(voicing)                2320\n## cor(Intercept,voicing)     2730\n## \n## ~subject (Number of levels: 16) \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)             27.46      5.64    19.05    41.61 1.01     1136\n## sd(voicing)                4.67      2.28     0.63     9.52 1.00     1534\n## sd(sigma_Intercept)        0.28      0.07     0.18     0.43 1.00     1627\n## cor(Intercept,voicing)     0.50      0.32    -0.27     0.94 1.00     3838\n##                        Tail_ESS\n## sd(Intercept)              1623\n## sd(voicing)                1743\n## sd(sigma_Intercept)        2635\n## cor(Intercept,voicing)     2602\n## \n## Regression Coefficients:\n##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept           156.70      7.12   142.69   171.32 1.01      800     1420\n## sigma_Intercept       2.93      0.08     2.78     3.07 1.00     1950     2513\n## voicing               9.41      1.95     5.70    13.41 1.00     2731     2583\n## place1               -9.59      2.78   -15.11    -4.16 1.00     2649     3009\n## place2                1.55      1.55    -1.55     4.55 1.00     2260     2599\n## vowel1                1.93      3.56    -5.17     9.04 1.00     2862     2436\n## vowel2              -16.90      2.04   -20.83   -12.78 1.00     2471     2795\n## vowel3               -3.59      1.31    -6.08    -0.99 1.00     2605     2389\n## vowel4               -6.53      1.05    -8.48    -4.40 1.00     2447     2645\n## prosodic_boundary    12.93      2.63     7.70    18.08 1.00     7406     3123\n## sigma_voicing        -0.01      0.06    -0.12     0.10 1.00     9021     2838\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nNote the new terms here:\n\nsigma_Intercept\nsigma_voicing\nsd(sigma_Intercept)\n\nThe 95% CI for sd(sigma_Intercept) is clearly above zero (speakers differ in the degree of noise), while the 95% CI for `sigma_voicing overlaps zero (residual SD doesn’t differ by voicing), as expected from our plots above.\nNew posterior predictive checks:\n\n## looks similar\npp_check(neutralization_m11_4, ndraws = 50, type = \"dens_overlay\")\n\n## any better?\nppc_dens_overlay_grouped(neutralization$vowel_dur,\n  yrep =\n    posterior_predict(neutralization_m11_4,\n      ndraws = 100\n    ),\n  group = neutralization$subject\n)\n\npp_check(neutralization_m11_4,\n  type = \"stat_grouped\",\n  ndraws = 1000,\n  group = \"subject\",\n  stat = \"sd\"\n)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe differences from the previous model are small, but we can see that the new model is slightly better. There is no no subject for whom the observed SD is outside the distribution of predicted SDs.\nThere would probably be larger changes (as in the (Nicenboim, Schad, and Vaishth 2024) example) if we had more data per subject.\n\nExercise 7.5 Perform a model comparison to check whether allowing \\(\\sigma\\) to vary, by (both) speaker and by voicing level, is justified. That is: does the data support using a distributional mixed-effects model rather than a normal (non-distributional) mixed-effects model?\n\n\n\n\n\nCiaccio, Laura Anna, and João Verı́ssimo. 2022. “Investigating Variability in Morphological Processing with Bayesian Distributional Models.” Psychonomic Bulletin & Review 29 (6): 2264–74.\n\n\nGronau, Quentin F., Henrik Singmann, and Eric-Jan Wagenmakers. 2020. “bridgesampling: An R Package for Estimating Normalizing Constants.” Journal of Statistical Software 92 (10): 1–29. https://doi.org/10.18637/jss.v092.i10.\n\n\nKurz, S. 2023. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nNicenboim, B., D. Schad, and S. Vaishth. 2024. An Introduction to Bayesian Data Analysis for Cognitive Science. https://vasishth.github.io/bayescogsci/book/. 5/2024 version.\n\n\nSonderegger, Morgan, Jane Stuart-Smith, and Jeff Mielke. 2023. “How Variable Are English Sibilants?” In Proceedings of the 20th International Congress of Phonetic Sciences, 3196–3200. Prague.\n\n\nWolock, T. M. 2020. “Distributional Regression Models: A Brms Tutorial.” https://www.tmwolock.com/index.php/2020/12/18/distributional-regression-models-brms-tutorial/.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Hierarchical Models 3</span>"
    ]
  },
  {
    "objectID": "week10.html#sec-brm2-mgf",
    "href": "week10.html#sec-brm2-mgf",
    "title": "6  Bayesian Hierarchical Models 2",
    "section": "6.2 Multiple grouping factors",
    "text": "6.2 Multiple grouping factors\nLet’s fit a first model of the neutralization data, with:\n\nA realistic set of fixed effects: voicing (of primary interest), plus all controls: vowel, place, prosodic_boundary.\nRandom intercepts for item_pair and subject\n\nThis is not a good model (we’d need random slopes for voicing), but it will do for now.\nLet’s first fit a frequentist lmer() model, to compare to what we’ll get with the Bayesian model:\n\nneut_m1 &lt;- lmer(vowel_dur ~ voicing + vowel + place + prosodic_boundary + (1 | item_pair) + (1 | subject), data = neutralization)\n\n\nsummary(neut_m1, correlation = FALSE)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: vowel_dur ~ voicing + vowel + place + prosodic_boundary + (1 |  \n##     item_pair) + (1 | subject)\n##    Data: neutralization\n## \n## REML criterion at convergence: 6693\n## \n## Scaled residuals: \n##    Min     1Q Median     3Q    Max \n## -4.562 -0.604 -0.021  0.601  3.019 \n## \n## Random effects:\n##  Groups    Name        Variance Std.Dev.\n##  item_pair (Intercept)  71.9     8.48   \n##  subject   (Intercept) 669.3    25.87   \n##  Residual              402.5    20.06   \n## Number of obs: 749, groups:  item_pair, 24; subject, 16\n## \n## Fixed effects:\n##                   Estimate Std. Error t value\n## (Intercept)        157.175      6.741   23.32\n## voicing              9.599      1.469    6.53\n## vowel1              -0.288      3.125   -0.09\n## vowel2             -18.239      1.722  -10.59\n## vowel3              -3.724      1.160   -3.21\n## vowel4              -6.757      0.897   -7.53\n## place1              -9.373      2.428   -3.86\n## place2               1.604      1.321    1.21\n## prosodic_boundary   12.392      2.612    4.74\n\nTo fit a Bayesian model, we need to set priors. We will determine weakly informative priors, for practice.\nTo choose a prior for the intercept, note that the response variable vowel_dur, has range of about 50–300:\n\nggplot(aes(x = vowel_dur), data = neutralization) +\n  geom_density(fill = \"blue\", alpha = 0.25)\n\n\n\n\n\n\n\n\nLet’s use the following priors:\n\nIntercept: \\(N(150, 50)\\)\n\nWhatever the intercept corresponds to in this model, reasonable values must be positive (vowel duration can only be positive).1\n\n\\(\\beta_i\\): \\(N(0, 50)\\) — a change of 50 (msec) in vowel_duration is huge.\nRandom intercept variances: \\(\\text{Exponential}(0.02)\\)\n\nWhy is \\(\\lambda = 0.02\\) for the exponential prior, \\(\\text{Exponential}(\\lambda)\\)? \\(\\lambda\\) should be on the order of 1/\\(SD_y\\), the standard deviation of the response variable, to be “weakly informative”.2 The \\(\\text{Exponential(1)}\\) prior we’ve seen in other models was based on the assumption that \\(y\\) has been standardized. In this case, it has not: sd(neutralization$vowel_dur) is 43.5, so \\(\\lambda\\) should be around 0.02.\n\n\n\n\n\n\nPractical note: Weakly-informative vs. default priors\n\n\n\n\n\nRemember: if you are not comfortable determining weakly-informative priors, it’s always an option to just use brms’ default priors, by not specifying the prior argument of your model. brms is particularly good for weakly-informative default priors for random effects (see ?prior() Sec. 2). To use these, we’d remove the class = sd line from our prior statement when fitting the model. Using weakly-informative priors is best practice, but it is better to use default/flat priors you don’t understand than to risk fitting a model with priors that don’t make sense.\nIt’s also a good idea to use brms default priors whenever you just don’t understand what a model parameter is doing. (Well, the second-best idea, after actually understanding it.). This can happen even if you’re pretty comfortable with Bayesian models, when you move to a new model type. For example: if you fit a negative binomial model, everything might make sense except the shape parameter, which in fact requires substantial digging in documentation to find a good definition. It’s fine to just use brms’s default prior for it.\n\n\n\n\nneutralization_m10_1 &lt;- brm(\n  data = neutralization,\n  family = gaussian,\n  vowel_dur ~ voicing + place + vowel + prosodic_boundary + (1 | subject) + (1 | item_pair),\n  prior = c(\n    prior(normal(150, 50), class = Intercept), # beta_0\n    prior(normal(0, 50), class = b),\n    prior(exponential(0.02), class = sd) # sigma\n  ),\n  iter = 4000, warmup = 2000, chains = 4, cores = 4,\n  file = \"models/neutralization_m10_1.brm\"\n)\n\n\nsummary(neutralization_m10_1)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: vowel_dur ~ voicing + place + vowel + prosodic_boundary + (1 | subject) + (1 | item_pair) \n##    Data: neutralization (Number of observations: 749) \n##   Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n##          total post-warmup draws = 8000\n## \n## Multilevel Hyperparameters:\n## ~item_pair (Number of levels: 24) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     9.10      1.98     5.99    13.71 1.00     3047     4798\n## \n## ~subject (Number of levels: 16) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)    27.80      5.70    19.16    41.02 1.00     2312     3711\n## \n## Regression Coefficients:\n##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept           156.99      7.09   143.28   171.16 1.00     1834     3038\n## voicing               9.60      1.51     6.61    12.57 1.00    11511     5544\n## place1               -9.36      2.56   -14.42    -4.26 1.00     5323     5081\n## place2                1.56      1.45    -1.29     4.51 1.00     5502     4747\n## vowel1               -0.23      3.37    -6.69     6.51 1.00     5919     4941\n## vowel2              -18.20      1.87   -21.95   -14.56 1.00     4969     4896\n## vowel3               -3.74      1.25    -6.26    -1.27 1.00     5544     4502\n## vowel4               -6.69      0.97    -8.52    -4.71 1.00     5548     5520\n## prosodic_boundary    12.35      2.63     7.25    17.51 1.00     9918     6003\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma    20.10      0.54    19.07    21.17 1.00    11169     4645\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nHere is one possible visualization of the model’s coefficients:\n\n# One possible posterior plot, showing all non-random effect parameters except intercept:\n# regexp means \"anything starting with b_X, where X doen't begin with I, and anything starting with s\".\nmcmc_plot(neutralization_m10_1, variable = c(\"^b_[^I]\", \"^s\"), regex = TRUE)\n\n\n\n\n\n\n\n\n\nExercise 6.1  \n\nMatch every fitted parameter shown in the Bayesian model output (there are 12) to the frequentist model output and verify that their values are (practically) the same.\n\n\n\nMake plots of the marginal effect of vowel for model neutralization_m10_1 (as in Section 5.3.2): one plot for an “average item/speaker” (CIs), and another for a new observation (PIs).\n\n\n\n\n\nRecall that the model contains many more fitted parameters than shown in the summary, where none of the random effects are shown. To see all parameters:\n\nposterior_summary(neutralization_m10_1)\n##                            Estimate Est.Error     Q2.5     Q97.5\n## b_Intercept                1.57e+02     7.095   143.28   171.164\n## b_voicing                  9.60e+00     1.510     6.61    12.569\n## b_place1                  -9.36e+00     2.564   -14.42    -4.259\n## b_place2                   1.56e+00     1.446    -1.29     4.510\n## b_vowel1                  -2.29e-01     3.370    -6.69     6.510\n## b_vowel2                  -1.82e+01     1.868   -21.95   -14.557\n## b_vowel3                  -3.74e+00     1.252    -6.26    -1.268\n## b_vowel4                  -6.69e+00     0.967    -8.52    -4.715\n## b_prosodic_boundary        1.24e+01     2.633     7.25    17.511\n## sd_item_pair__Intercept    9.10e+00     1.983     5.99    13.712\n## sd_subject__Intercept      2.78e+01     5.699    19.16    41.017\n## sigma                      2.01e+01     0.537    19.07    21.171\n## Intercept                  1.56e+02     7.096   141.81   169.675\n## r_item_pair[1,Intercept]   3.87e+00     5.878    -7.53    16.024\n## r_item_pair[2,Intercept]   1.81e+00     5.913   -10.04    13.362\n## r_item_pair[3,Intercept]  -7.10e+00     5.706   -18.71     4.011\n## r_item_pair[4,Intercept]  -6.95e+00     5.440   -17.22     3.702\n## r_item_pair[5,Intercept]   1.72e+01     6.078     6.09    29.742\n## r_item_pair[6,Intercept]   4.96e+00     5.470    -5.48    15.713\n## r_item_pair[7,Intercept]  -1.96e+00     5.539   -13.20     8.793\n## r_item_pair[8,Intercept]  -1.81e-02     5.590   -11.06    11.100\n## r_item_pair[9,Intercept]   7.04e+00     5.878    -4.24    18.692\n## r_item_pair[10,Intercept] -1.25e+01     6.242   -25.10    -0.622\n## r_item_pair[11,Intercept]  1.14e+00     5.554    -9.76    12.399\n## r_item_pair[12,Intercept]  6.88e+00     6.000    -4.72    18.763\n## r_item_pair[13,Intercept] -6.70e+00     5.583   -17.84     4.181\n## r_item_pair[14,Intercept]  5.86e-01     5.503   -10.41    11.221\n## r_item_pair[15,Intercept]  9.94e+00     6.283    -2.39    22.866\n## r_item_pair[16,Intercept] -6.58e+00     5.539   -17.70     4.332\n## r_item_pair[17,Intercept]  4.98e+00     5.445    -6.05    15.538\n## r_item_pair[18,Intercept]  5.13e-01     5.605   -10.40    11.993\n## r_item_pair[19,Intercept] -7.22e+00     5.584   -18.39     3.626\n## r_item_pair[20,Intercept]  2.18e-01     5.631   -10.80    11.244\n## r_item_pair[21,Intercept]  1.43e+00     5.456    -9.31    12.317\n## r_item_pair[22,Intercept]  1.98e+00     5.436    -8.57    12.995\n## r_item_pair[23,Intercept] -8.99e+00     5.552   -20.54     1.619\n## r_item_pair[24,Intercept] -4.11e+00     5.867   -15.62     7.216\n## r_subject[1,Intercept]     5.87e+01     7.473    44.10    73.399\n## r_subject[2,Intercept]    -2.84e+01     7.394   -43.27   -13.967\n## r_subject[3,Intercept]    -3.15e+01     7.421   -46.29   -16.872\n## r_subject[4,Intercept]    -4.32e+00     7.401   -19.15    10.203\n## r_subject[5,Intercept]    -3.57e-01     7.477   -15.16    14.374\n## r_subject[6,Intercept]    -2.68e+01     7.360   -41.44   -12.377\n## r_subject[7,Intercept]    -3.05e+00     7.422   -18.04    11.295\n## r_subject[8,Intercept]     3.20e+00     7.572   -11.90    18.068\n## r_subject[9,Intercept]    -2.08e+00     7.449   -16.80    12.308\n## r_subject[10,Intercept]   -1.44e+00     7.374   -16.22    13.181\n## r_subject[11,Intercept]   -1.80e+01     7.440   -32.88    -3.690\n## r_subject[12,Intercept]    2.64e+00     7.388   -12.13    17.021\n## r_subject[13,Intercept]    3.12e+01     7.439    16.42    45.626\n## r_subject[14,Intercept]    4.72e+01     7.494    32.32    62.069\n## r_subject[15,Intercept]   -6.02e+00     7.355   -20.66     8.392\n## r_subject[16,Intercept]   -1.82e+01     7.458   -33.17    -3.662\n## lprior                    -5.64e+01     0.128   -56.68   -56.178\n## lp__                      -3.41e+03     6.583 -3427.23 -3401.640\n\nBecause the posterior distribution is over all terms, we can examine the posterior for any parameter(s), using tools to manipulate the posterior from previous models (Chapter 3).\nFor example, to examine just the posterior for the by-item and by-subject random intercept variances:\n\n\nCode\nneutralization_m10_1 %&gt;%\n  spread_draws(sd_item_pair__Intercept, sd_subject__Intercept) %&gt;%\n  ggplot(aes(x = sd_item_pair__Intercept, y = sd_subject__Intercept)) +\n  geom_hex() +\n  xlab(\"by-item variability\") +\n  ylab(\"by-subject variability\") +\n  geom_vline(aes(xintercept = 0), lty = 2) +\n  geom_hline(aes(yintercept = 0), lty = 2) +\n  geom_abline(lty = 1, alpha = 0.2)\n\n\n\n\n\n\n\n\n\nA \\(y = x\\) line has been added here to aid in an exercise.\n\nExercise 6.2  \n\nBased on the plot just above and/or the model’s output, do you think that subjects or items vary more in vowel duration? Explain.\nThis question can be formally tested using this command:\n\n\nhypothesis(neutralization_m10_1, \"sd_subject__Intercept &gt; sd_item_pair__Intercept\", class = NULL)\n\nRun this command to evaluate the hypothesis: Post.Prob is \\(p_d\\) and Evid.Ratio is a Bayes Factor.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hierarchical Models 2</span>"
    ]
  },
  {
    "objectID": "week12.html",
    "href": "week12.html",
    "title": "8  Models for discrete data: counts, scales, and frequencies",
    "section": "",
    "text": "8.1 Preliminaries\nLoad libraries we will need:\nlibrary(brms)\nlibrary(lme4)\nlibrary(arm)\nlibrary(tidyverse)\n\nlibrary(tidybayes)\nlibrary(bayestestR)\n\nlibrary(bayesplot)\nlibrary(loo)\nlibrary(modelr)\nlibrary(languageR)\n\nlibrary(emmeans)\nlibrary(broom.mixed)\n\nlibrary(mlogit)\n\n## needed for cut2()\nlibrary(Hmisc)\n\nlibrary(patchwork)\n\n# avoids bug where select from MASS is used\nselect &lt;- dplyr::select",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Models for discrete data: counts, scales, and frequencies</span>"
    ]
  },
  {
    "objectID": "week12.html#preliminaries",
    "href": "week12.html#preliminaries",
    "title": "8  Models for discrete data: counts, scales, and frequencies",
    "section": "",
    "text": "Practical notes\n\n\n\n\n\n\nIf you have loaded rethinking, you need to detach it before using brms. See Kurz (2023) Sec. 4.3.1.\nI use the file argument when fitting brms models to make compiling this document easier (so the models don’t refit every time I compile). You may or may not want to do this for your own models. See file and file_refit arguments in ?brm.\nHere I set the file_refit option so “brms will refit the model if model, data or algorithm as passed to Stan differ from what is stored in the file.”\n\n\noptions(brms.file_refit = \"on_change\")\n\n\nI use chains = 4, cores = 4 when fitting brm models below—this means 4 chains, each to be run on one core on my laptop. cores = 4 may need to be adjusted for your computer. (You may have fewer or more cores; I have 8 cores, so this leaves 50% free.) You should figure out how to use multiple cores on your machine.\nMake numbers be printed only to 3 digits, for neater output:\n\n\noptions(digits = 3)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Models for discrete data: counts, scales, and frequencies</span>"
    ]
  },
  {
    "objectID": "week12.html#data",
    "href": "week12.html#data",
    "title": "8  Models for discrete data: counts, scales, and frequencies",
    "section": "8.2 Data",
    "text": "8.2 Data\n\n8.2.1 Dyads\nLoad the dyads dataset discussed in Winter and Bürkner (2021), which we used in Chapter 2:1\n\ndyads &lt;- read.csv(\"https://osf.io/6j8kc/download\", stringsAsFactors = TRUE)\n\nThe structure of the data is described in Section 2.1, including descriptions of relevant columns.\n\n\n\n\n\n\n\n\n\n\n\n\nWe’ll assume below that the research questions are:\n\nDo participants gesture more with professors than with friends?\nDoes this effect differ between Catalan and Korean speakers?\n\nThus, these effects are of interest for the RQs:\n\ncontext, language, context:language\n\nAll predictors are two-level factors. Transform to numeric and center:\n\ndyads &lt;- dyads %&gt;% mutate(\n  context_prof = rescale(context),\n  gender_M = arm::rescale(gender),\n  language_K = arm::rescale(language)\n)\n\nEmpirical effects:\n\n\nCode\ndyads %&gt;% ggplot(aes(x = language, y = gestures / dur)) +\n  geom_boxplot(aes(color = context)) +\n  xlab(\"Context\") +\n  ylab(\"Gestures\") +\n  scale_y_log10() +\n  annotation_logticks(sides = \"l\")\n\n\n\n\n\n\n\n\n\nNote:\n\nWe plot the rate gestures/second: gestures divided by dur.\nThe \\(y\\)-axis is on a log scale.\n\nBoth correspond to the Poisson model we will fit, which models log counts (log(gestures)) using an offset for log(dur).\n\n\n8.2.2 Dutch verb etymology\netymology from the languageR package. This is a dataset of “Estimated etymological age for regular and irregular monomorphemic Dutch verbs, together with other distributional predictors of regularity”, from languageR. The data is originally from Tabak, Schreuder, and Baayen (2005), and is analyzed in Baayen (2008) 6.3.2.\nVariables of interest:\n\nResponse: EtymAge – how old a (Dutch) verb is, described as the age of the set of languages it shows up in (as a cognate)\n\nDutch \\(&lt;\\) DutchGerman \\(&lt;\\) WestGermanic \\(&lt;\\) Germanic \\(&lt;\\) IndoEuropean\n\nPredictors:\n\nRegularity: levels irregular, regular (0, 1)\nWrittenFrequency: centered\nNcountStem: centered\n\n\nPreprocessing:\n\netymology &lt;- etymology %&gt;%\n  mutate(\n    ## make EtymAge an ordered factor\n    EtymAge = ordered(\n      EtymAge,\n      levels = c(\"Dutch\", \"DutchGerman\", \"WestGermanic\", \"Germanic\", \"IndoEuropean\")\n    ),\n    ## Center continous vars\n    WrittenFrequency = scale(WrittenFrequency, scale = FALSE),\n    ## Center WrittenFrequency\n    NcountStem = scale(NcountStem, scale = FALSE),\n    ## Center and scale Regularity\n    Regularity.std = arm::rescale(Regularity),\n  )\n\ncontrasts(etymology$Regularity) &lt;- contr.helmert\n\nDifferent ways to visualize empirical effects for ordinal data:\n\n\nCode\np1 &lt;- etymology %&gt;% ggplot(aes(x = EtymAge, y = WrittenFrequency)) +\n  geom_boxplot() +\n  scale_x_discrete(guide = guide_axis(angle = 90))\n\np2 &lt;- etymology %&gt;% ggplot(aes(y = EtymAge, x = NcountStem)) +\n  geom_boxplot()\n\np1 + p2\n\n# histograms\netymology %&gt;% ggplot(aes(x = EtymAge)) +\n  geom_histogram(stat = \"count\", aes(color = Regularity, fill = Regularity), position = \"dodge\")\n## Warning in geom_histogram(stat = \"count\", aes(color = Regularity, fill =\n## Regularity), : Ignoring unknown parameters: `binwidth`, `bins`, and `pad`\n\n# histogram by quantile of NcountStem\netymology %&gt;% ggplot(aes(x = EtymAge)) +\n  geom_histogram(stat = \"count\") +\n  facet_wrap(~ cut2(NcountStem, g = 5)) +\n  scale_x_discrete(guide = guide_axis(angle = 90))\n## Warning in geom_histogram(stat = \"count\"): Ignoring unknown parameters:\n## `binwidth`, `bins`, and `pad`\n\n\n# proportions\np3 &lt;- etymology %&gt;% ggplot(aes(x = EtymAge, fill = Regularity)) +\n  geom_bar(position = \"fill\") +\n  theme(legend.position = \"bottom\") +\n  scale_x_discrete(guide = guide_axis(angle = 90))\n\n\n\n# Empirical CDF: requires converting EtymAge to numeric\np4 &lt;- ggplot(etymology, aes(x = as.numeric(EtymAge))) +\n  stat_ecdf(geom = \"step\", aes(color = Regularity)) +\n  xlab(\"EtymAge\") +\n  theme(legend.position = \"bottom\")\n\n\np3 + p4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt seems like lower frequency, lower neighborhood density (NcountStem), and being regular are associated with a verb being younger (closer to Dutch).\n\n\n8.2.3 Idioms\nThis data is originally from Reddy, McCarthy, and Manandhar (2011), a study of what factors influence perception of `how literal’ a compound noun is (like “smoking jacket”, “cutting edge”).2 Traditionally compounds are classified as “idioms” or “compositional”, but in reality this is a fuzzy boundary.\nThe compounds can be roughly thought of as three types (column literal_meaning):\n\nyes – compound has a literal as well as idiomatic meaning (“melting pot”)\nno – compound does not have a (plausible) literal meanng (“front runner”)\nnone – compound only has a literal meaning (“credit card”)\n\nImportant columns, after processing done below:\n\nparticipant: 151 participants\nscore: rating (1-6 scale) given by participant to this \\(N_1\\)-\\(N_2\\) compound noun.\nfreq_word_1: frequency (log-transformed) of \\(N_1\\)\nfreq_word_2: frequency (log-transformed) of \\(N_2\\)\nfreq_word_3: frequency (log-transformed) of \\(N_1\\)+\\(N_2\\) compound\nword: the \\(N_1\\)+\\(N_2\\) compound (90 compounds)\nliteral_meaning: see above (factor with 3 levels)\n\n\nidioms &lt;- read.csv(\"../../data/idioms_reddy.csv\", stringsAsFactors = TRUE)\n\nLet’s assume the research questions are:\n\nDo the frequencies of the individual nouns (freq_word_1, freq_word_2) affect score?\nDoes the frequency of the N+N compound (freq_compound) affect score?\n\nSome processing:\n\n## add 0.1 to all frequencies to avoid zero counts\n## log-transform then standardize each frequency measure\n\n## change score to 1-6 ordinal scale\nidioms &lt;- idioms %&gt;% mutate(\n  freq_word_1 = arm::rescale(log(Word1_freq + 0.1)),\n  freq_word_2 = arm::rescale(log(Word2_freq + 0.1)),\n  freq_compound = arm::rescale(log(Cpd_freq + 0.1)),\n  participant = as.factor(as.numeric(str_match(participant, \"[0-9]+\"))),\n  score = ordered(score + 1)\n)\n\n## Remove ratings for N1 and N2 so we only have compound word ratings!\n## The full dataset also has ratings of how literal N1 and N2 are.\nidioms &lt;- idioms %&gt;%\n  filter(!str_detect(word, \"_[0-9]\")) %&gt;%\n  droplevels()\n\n## helmert coding for the 3-level factor\ncontrasts(idioms$literal_meaning) &lt;- contr.helmert\n\n\n8.2.3.1 Data cleaning\nThis data comes from Amazon Mechanical Turk, and is messy—not all participants are alike. There are 90 compounds, but most participants didn’t rate all compounds. Also, many participants didn’t use the whole 1-6 scale (e.g. just 2, 4, 6; or always chose 3). Let’s consider just those who rated at least 30 compounds and used the whole scale, as an approximation of “OK participant”:\n\n## partic who use the whole scale\np1 &lt;- idioms %&gt;%\n  group_by(participant) %&gt;%\n  count(score) %&gt;%\n  dplyr::summarize(ct = n()) %&gt;%\n  filter(ct == 6) %&gt;%\n  pull(participant) %&gt;%\n  as.character()\n\n## partic who rated at least 30 words\np2 &lt;- names(which(xtabs(~participant, idioms) &gt; 30))\n\ngood_participants &lt;- intersect(p1, p2)\n\nidioms &lt;- idioms %&gt;% filter(participant %in% good_participants)\n\nThere are 23 OK participants.\nBasic empirical effects:\n\n\nCode\nidioms %&gt;% ggplot(aes(x = score, y = freq_word_1)) +\n  geom_boxplot() +\n  ylab(\"Word 1 frequency\")\n\nidioms %&gt;% ggplot(aes(x = score, y = freq_word_2)) +\n  geom_boxplot() +\n  ylab(\"Word 2 frequency\")\n\nidioms %&gt;% ggplot(aes(x = score, y = freq_compound)) +\n  geom_boxplot() +\n  ylab(\"Compound frequency\")\n\nidioms %&gt;% ggplot(aes(x = score)) +\n  geom_histogram(stat = \"count\") +\n  facet_wrap(~literal_meaning, scale = \"free_y\")\n## Warning in geom_histogram(stat = \"count\"): Ignoring unknown parameters:\n## `binwidth`, `bins`, and `pad`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponse pattern by participant:\n\n\nCode\nidioms %&gt;% ggplot(aes(x = score)) +\n  geom_histogram(stat = \"count\") +\n  facet_wrap(~participant, scales = \"free_y\") +\n  ggtitle(\"Participants\")\n## Warning in geom_histogram(stat = \"count\"): Ignoring unknown parameters:\n## `binwidth`, `bins`, and `pad`\n\n\n\n\n\n\n\n\n\nPattern by word, for the first 30 N-N compounds:\n\n\nCode\nidioms %&gt;%\n  filter(word %in% unique(idioms$word)[1:30]) %&gt;%\n  ggplot(aes(x = score)) +\n  geom_histogram(stat = \"count\") +\n  facet_wrap(~word, scales = \"free_y\") +\n  ggtitle(\"Compounds\")\n## Warning in geom_histogram(stat = \"count\"): Ignoring unknown parameters:\n## `binwidth`, `bins`, and `pad`\n\n\n\n\n\n\n\n\n\nIt looks like participants differ in their response patterns. Note in particular how some participants have “wider” distribtions (like 191), suggesting that participants differ not just in the “intercept” but the “variance” of the distribution.\nWords (= N+N compounds) also differ in their response patterns, but in a simpler way. The whole distribution is shifted more left or right; it’s not as clear that its “variance” differs by-word.\n\n\n\n8.2.4 Dutch verb regularity\nThis is a dataset on Dutch verb regularity from languageR. Preprocessing:\n\n## relevel Auxiliary to intuitive order,\n## center WrittenFrequency\n##  make a numeric + centered version of Regularity:\n\nregularity &lt;- regularity %&gt;% mutate(\n  Auxiliary = fct_relevel(regularity$Auxiliary, \"hebben\", \"zijnheb\"),\n  WrittenFrequency = scale(WrittenFrequency, scale = FALSE),\n  Regularity.num = arm::rescale(Regularity)\n)\n\nWe’ll assume:\n\nResponse: Auxiliary (3 levels)\nPredictors: Regularity, WrittenFrequency\n\nIntuitively: “default” auxiliary (hebben) associated with lower frequency and regular verbs (Regularity).\n\np1 &lt;- regularity %&gt;% ggplot(aes(y = WrittenFrequency, x = Auxiliary)) +\n  geom_boxplot()\np2 &lt;- regularity %&gt;% ggplot(aes(x = Auxiliary, y = as.numeric(Regularity) - 1)) +\n  stat_summary(fun.data = \"mean_cl_boot\") +\n  ylab(\"Proportion Regularity=regular\") +\n  ylim(0, 1)\np1 + p2",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Models for discrete data: counts, scales, and frequencies</span>"
    ]
  },
  {
    "objectID": "week12.html#footnotes",
    "href": "week12.html#footnotes",
    "title": "8  Models for discrete data: counts, scales, and frequencies",
    "section": "",
    "text": "I found their OSF project helpful, especially the Rmd file showing code.↩︎\nI thank Siva Reddy for posting this data, and Michaela Socolof for providing a cleaned-up version with word frequency information added. This dataset was analyzed for her LING 620 project.↩︎\nI think what’s going on is that log(shape) already has a default value (0) implied by the structure of a negative binomial model, so each language can’t have an independent shape parameter estimated—their mean has to be zero. We need to force the model to just allow log(shape) to differ from its default value (0) for each language, while not fitting an additional overall log(shape) coefficient.↩︎\nWinter and Bürkner (2021): “The pp_check() function allows a number of different visualisation types. Here we specify the argument type = 'ecdf_overlay' to return an empirical cumulative distribution func- tion (ECDF). By default, pp_check() returns a smoothed output which may be inappropriate for discrete data, such as count data.”↩︎\nExercise: why will plot (2) never show this mismatch, and in fact we don’t expect it to look like the empirical data in terms of the width of participants’ distributions? This took me a while to realize.↩︎\nAnother step would be making the by-participant intercept category specific, but this means adding many more coefficients: 5 per participant.↩︎\nHint: Two ways to do this are to do some post-processing of the output of conditional_effects(), or just re-fit the model with Regularity coded as a factor.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Models for discrete data: counts, scales, and frequencies</span>"
    ]
  },
  {
    "objectID": "week12.html#poisson",
    "href": "week12.html#poisson",
    "title": "8  Models for discrete data: counts, scales, and frequencies",
    "section": "9.1 Poisson",
    "text": "9.1 Poisson\nWe would like to fit a mixed-effects model for the dyads data testing the RQs above. See below for the model formula.\nWe would like to fit a mixed-effects model testing the RQs above. This is similar to Exercise 3.6, but:\n\nIncluding random effects\nIncluding an offset term\n\nBoth better reflect the structure of this data.\nTo choose weakly informative priors, let’s consider the range of the data, after accounting for the offset:\n\nrange(log(dyads$gestures) - log(dyads$dur))\n## [1] -2.355032 -0.513021\n\ndyads %&gt;% ggplot(aes(x=log(gestures) - log(dur))) + geom_histogram() + xlab(\"Log(gestures) - offset\")\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nLet’s do:\n\nIntercept: \\(\\beta_0 \\sim N(-1, 0.5)\\)\n\nChosen so that mean +- 3 SD covers about the range of the data.\n\n\nOn log scale, 1 would be a very large effect, corresponding to multiplying gestures by exp(1) = 2.7, so let’s choose a regularizing prior s.t. 3 SD is 1:\n\n\\(\\beta_i\\) slopes: \\(\\beta_i \\sim N(0, 0.33)\\)\n\nRandom effects: reasonable scales, given priors we’ve chosen above, are 0.5 and 0.33 for slope/intercept. Let’s use 0.5 to be more conservative (flatter prior):\n\nRandom effect variances: \\(\\sigma_i \\sim \\text{Exponential}(2)\\) = 1/0.5\nRandom effect correlations: \\(LKJ(2)\\)\n\nWe use an offset term, log(dur). \nFollowing the tutorial, I’ll increase the iter/warmup values from their defaults.\n\ndyads_m12_1 &lt;- brm(gestures ~ 1 + context_prof*language_K + gender_M + \n                     offset(log(dur)) +\n                     (1 + context_prof|ID),\n                   data = dyads, family = poisson,\n                   prior = c(\n                     prior(normal(-1, 0.5), class = Intercept), # beta_0\n                     prior(normal(0, 0.33), class = b),\n                     prior(exponential(2), class = sd), # random-effect SDs\n                     prior(lkj(2), class=cor)\n                   ),\n                   iter = 8000, warmup = 4000, chains = 4, cores = 4,\n                   file='models/dyads_m12_1.brm'\n)\n\n\nsummary(dyads_m12_1)\n##  Family: poisson \n##   Links: mu = log \n## Formula: gestures ~ 1 + context_prof * language_K + gender_M + offset(log(dur)) + (1 + context_prof | ID) \n##    Data: dyads (Number of observations: 54) \n##   Draws: 4 chains, each with iter = 8000; warmup = 4000; thin = 1;\n##          total post-warmup draws = 16000\n## \n## Multilevel Hyperparameters:\n## ~ID (Number of levels: 27) \n##                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)                   0.39      0.06     0.28     0.53 1.00     4615\n## sd(context_prof)                0.15      0.07     0.01     0.29 1.00     4286\n## cor(Intercept,context_prof)     0.32      0.32    -0.40     0.84 1.00    12191\n##                             Tail_ESS\n## sd(Intercept)                   8251\n## sd(context_prof)                4544\n## cor(Intercept,context_prof)    10098\n## \n## Regression Coefficients:\n##                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## Intercept                  -1.03      0.08    -1.18    -0.88 1.00     3463\n## context_prof               -0.11      0.05    -0.22    -0.01 1.00     9501\n## language_K                 -0.06      0.14    -0.34     0.21 1.00     4010\n## gender_M                   -0.08      0.14    -0.35     0.19 1.00     4017\n## context_prof:language_K    -0.14      0.10    -0.33     0.05 1.00     9811\n##                         Tail_ESS\n## Intercept                   6433\n## context_prof               10437\n## language_K                  6577\n## gender_M                    6106\n## context_prof:language_K     9926\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nSanity check against frequentist model:\n\ndyads_freq_m12 &lt;- glmer(gestures ~ 1 + context_prof*language_K + gender_M + \n                          offset(log(dur)) +\n                     (1 + context_prof|ID), data=dyads, family='poisson')\n\nsummary(dyads_freq_m12, corr=FALSE)\n## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: poisson  ( log )\n## Formula: \n## gestures ~ 1 + context_prof * language_K + gender_M + offset(log(dur)) +  \n##     (1 + context_prof | ID)\n##    Data: dyads\n## \n##      AIC      BIC   logLik deviance df.resid \n##    449.8    465.7   -216.9    433.8       46 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -1.6686 -0.5031 -0.0188  0.4156  1.2748 \n## \n## Random effects:\n##  Groups Name         Variance Std.Dev. Corr\n##  ID     (Intercept)  0.12512  0.3537       \n##         context_prof 0.02234  0.1495   0.61\n## Number of obs: 54, groups:  ID, 27\n## \n## Fixed effects:\n##                         Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)             -1.03195    0.07154 -14.426   &lt;2e-16 ***\n## context_prof            -0.12537    0.05197  -2.412   0.0158 *  \n## language_K              -0.08378    0.14304  -0.586   0.5581    \n## gender_M                -0.07969    0.13416  -0.594   0.5525    \n## context_prof:language_K -0.15701    0.10019  -1.567   0.1171    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nExercise 9.1  \n\nConsider the output of dyads_m12_1. Two rows of the regression table address RQ1 and RQ2. Which rows?\nCalculate Bayes Factors, using bf_pointnull, for the model. What do the BFs, together with the 95% CredI’s from the model output for the two rows, say about RQ1 and RQ2?\nCalculate the 95% CredI’s for the context effect when language=korean and when language=catalan. (You could use functionality from emmeans or brms::hypothesis, as demonstrated in the tutorial.). Does this change your interpretation from part (a)?\n\nCode for calculating the posteriors for these effects “by hand” is commented-out. (The code works, but I’m sure there’s an easier way to do this with e.g. emmeans, let me know if you figure it out!)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can perform a posterior predictive check, which suggests that the model isn’t overdispersed:\n\n\n pp_check(dyads_m12_1, ndraws = 25)\n\n\n\n\n\n\n\n\nThis makes sense, because the structure of the data means that the random-effect terms we’ve included are almost equivalent to including an offset for each observation.\nTo see the importance of the offset term, let’s fit the same model, just using default priors. (To choose weakly informative priors, we’d need to think about the distribution of gestures, without first subtracting log(dur).) This is not a correct model—as the tutorial discusses, the offset is crucial—but it is instructive to see what effect this term has.\n\ndyads_m12_2 &lt;- brm(gestures ~ 1 + context_prof*language_K + gender_M +\n                     (1 + context_prof|ID),\n                   data = dyads, family = poisson,\n                   iter = 8000, warmup = 4000, chains = 4, cores = 4,\n                   file='models/dyads_m12_2.brm'\n)\ndyads_m12_2\n##  Family: poisson \n##   Links: mu = log \n## Formula: gestures ~ 1 + context_prof * language_K + gender_M + (1 + context_prof | ID) \n##    Data: dyads (Number of observations: 54) \n##   Draws: 4 chains, each with iter = 8000; warmup = 4000; thin = 1;\n##          total post-warmup draws = 16000\n## \n## Multilevel Hyperparameters:\n## ~ID (Number of levels: 27) \n##                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)                   0.53      0.09     0.39     0.72 1.00     4428\n## sd(context_prof)                0.22      0.07     0.09     0.37 1.00     5545\n## cor(Intercept,context_prof)     0.41      0.27    -0.18     0.84 1.00    12218\n##                             Tail_ESS\n## sd(Intercept)                   8135\n## sd(context_prof)                5416\n## cor(Intercept,context_prof)     8854\n## \n## Regression Coefficients:\n##                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## Intercept                   3.79      0.10     3.58     3.99 1.00     3616\n## context_prof               -0.23      0.06    -0.35    -0.11 1.00     9995\n## language_K                 -0.23      0.21    -0.64     0.19 1.00     3343\n## gender_M                    0.06      0.20    -0.33     0.45 1.00     3622\n## context_prof:language_K    -0.36      0.12    -0.61    -0.12 1.00     9864\n##                         Tail_ESS\n## Intercept                   6080\n## context_prof               10718\n## language_K                  5935\n## gender_M                    6077\n## context_prof:language_K    10330\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nExercise 9.2  \n\nJust based on the model’s output: What results have changed from the model (dyads_m12_1) with the offset term? Qualitatively or quantitatively?\nMake an empirical plot like in Section 8.2.1 but where the \\(y\\)-axis is gestures, not gestures/dur. Can you see, comparing the two plots, why your results did or did not change between dyads_m12_1 and dyads_m12_2?\n\n\n\n\n\n\nBarreda, Santiago, and Noah Silbert. 2023. Bayesian Multilevel Models for Repeated Measures Data: A Conceptual and Practical Introduction in r. Taylor & Francis. https://santiagobarreda.com/bmmrmd/.\n\n\nKurz, S. 2023. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nLevshina, Natalia. 2015. How to do linguistics with R: Data exploration and statistical analysis. John Benjamins.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nWinter, Bodo, and Paul-Christian Bürkner. 2021. “Poisson Regression for Linguists: A Tutorial Introduction to Modelling Count Data with Brms.” Language and Linguistics Compass 15 (11): e12439.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Models for discrete data: counts, scales, and frequencies</span>"
    ]
  },
  {
    "objectID": "week12.html#poisson-and-negative-binomial-mixed-effects-models",
    "href": "week12.html#poisson-and-negative-binomial-mixed-effects-models",
    "title": "8  Models for discrete data: counts, scales, and frequencies",
    "section": "8.3 Poisson and negative binomial mixed-effects models",
    "text": "8.3 Poisson and negative binomial mixed-effects models\n\n8.3.1 Poisson\nWe would like to fit a mixed-effects model for the dyads data testing the RQs above. See below for the model formula.\nWe would like to fit a mixed-effects model testing the RQs above. This is similar to Exercise 3.6, but:\n\nIncluding random effects\nIncluding an offset term\n\nBoth better reflect the structure of this data.\nTo choose weakly informative priors, let’s consider the range of the data, after accounting for the offset:\n\n\nCode\nrange(log(dyads$gestures) - log(dyads$dur))\n## [1] -2.355032 -0.513021\n\ndyads %&gt;% ggplot(aes(x = log(gestures) - log(dur))) +\n  geom_histogram() +\n  xlab(\"Log(gestures) - offset\")\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLet’s do:\n\nIntercept: \\(\\beta_0 \\sim N(-1, 0.5)\\)\n\nChosen so that mean +- 3 SD covers about the range of the data.\n\n\nOn log scale, 1 would be a very large effect, corresponding to multiplying gestures by exp(1) = 2.7, so let’s choose a regularizing prior s.t. 3 SD is 1:\n\n\\(\\beta_i\\) slopes: \\(\\beta_i \\sim N(0, 0.33)\\)\n\nRandom effects: reasonable scales, given priors we’ve chosen above, are 0.5 and 0.33 for slope/intercept. Let’s use 0.5 to be more conservative (flatter prior):\n\nRandom effect variances: \\(\\sigma_i \\sim \\text{Exponential}(2)\\) = 1/0.5\nRandom effect correlations: \\(LKJ(2)\\)\n\nWe use an offset term, log(dur). \nFollowing the tutorial, I’ll increase the iter/warmup values from their defaults.\n\ndyads_m12_1 &lt;- brm(\n  gestures ~ 1 + context_prof * language_K + gender_M +\n    offset(log(dur)) +\n    (1 + context_prof | ID),\n  data = dyads, family = poisson,\n  prior = c(\n    prior(normal(-1, 0.5), class = Intercept), # beta_0\n    prior(normal(0, 0.33), class = b),\n    prior(exponential(2), class = sd), # random-effect SDs\n    prior(lkj(2), class = cor)\n  ),\n  iter = 8000, warmup = 4000, chains = 4, cores = 4,\n  file = \"models/dyads_m12_1.brm\"\n)\n\n\nsummary(dyads_m12_1)\n##  Family: poisson \n##   Links: mu = log \n## Formula: gestures ~ 1 + context_prof * language_K + gender_M + offset(log(dur)) + (1 + context_prof | ID) \n##    Data: dyads (Number of observations: 54) \n##   Draws: 4 chains, each with iter = 8000; warmup = 4000; thin = 1;\n##          total post-warmup draws = 16000\n## \n## Multilevel Hyperparameters:\n## ~ID (Number of levels: 27) \n##                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)                   0.39      0.06     0.28     0.53 1.00     4615\n## sd(context_prof)                0.15      0.07     0.01     0.29 1.00     4286\n## cor(Intercept,context_prof)     0.32      0.32    -0.40     0.84 1.00    12191\n##                             Tail_ESS\n## sd(Intercept)                   8251\n## sd(context_prof)                4544\n## cor(Intercept,context_prof)    10098\n## \n## Regression Coefficients:\n##                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## Intercept                  -1.03      0.08    -1.18    -0.88 1.00     3463\n## context_prof               -0.11      0.05    -0.22    -0.01 1.00     9501\n## language_K                 -0.06      0.14    -0.34     0.21 1.00     4010\n## gender_M                   -0.08      0.14    -0.35     0.19 1.00     4017\n## context_prof:language_K    -0.14      0.10    -0.33     0.05 1.00     9811\n##                         Tail_ESS\n## Intercept                   6433\n## context_prof               10437\n## language_K                  6577\n## gender_M                    6106\n## context_prof:language_K     9926\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nExercise 8.1  \n\nConsider the output of dyads_m12_1. Two rows of the regression table address RQ1 and RQ2. Which rows?\nCalculate Bayes Factors, using bf_pointnull, for the model. What do the BFs, together with the 95% CredI’s from the model output for the two rows, say about RQ1 and RQ2?\nCalculate the 95% CredI’s for the context effect when language=korean and when language=catalan. (You could use functionality from emmeans or brms::hypothesis, as demonstrated in the tutorial.). Does this change your interpretation from part (a)?\n\nCode for calculating the posteriors for these effects the easy way with emmeans`, and “by hand” the harder way (but more nicely visualized), is commented-out.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can perform a posterior predictive check, which suggests that the model isn’t overdispersed:\n\n\npp_check(dyads_m12_1, ndraws = 25)\n\n\n\n\n\n\n\n\nThis makes sense, because the structure of the data means that the random-effect terms we’ve included are almost equivalent to including an offset for each observation.\nTo see the importance of the offset term, let’s fit the same model, just using default priors. (To choose weakly informative priors, we’d need to think about the distribution of gestures, without first subtracting log(dur).) This is not a correct model—as the tutorial discusses, the offset is crucial—but it is instructive to see what effect this term has.\n\ndyads_m12_2 &lt;- brm(\n  gestures ~ 1 + context_prof * language_K + gender_M +\n    (1 + context_prof | ID),\n  data = dyads, family = poisson,\n  iter = 8000, warmup = 4000, chains = 4, cores = 4,\n  file = \"models/dyads_m12_2.brm\"\n)\ndyads_m12_2\n##  Family: poisson \n##   Links: mu = log \n## Formula: gestures ~ 1 + context_prof * language_K + gender_M + (1 + context_prof | ID) \n##    Data: dyads (Number of observations: 54) \n##   Draws: 4 chains, each with iter = 8000; warmup = 4000; thin = 1;\n##          total post-warmup draws = 16000\n## \n## Multilevel Hyperparameters:\n## ~ID (Number of levels: 27) \n##                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)                   0.53      0.09     0.39     0.72 1.00     4428\n## sd(context_prof)                0.22      0.07     0.09     0.37 1.00     5545\n## cor(Intercept,context_prof)     0.41      0.27    -0.18     0.84 1.00    12218\n##                             Tail_ESS\n## sd(Intercept)                   8135\n## sd(context_prof)                5416\n## cor(Intercept,context_prof)     8854\n## \n## Regression Coefficients:\n##                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## Intercept                   3.79      0.10     3.58     3.99 1.00     3616\n## context_prof               -0.23      0.06    -0.35    -0.11 1.00     9995\n## language_K                 -0.23      0.21    -0.64     0.19 1.00     3343\n## gender_M                    0.06      0.20    -0.33     0.45 1.00     3622\n## context_prof:language_K    -0.36      0.12    -0.61    -0.12 1.00     9864\n##                         Tail_ESS\n## Intercept                   6080\n## context_prof               10718\n## language_K                  5935\n## gender_M                    6077\n## context_prof:language_K    10330\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nExercise 8.2  \n\nJust based on the model’s output: What results have changed from the model (dyads_m12_1) with the offset term? Qualitatively or quantitatively?\nMake an empirical plot like in Section 8.2.1 but where the \\(y\\)-axis is gestures, not gestures/dur. Can you see, comparing the two plots, why your results did or did not change between dyads_m12_1 and dyads_m12_2?\n\n\n\n\n8.3.2 Negative binomial\nThe dyads data isn’t a good case for a negative binomial model because adding the random effects already takes care of overdispersion.\nInstead, let’s return to the wordbank data from Homework 1 (?sec-hw1), where the structure of the data is described:\n\nwordbank_data &lt;- read.csv(file = \"../../data/wordbank_data_hw1.csv\")\n\nWe’ll build a negative binomial model for all languages, using random effects, instead of just one language (as in ?sec-hw1). This model will also contain a couple interesting additional features.\nFirst, some pre-processing:\n\ndata_all &lt;- wordbank_data %&gt;%\n  ## at least one language (Russian) has an extra lexical class (adverbs),\n  ## restrict to these four levels for comparability across languages\n  filter(lexical_class %in% c(\"function_words\", \"verbs\", \"adjectives\", \"nouns\")) %&gt;%\n  ## rows with missing values will be dropped when fitting the model anyway\n  filter(!is.na(lexical_class) & !is.na(frequency) & !is.na(MLU) & !is.na(num_phons)) %&gt;%\n  ## for convenience: total number of kids for this language\n  mutate(num_tot = num_true + num_false) %&gt;%\n  ## always run this after excluding all data with a given factor level\n  ## (here, lexical_class)\n  droplevels()\n\nWe can use a negative binomial model for this data, where:\n\n\\(y\\) = num_true is the response\nThe offset is log(num_tot), because \\(y\\) is not comparable across different languages, which summarize data from different numbers of children.\n\nWe’ll use a negative binomial model because this is a good default for count data (as Winter and Bürkner (2021) discuss).\nRecall that for this data:\n\nThe main effects of interest are frequency and lexical_class.\n\nMLU and num_phons are included as controls.\n\nWe’ll allow the effects of frequency and lexical_class to differ by language, as they’re of primary interest. There is no conceptual reason to not use “maximal” random effect structure alloiwng other predictors’ effects to differ by language. We don’t do so here just so the model fits faster.\nA negative binomial model allows for overdispersion using the shape parameter. The degree of overdispersion tends to differ between datasets, and it’s plausible that it would differ between languages here. We’ll allow for this by letting the shape parameter vary between languages, making this a case of distributional regression (Section 7.3), a distributional mixed-effects negative binomial model (whew!). The code for this is:\n\nshape ~ 0 + (1 | language)\n\nThe 0 is needed to keep the model from being overparametrized, for reasons I don’t totally understand (see discussion for ordinal models in Bürkner and Vuorre (2019)),3 which also require turning off how R treats 0 in model formulas by default, using the cmc = FALSE flag.\nTo fit this model:\n\nwordbank_m12_1 &lt;- brm(\n  data = data_all,\n  family = negbinomial,\n  brmsformula(\n    num_true ~ offset(log(num_tot)) + frequency + lexical_class + MLU +\n      num_phons + (1 + lexical_class + frequency | language),\n    shape ~ 0 + (1 | language),\n    cmc = FALSE\n  ),\n  prior = prior(lkj(2), class = cor), control = list(adapt_delta = 0.99),\n  cores = 4, chains = 4, iter = 4000,\n  file = \"models/wordbank_m12_1.brm\"\n)\n\nResults:\n\nwordbank_m12_1\n##  Family: negbinomial \n##   Links: mu = log; shape = log \n## Formula: num_true ~ offset(log(num_tot)) + frequency + lexical_class + MLU + num_phons + (1 + lexical_class + frequency | language) \n##          shape ~ 0 + (1 | language)\n##    Data: data_all (Number of observations: 2209) \n##   Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n##          total post-warmup draws = 8000\n## \n## Multilevel Hyperparameters:\n## ~language (Number of levels: 9) \n##                                                     Estimate Est.Error l-95% CI\n## sd(Intercept)                                           0.15      0.07     0.05\n## sd(lexical_classfunction_words)                         0.06      0.05     0.00\n## sd(lexical_classnouns)                                  0.09      0.04     0.02\n## sd(lexical_classverbs)                                  0.07      0.05     0.00\n## sd(frequency)                                           0.01      0.01     0.00\n## sd(shape_Intercept)                                     2.61      0.63     1.69\n## cor(Intercept,lexical_classfunction_words)             -0.19      0.35    -0.78\n## cor(Intercept,lexical_classnouns)                       0.05      0.32    -0.57\n## cor(lexical_classfunction_words,lexical_classnouns)     0.08      0.35    -0.61\n## cor(Intercept,lexical_classverbs)                      -0.04      0.34    -0.66\n## cor(lexical_classfunction_words,lexical_classverbs)     0.05      0.36    -0.63\n## cor(lexical_classnouns,lexical_classverbs)              0.11      0.34    -0.58\n## cor(Intercept,frequency)                                0.21      0.35    -0.54\n## cor(lexical_classfunction_words,frequency)             -0.06      0.35    -0.69\n## cor(lexical_classnouns,frequency)                       0.08      0.34    -0.60\n## cor(lexical_classverbs,frequency)                       0.09      0.34    -0.59\n##                                                     u-95% CI Rhat Bulk_ESS\n## sd(Intercept)                                           0.32 1.00     2335\n## sd(lexical_classfunction_words)                         0.17 1.00     2436\n## sd(lexical_classnouns)                                  0.20 1.00     2625\n## sd(lexical_classverbs)                                  0.18 1.00     2635\n## sd(frequency)                                           0.04 1.00     1796\n## sd(shape_Intercept)                                     4.11 1.00     1469\n## cor(Intercept,lexical_classfunction_words)              0.54 1.00     7200\n## cor(Intercept,lexical_classnouns)                       0.66 1.00     6042\n## cor(lexical_classfunction_words,lexical_classnouns)     0.70 1.00     4953\n## cor(Intercept,lexical_classverbs)                       0.63 1.00     8018\n## cor(lexical_classfunction_words,lexical_classverbs)     0.70 1.00     6764\n## cor(lexical_classnouns,lexical_classverbs)              0.71 1.00     5182\n## cor(Intercept,frequency)                                0.79 1.00     5279\n## cor(lexical_classfunction_words,frequency)              0.62 1.00     6153\n## cor(lexical_classnouns,frequency)                       0.71 1.00     6002\n## cor(lexical_classverbs,frequency)                       0.70 1.00     6348\n##                                                     Tail_ESS\n## sd(Intercept)                                           3680\n## sd(lexical_classfunction_words)                         4087\n## sd(lexical_classnouns)                                  2502\n## sd(lexical_classverbs)                                  4163\n## sd(frequency)                                           3293\n## sd(shape_Intercept)                                     2624\n## cor(Intercept,lexical_classfunction_words)              6070\n## cor(Intercept,lexical_classnouns)                       5728\n## cor(lexical_classfunction_words,lexical_classnouns)     6001\n## cor(Intercept,lexical_classverbs)                       5525\n## cor(lexical_classfunction_words,lexical_classverbs)     6716\n## cor(lexical_classnouns,lexical_classverbs)              5861\n## cor(Intercept,frequency)                                5884\n## cor(lexical_classfunction_words,frequency)              5939\n## cor(lexical_classnouns,frequency)                       6045\n## cor(lexical_classverbs,frequency)                       6349\n## \n## Regression Coefficients:\n##                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## Intercept                       0.52      0.09     0.35     0.70 1.00     5925\n## frequency                       0.10      0.01     0.08     0.12 1.00     5029\n## lexical_classfunction_words    -0.35      0.04    -0.44    -0.27 1.00     6532\n## lexical_classnouns              0.33      0.04     0.25     0.41 1.00     4795\n## lexical_classverbs              0.02      0.04    -0.06     0.10 1.00     5350\n## MLU                            -0.06      0.01    -0.07    -0.05 1.00    11935\n## num_phons                      -0.05      0.01    -0.06    -0.04 1.00    11705\n##                             Tail_ESS\n## Intercept                       5320\n## frequency                       4084\n## lexical_classfunction_words     5697\n## lexical_classnouns              4541\n## lexical_classverbs              4832\n## MLU                             6152\n## num_phons                       5877\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThe model is sure that the degree of overdispersion differs between languages (which row of the model table shows this?). Estimated shape by language:\n\ncoefficients(wordbank_m12_1)$language[, , \"shape_Intercept\"]\n##                    Estimate  Est.Error     Q2.5    Q97.5\n## Croatian           1.936425 0.15791380 1.624427 2.252422\n## Danish             1.580384 0.09771266 1.384927 1.768805\n## English (American) 2.467775 0.08303459 2.303834 2.626870\n## French (Quebec)    1.926830 0.10691380 1.717689 2.134946\n## Italian            2.768240 0.12506118 2.527791 3.011483\n## Norwegian          2.239087 0.10422929 2.036964 2.443160\n## Russian            3.212004 0.21137419 2.814662 3.644643\n## Spanish (Mexican)  2.017617 0.10038822 1.819770 2.215731\n## Turkish            2.923213 0.14966763 2.636315 3.225239\n\nIn contrast, languages don’t clearly differ in the (linear) effect of frequency. Estimated effect by language:\n\ncoefficients(wordbank_m12_1)$language[, , \"frequency\"]\n##                      Estimate  Est.Error       Q2.5     Q97.5\n## Croatian           0.10363656 0.01440393 0.07856115 0.1370501\n## Danish             0.10328881 0.01339693 0.07788560 0.1324924\n## English (American) 0.10000720 0.01013044 0.08060470 0.1211183\n## French (Quebec)    0.10149031 0.01200997 0.07877119 0.1275285\n## Italian            0.09130544 0.01151498 0.06664543 0.1116332\n## Norwegian          0.11350370 0.01746606 0.08730528 0.1543931\n## Russian            0.09062921 0.01565825 0.05231868 0.1160371\n## Spanish (Mexican)  0.09556585 0.01246951 0.06692819 0.1173372\n## Turkish            0.10228621 0.01356939 0.07517664 0.1297730\n\nHere is one way to show the predicted effect of frequency by language (for nouns):\n\nnew_data &lt;- data_all %&gt;%\n  data_grid(\n    language,\n    frequency = seq_range(frequency, n = 100),\n    num_phons = mean(num_phons),\n    lexical_class = \"nouns\",\n    MLU = mean(MLU),\n    num_tot = mean(num_tot)\n  )\n\npredictions &lt;- add_epred_draws(wordbank_m12_1, newdata = new_data, re_formula = NULL)\n\n\nggplot(predictions, aes(x = frequency, y = .epred, color = language)) +\n  stat_summary(fun = mean, geom = \"line\") +\n  labs(\n    x = \"Frequency\", y = \"Predicted Number of True Items\",\n    title = \"Effect of Frequency by Language\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nExercise 8.3  \n\nMake a plot showing the predicted effect of lexical_class (\\(x\\)-axis) for each language, holding other predictors constant.\nExtra: Make your plot more informative and accurate, as follows:\n\n\nInclude 95% CredI’s on predictions\nLet MLU num_phons, frequency, and num_total have their average values for a given language, rather than across the whole dataset.\nAnything else you can think of.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Models for discrete data: counts, scales, and frequencies</span>"
    ]
  },
  {
    "objectID": "week12.html#idioms",
    "href": "week12.html#idioms",
    "title": "8  Models for discrete data: counts, scales, and frequencies",
    "section": "8.3 Idioms",
    "text": "8.3 Idioms\nThis data is originally from Reddy, McCarthy, and Manandhar (2011), a study of what factors influence perception of `how literal’ a compound noun is (like “smoking jacket”, “cutting edge”).2 Traditionally compounds are classified as “idioms” or “compositional”, but in reality this is a fuzzy boundary.\nThe compounds can be roughly thought of as three types (column literal_meaning):\n\nyes – compound has a literal as well as idiomatic meaning (“melting pot”)\nno – compound does not have a (plausible) literal meanng (“front runner”)\nnone – compound only has a literal meaning (“credit card”)\n\nImportant columns, after processing done below:\n\nparticipant: 151 participants\nscore: rating (1-6 scale) given by participant to this \\(N_1\\)-\\(N_2\\) compound noun.\nfreq_word_1: frequency (log-transformed) of \\(N_1\\)\nfreq_word_2: frequency (log-transformed) of \\(N_2\\)\nfreq_word_3: frequency (log-transformed) of \\(N_1\\)+\\(N_2\\) compound\nword: the \\(N_1\\)+\\(N_2\\) compound (90 compounds)\nliteral_meaning: see above (factor with 3 levels)\n\n\nidioms &lt;- read.csv(\"../../data/idioms_reddy.csv\", stringsAsFactors = TRUE)\n\nLet’s assume the research questions are:\n\nDo the frequencies of the individual nouns (freq_word_1, freq_word_2) affect score?\nDoes the frequency of the N+N compound (freq_compound) affect score?\n\nSome processing:\n\n## add 0.1 to all frequencies to avoid zero counts\n## log-transform then standardize each frequency measure\n\n## change score to 1-6 ordinal scale\nidioms &lt;- idioms %&gt;% mutate(freq_word_1 = arm::rescale(log(Word1_freq+0.1)),\n                  freq_word_2 = arm::rescale(log(Word2_freq+0.1)),\n                  freq_compound = arm::rescale(log(Cpd_freq+0.1)),\n                 participant = as.factor(as.numeric(str_match(participant, '[0-9]+'))),\n                  score = ordered(score + 1)\n)\n\n## Remove ratings for N1 and N2 so we only have compound word ratings!\n## The full dataset also has ratings of how literal N1 and N2 are.\nidioms &lt;- idioms %&gt;% filter(!str_detect(word, '_[0-9]')) %&gt;% droplevels()\n\n## helmert coding for the 3-level factor\ncontrasts(idioms$literal_meaning) &lt;- contr.helmert\n\nData cleaning\nThis data comes from Mechanical Turk, and is messy—not all participants are alike. There are 90 compounds, but most participants didn’t rate all compounds. Also, many participants didn’t use the whole 1-6 scale (e.g. just 2, 4, 6; or always chose 3). Let’s consider just those who rated at least 30 compounds and used the whole scale, as an approximation of “OK participant”:\n\n## partic who use the whole scale\np1 &lt;- idioms %&gt;% group_by(participant) %&gt;% count(score) %&gt;% dplyr::summarize(ct=n()) %&gt;% filter(ct==6) %&gt;% pull(participant) %&gt;% as.character()\n\n## partic who rated at least 30 words\np2 &lt;- names(which(xtabs(~participant, idioms)&gt;30))\n\ngood_participants &lt;- intersect(p1, p2)\n\nidioms &lt;- idioms %&gt;% filter(participant %in% good_participants)\n\nThere are length(good_participants) OK participants.\nBasic empirical effects:\n\nidioms %&gt;% ggplot(aes(x=score, y=freq_word_1)) + geom_boxplot() + ylab(\"Word 1 frequency\")\n\nidioms %&gt;% ggplot(aes(x=score, y=freq_word_2)) + geom_boxplot() + ylab(\"Word 2 frequency\")\n\nidioms %&gt;% ggplot(aes(x=score, y=freq_compound)) + geom_boxplot() + ylab(\"Compound frequency\")\n\nidioms %&gt;% ggplot(aes(x=score)) + geom_histogram(stat='count') + facet_wrap(~literal_meaning, scale='free_y')\n## Warning in geom_histogram(stat = \"count\"): Ignoring unknown parameters:\n## `binwidth`, `bins`, and `pad`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponse pattern by participant:\n\nidioms %&gt;% ggplot(aes(x=score)) + geom_histogram(stat='count') + facet_wrap(~participant, scales = 'free_y') + ggtitle(\"Participants\") \n## Warning in geom_histogram(stat = \"count\"): Ignoring unknown parameters:\n## `binwidth`, `bins`, and `pad`\n\n\n\n\n\n\n\n\nPattern by word, for the first 30 N-N compounds:\n\nidioms %&gt;% filter(word %in% unique(idioms$word)[1:30]) %&gt;% ggplot(aes(x=score)) + geom_histogram(stat='count') + facet_wrap(~word, scales = 'free_y') + ggtitle(\"Compounds\") \n## Warning in geom_histogram(stat = \"count\"): Ignoring unknown parameters:\n## `binwidth`, `bins`, and `pad`\n\n\n\n\n\n\n\n\nIt looks like participants differ in their response patterns. Note in particular how some participants have “wider” distribtions (like 191), suggesting that partiicpants differ not just in the “intercept” but the “variance” of the distribution.\nWords (= N+N compounds) also differ in their response patterns, but in a simpler way. The whole distribution is shifted more left or right; it’s not as clear that its “variance” differs by-word.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Models for discrete data: counts, scales, and frequencies</span>"
    ]
  },
  {
    "objectID": "week12.html#ordinal-models",
    "href": "week12.html#ordinal-models",
    "title": "8  Models for discrete data: counts, scales, and frequencies",
    "section": "8.4 Ordinal models",
    "text": "8.4 Ordinal models\nLaurestine made a nice visualization app here to help understand (cumulative) ordinal models!\n\n8.4.1 Without random effects: etymology\nWe use the brms cumulative family, with default priors (for simplicity):\n\netymology_m12_1 &lt;- brm(\n  data = etymology,\n  family = cumulative,\n  EtymAge ~ 1 + WrittenFrequency + NcountStem + Regularity,\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  file = \"models/etymology_m12_1.brm\"\n)\n\n\nsummary(etymology_m12_1)\n##  Family: cumulative \n##   Links: mu = logit; disc = identity \n## Formula: EtymAge ~ 1 + WrittenFrequency + NcountStem + Regularity \n##    Data: etymology (Number of observations: 285) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]         -4.13      0.39    -4.96    -3.41 1.00     1900     2148\n## Intercept[2]         -2.44      0.23    -2.90    -2.00 1.00     3657     3275\n## Intercept[3]         -1.42      0.20    -1.81    -1.04 1.00     3480     3121\n## Intercept[4]          1.76      0.22     1.35     2.19 1.00     4900     3430\n## WrittenFrequency      0.08      0.07    -0.07     0.22 1.00     3887     3005\n## NcountStem            0.05      0.02     0.01     0.10 1.00     3564     2510\n## Regularityregular    -0.83      0.24    -1.32    -0.36 1.00     3824     2775\n## \n## Further Distributional Parameters:\n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nInterpreting some coefficients:\nCumulative probability up to each age:\n\ncumsum(prop.table(xtabs(~EtymAge, data = etymology)))\n##        Dutch  DutchGerman WestGermanic     Germanic IndoEuropean \n##   0.02807018   0.12631579   0.27719298   0.88421053   1.00000000\n\nCumulative log-odds:\n\ntemp &lt;- cumsum(prop.table(xtabs(~EtymAge, data = etymology)))\nlogit(temp)\n##        Dutch  DutchGerman WestGermanic     Germanic IndoEuropean \n##   -3.5445760   -1.9339340   -0.9584283    2.0329215          Inf\n\nThese are similar to the “Intercepts” of the model.\nRegularity effect:\n\nThe log-odds of level \\(k\\) vs level \\(k-1\\) decreases by 0.41 for regular vs. irregular verbs.\n\n\nprop.table(xtabs(~ Regularity + EtymAge, data = etymology)) -&gt; tab\ntab\n##            EtymAge\n## Regularity        Dutch DutchGerman WestGermanic    Germanic IndoEuropean\n##   irregular 0.003508772 0.038596491  0.059649123 0.322807018  0.073684211\n##   regular   0.024561404 0.059649123  0.091228070 0.284210526  0.042105263\n\nLets examine the empirical values of this for different \\(k\\):\n\n## DutchGerman vs German\nlog(tab[2, 2] / tab[2, 1]) - log(tab[1, 2] / tab[1, 1])\n## [1] -1.510592\n\n## WestGermanic vs DutchGerman\nlog(tab[2, 3] / tab[2, 2]) - log(tab[1, 3] / tab[1, 2])\n## [1] -0.01043488\n\n## etc.\nlog(tab[2, 4] / tab[2, 3]) - log(tab[1, 4] / tab[1, 3])\n## [1] -0.5522226\nlog(tab[2, 5] / tab[2, 4]) - log(tab[1, 5] / tab[1, 4])\n## [1] -0.4322764\n\nThese values around -0.4 on average, but they seem to differ a lot across \\(k\\). This means the proportional odds assumption may not be satisfied. (There is surely a function to check for this more cleanly.) We can account for this by allowing a category-specific effect of Regularity, which is shown in Section 8.6.3.\nLet’s visualize the effects using marginal effect plots. (The coefficients are not easy to interpret directly.)\n\nconditional_effects(etymology_m12_1, categorical = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrittenFrequency doesn’t have a strong effect. A higher NcountStem decreases the probability of “smaller” \\(y\\), and increases the probability of “larger” \\(y\\) (IndoEuropean). This follows from the positive sign of the coefficient: probability mass is shifted to higher \\(y\\) values as the predictor is increased.\n\nExercise 8.4 Interpret the Regularity coefficient, by filling in the following:\n“The Regularity coefficient has ______ (positive/negative) sign. This means that probability mass is shifted to ____ (higher/lower) \\(y\\) values for Regular verbs (= higher Regularity). That is, Regular verbs are predicted to be _____ (older/younger) than Irregular verbs.”\n\n\nUse an empirical CDF posterior predictive check:\n\npp_check(etymology_m12_1, ndraws = 50, type = \"ecdf_overlay\")\n\n\n\n\n\n\n\n\n(An ECDF plot is preferred to the default pp_check() when \\(y\\) is discrete.4)\nThis looks fine.\n\n\n\n\n\n8.4.2 Ordinal regression with random effects\nFor the idioms data, a minimal model given the research questions (effect of all freq terms) should include by-participant random slopes for all freq terms. It should also include by-word random intercepts. Let’s fit this model, to the entire dataset, using LKJ(1.5) rather than the LKJ(1) default for random effect correlations:\n\nidioms_m12_1 &lt;- brm(\n  data = idioms,\n  family = cumulative,\n  score ~ 1 + freq_word_1 + freq_word_2 + freq_compound + literal_meaning + (1 + freq_word_1 + freq_word_2 + freq_compound | participant) + (1 | word),\n  prior = c(\n    prior(lkj(1.5), class = cor)\n  ),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  file = \"models/idioms_m12_1.brm\"\n)\n\n\nsummary(idioms_m12_1)\n##  Family: cumulative \n##   Links: mu = logit; disc = identity \n## Formula: score ~ 1 + freq_word_1 + freq_word_2 + freq_compound + literal_meaning + (1 + freq_word_1 + freq_word_2 + freq_compound | participant) + (1 | word) \n##    Data: idioms (Number of observations: 1623) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~participant (Number of levels: 23) \n##                                Estimate Est.Error l-95% CI u-95% CI Rhat\n## sd(Intercept)                      0.92      0.16     0.66     1.30 1.00\n## sd(freq_word_1)                    0.17      0.12     0.01     0.44 1.00\n## sd(freq_word_2)                    0.25      0.15     0.02     0.57 1.00\n## sd(freq_compound)                  0.19      0.13     0.01     0.51 1.00\n## cor(Intercept,freq_word_1)        -0.14      0.37    -0.79     0.62 1.00\n## cor(Intercept,freq_word_2)        -0.01      0.34    -0.67     0.65 1.00\n## cor(freq_word_1,freq_word_2)       0.13      0.40    -0.67     0.82 1.00\n## cor(Intercept,freq_compound)      -0.20      0.37    -0.82     0.57 1.00\n## cor(freq_word_1,freq_compound)     0.08      0.40    -0.69     0.78 1.00\n## cor(freq_word_2,freq_compound)     0.01      0.39    -0.72     0.74 1.00\n##                                Bulk_ESS Tail_ESS\n## sd(Intercept)                      1064     1716\n## sd(freq_word_1)                    1560     1704\n## sd(freq_word_2)                    1316     1755\n## sd(freq_compound)                  1939     2357\n## cor(Intercept,freq_word_1)         5338     2810\n## cor(Intercept,freq_word_2)         5493     2864\n## cor(freq_word_1,freq_word_2)       2102     3018\n## cor(Intercept,freq_compound)       4561     2700\n## cor(freq_word_1,freq_compound)     3337     3086\n## cor(freq_word_2,freq_compound)     3927     3088\n## \n## ~word (Number of levels: 90) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     1.80      0.16     1.52     2.15 1.00     1035     1723\n## \n## Regression Coefficients:\n##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]        -3.38      0.29    -3.95    -2.82 1.00      736     1318\n## Intercept[2]        -1.71      0.28    -2.26    -1.16 1.00      718     1386\n## Intercept[3]        -0.61      0.28    -1.15    -0.07 1.00      716     1220\n## Intercept[4]         0.81      0.28     0.28     1.36 1.00      707     1367\n## Intercept[5]         2.15      0.29     1.61     2.71 1.00      762     1440\n## freq_word_1          0.43      0.43    -0.40     1.28 1.00      616     1071\n## freq_word_2          0.04      0.43    -0.77     0.90 1.01      909     1334\n## freq_compound        1.04      0.51     0.04     2.08 1.01      765     1056\n## literal_meaning1     1.55      0.30     0.97     2.13 1.01      623     1098\n## literal_meaning2    -0.88      0.16    -1.20    -0.58 1.00      703     1256\n## \n## Further Distributional Parameters:\n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nfreq_compound and literal_meaning have clear effects, while individual word frequencies do not (freq_word_1, freq_word_2).\n\n\n\n\n\n\n\n\n\n\n\n\n\nconditional_effects(idioms_m12_1, categorical = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting model predictions by participant is a bit confusing, if we want to compare to empirical patterns. We consider predictions for an “average word”—a single observation—in two ways:\n\nThe distribution of predictions, by participant (= posterior predictive distribution)\nThe fitted value, by participant.\n\nWhat makes this confusing is that both look like histograms of values from 1-6. This is because the “fitted value” for a single observation is a vector of probabilities: the probability of each of categories 1–6. If you look closely at the two plots, you can see that the distributions in (1) are wider, for each participant. This is because on the latent scale, noise is added to the fitted value (plot (2)), resulting in a distribution which is slightly flattened out.\n\nidioms %&gt;%\n  droplevels() %&gt;%\n  data_grid(freq_word_1 = 0, freq_word_2 = 0, freq_compound = 0, literal_meaning = \"yes\", participant) %&gt;%\n  add_predicted_draws(idioms_m12_1, re_formula = ~ (1 | participant), ndraws = 500) %&gt;%\n  ggplot(aes(x = .prediction)) +\n  geom_histogram(stat = \"count\") +\n  facet_wrap(~participant, scales = \"free_y\")\n## Warning in geom_histogram(stat = \"count\"): Ignoring unknown parameters:\n## `binwidth`, `bins`, and `pad`\n\n## *average* prediction for an 'average word', by-participant.\nidioms %&gt;%\n  droplevels() %&gt;%\n  data_grid(freq_word_1 = 0, freq_word_2 = 0, freq_compound = 0, literal_meaning = \"yes\", participant) %&gt;%\n  add_epred_draws(idioms_m12_1, re_formula = ~ (1 + freq_word_1 + freq_word_2 + freq_compound | participant)) %&gt;%\n  group_by(.category, participant) %&gt;%\n  median_qi(.epred) %&gt;%\n  ggplot(aes(x = .category, y = .epred)) +\n  geom_line(group = 1) +\n  facet_wrap(~participant) +\n  ylim(0, 0.4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote the mismatch match of plot (1) with the empirical data, where subjects see to differ in the “width” of the distribution, to the extent that some look bimodal while others look unimodal.5\nWe can also see this mismatch in a posterior predictive plot, grouped by subject:\n\npp_check(idioms_m12_1, type = \"ecdf_overlay_grouped\", ndraws = 50, group = \"participant\")\n\n\n\n\n\n\n\n\n(Subject 141, for example)\n\n8.4.2.1 Distributional model\nThe simplest step to address this is to allow participants to differ in variance:6\n\nidioms_m12_2 &lt;- brm(\n  data = idioms,\n  family = cumulative,\n  formula =\n    bf(score ~ 1 + freq_word_1 + freq_word_2 + freq_compound + literal_meaning + (1 + freq_word_1 + freq_word_2 + freq_compound | participant) + (1 | word)) +\n      lf(disc ~ 0 + (1 | participant), cmc = FALSE),\n  prior = c(\n    prior(lkj(1.5), class = cor)\n  ),\n  iter = 4000, warmup = 2000, cores = 4, chains = 4,\n  control = list(adapt_delta = 0.9),\n  file = \"models/idioms_m12_2.brm\"\n)\n\nVariance is controlled by a parameter disc: variance = 1/disc. By default (in the model fitted above), it is fixed to 1. Because disc must be positive, if we model it (a case of distributional regression: Section 7.3), brms automatically puts the model in log space:\n\\[\n\\log(\\text{disc}) = \\text{(predictors)}\n\\] The model sets \\(\\log(\\text{disc})\\) to have mean zero (corresponding to disc=1), and participants vary (by-participant random effect). The 0 + (1|participant) notation is important, and relates to the fact that we don’t want the baseline value of disc to be estimated—see the Burkner & Vuorre tutorial (Bürkner and Vuorre 2019).\nNote the extra term in the model summary:\n\nsummary(idioms_m12_2)\n##  Family: cumulative \n##   Links: mu = logit; disc = log \n## Formula: score ~ 1 + freq_word_1 + freq_word_2 + freq_compound + literal_meaning + (1 + freq_word_1 + freq_word_2 + freq_compound | participant) + (1 | word) \n##          disc ~ 0 + (1 | participant)\n##    Data: idioms (Number of observations: 1623) \n##   Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n##          total post-warmup draws = 8000\n## \n## Multilevel Hyperparameters:\n## ~participant (Number of levels: 23) \n##                                Estimate Est.Error l-95% CI u-95% CI Rhat\n## sd(Intercept)                      1.05      0.21     0.72     1.52 1.00\n## sd(freq_word_1)                    0.16      0.13     0.01     0.47 1.00\n## sd(freq_word_2)                    0.23      0.16     0.01     0.60 1.00\n## sd(freq_compound)                  0.34      0.20     0.02     0.77 1.00\n## sd(disc_Intercept)                 0.40      0.08     0.28     0.59 1.00\n## cor(Intercept,freq_word_1)        -0.09      0.38    -0.76     0.67 1.00\n## cor(Intercept,freq_word_2)        -0.04      0.36    -0.72     0.66 1.00\n## cor(freq_word_1,freq_word_2)       0.09      0.41    -0.71     0.81 1.00\n## cor(Intercept,freq_compound)      -0.32      0.32    -0.83     0.39 1.00\n## cor(freq_word_1,freq_compound)     0.10      0.40    -0.71     0.80 1.00\n## cor(freq_word_2,freq_compound)     0.03      0.39    -0.70     0.75 1.00\n##                                Bulk_ESS Tail_ESS\n## sd(Intercept)                      1740     3100\n## sd(freq_word_1)                    3540     3759\n## sd(freq_word_2)                    2296     3192\n## sd(freq_compound)                  2091     2221\n## sd(disc_Intercept)                 1782     3076\n## cor(Intercept,freq_word_1)         9278     5089\n## cor(Intercept,freq_word_2)         8112     5303\n## cor(freq_word_1,freq_word_2)       4868     5500\n## cor(Intercept,freq_compound)       6611     5594\n## cor(freq_word_1,freq_compound)     4007     5157\n## cor(freq_word_2,freq_compound)     4922     6015\n## \n## ~word (Number of levels: 90) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     2.06      0.27     1.61     2.68 1.00     1092     1772\n## \n## Regression Coefficients:\n##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept[1]        -3.95      0.50    -5.06    -3.09 1.00      825     1304\n## Intercept[2]        -1.96      0.38    -2.75    -1.26 1.00      920     1970\n## Intercept[3]        -0.69      0.33    -1.35    -0.03 1.00     1154     2066\n## Intercept[4]         0.99      0.34     0.35     1.73 1.00     1418     2329\n## Intercept[5]         2.59      0.42     1.83     3.52 1.00     1385     2010\n## freq_word_1          0.52      0.50    -0.47     1.53 1.00     1319     1914\n## freq_word_2          0.04      0.50    -0.96     1.02 1.00     1269     2066\n## freq_compound        1.18      0.57     0.09     2.33 1.00     1827     3020\n## literal_meaning1     1.71      0.37     1.02     2.49 1.00     1319     1996\n## literal_meaning2    -1.00      0.20    -1.44    -0.64 1.00     1010     1503\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nconditional_effects(idioms_m12_2, categorical = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution of predictions, by participant, for a single observation, like plot (1) for the previous model:\n\nidioms %&gt;%\n  droplevels() %&gt;%\n  data_grid(freq_word_1 = 0, freq_word_2 = 0, freq_compound = 0, literal_meaning = \"yes\", participant) %&gt;%\n  add_predicted_draws(idioms_m12_2, re_formula = ~ (1 | participant), ndraws = 500) %&gt;%\n  ggplot(aes(x = .prediction)) +\n  geom_histogram(stat = \"count\") +\n  facet_wrap(~participant, scales = \"free_y\")\n## Warning in geom_histogram(stat = \"count\"): Ignoring unknown parameters:\n## `binwidth`, `bins`, and `pad`\n\n\n\n\n\n\n\n\nNote how some participants are allowed to have “wider” distributions. This follows naturally from allowing variance to differ by-participant: high variance on the latent scale gives a “wider” distribution on the \\(y\\) scale. It is hard to judge visually, but we can see which participants are predicted to have wider/narrow distributions by examining their predicted disc values:\n\ncoefficients(idioms_m12_2)$participant[, , \"disc_Intercept\"] %&gt;%\n  data.frame() %&gt;%\n  arrange(Estimate) # sort by value\n##         Estimate Est.Error        Q2.5        Q97.5\n## 191 -0.947066453 0.1519010 -1.25949993 -0.666344650\n## 141 -0.565839801 0.1644132 -0.90021768 -0.265309621\n## 227 -0.506142006 0.1677428 -0.85512769 -0.197907410\n## 185 -0.379219072 0.1602355 -0.70145440 -0.081895720\n## 130 -0.305350542 0.2014921 -0.71506040  0.073489452\n## 107 -0.284232571 0.1554955 -0.60984493  0.009503652\n## 208 -0.251524300 0.1668371 -0.60027040  0.058852802\n## 262 -0.242062327 0.1635689 -0.57177186  0.065343800\n## 207 -0.136974556 0.1964239 -0.54086200  0.234382453\n## 56  -0.122097207 0.1625797 -0.45204590  0.187810998\n## 171 -0.115869132 0.1845028 -0.48317137  0.234947744\n## 101 -0.024571300 0.1884720 -0.40192606  0.332648693\n## 245  0.008732047 0.1580529 -0.31442829  0.309776164\n## 142  0.047321507 0.1729586 -0.29785673  0.381205670\n## 28   0.059414231 0.1483574 -0.23781241  0.349021356\n## 170  0.076133128 0.1470500 -0.22530162  0.356171403\n## 172  0.081753701 0.1405467 -0.20016283  0.351969050\n## 96   0.089601536 0.1446953 -0.20506170  0.358193357\n## 256  0.228295786 0.1509693 -0.07919182  0.516153133\n## 46   0.230117956 0.1974157 -0.16383852  0.615569256\n## 115  0.275342389 0.1503049 -0.03129318  0.567483722\n## 244  0.484554076 0.2218011  0.05488517  0.929387860\n## 5    0.584885794 0.1911085  0.21885161  0.971097714\n\nParticipant 191 is predicted to have the widest distribution (because variance = 1/disc), and participant 5 the narrowest. This seems plausible from the empirical data.\nComparing LOO also suggests the more complex model is better:\n\nloo(idioms_m12_1, idioms_m12_2, cores = 4)\n## Output of model 'idioms_m12_1':\n## \n## Computed from 4000 by 1623 log-likelihood matrix.\n## \n##          Estimate   SE\n## elpd_loo  -2067.7 34.5\n## p_loo       119.7  3.2\n## looic      4135.5 69.0\n## ------\n## MCSE of elpd_loo is 0.2.\n## MCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 2.9]).\n## \n## All Pareto k estimates are good (k &lt; 0.7).\n## See help('pareto-k-diagnostic') for details.\n## \n## Output of model 'idioms_m12_2':\n## \n## Computed from 8000 by 1623 log-likelihood matrix.\n## \n##          Estimate   SE\n## elpd_loo  -1993.0 32.8\n## p_loo       146.4  4.7\n## looic      3986.0 65.6\n## ------\n## MCSE of elpd_loo is 0.2.\n## MCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.8]).\n## \n## All Pareto k estimates are good (k &lt; 0.7).\n## See help('pareto-k-diagnostic') for details.\n## \n## Model comparisons:\n##              elpd_diff se_diff\n## idioms_m12_2   0.0       0.0  \n## idioms_m12_1 -74.7      13.6",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Models for discrete data: counts, scales, and frequencies</span>"
    ]
  },
  {
    "objectID": "week12.html#extra",
    "href": "week12.html#extra",
    "title": "8  Models for discrete data: counts, scales, and frequencies",
    "section": "8.6 Extra",
    "text": "8.6 Extra\n\n8.6.1 Frequentist Poisson mixed-effects model\nCorresponding to dyads_m12_1 in Section 8.3.1\n\ndyads_freq_m12 &lt;- glmer(gestures ~ 1 + context_prof * language_K + gender_M +\n  offset(log(dur)) +\n  (1 + context_prof | ID), data = dyads, family = \"poisson\")\n\nsummary(dyads_freq_m12, corr = FALSE)\n## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: poisson  ( log )\n## Formula: \n## gestures ~ 1 + context_prof * language_K + gender_M + offset(log(dur)) +  \n##     (1 + context_prof | ID)\n##    Data: dyads\n## \n##      AIC      BIC   logLik deviance df.resid \n##    449.8    465.7   -216.9    433.8       46 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -1.6686 -0.5031 -0.0188  0.4156  1.2748 \n## \n## Random effects:\n##  Groups Name         Variance Std.Dev. Corr\n##  ID     (Intercept)  0.12512  0.3537       \n##         context_prof 0.02234  0.1495   0.61\n## Number of obs: 54, groups:  ID, 27\n## \n## Fixed effects:\n##                         Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)             -1.03195    0.07154 -14.426   &lt;2e-16 ***\n## context_prof            -0.12537    0.05197  -2.412   0.0158 *  \n## language_K              -0.08378    0.14304  -0.586   0.5581    \n## gender_M                -0.07969    0.13416  -0.594   0.5525    \n## context_prof:language_K -0.15701    0.10019  -1.567   0.1171    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n8.6.2 Frequentist ordinal model\nCorresponding to model etymology_m12_2 in Section 8.4.1.\nFitted using polr() from MASS:\n\netymology_freq_m12 &lt;- polr(EtymAge ~ WrittenFrequency + Regularity + NcountStem, data = etymology, Hess = TRUE)\n\nsummary(etymology_freq_m12)\n## Call:\n## polr(formula = EtymAge ~ WrittenFrequency + Regularity + NcountStem, \n##     data = etymology, Hess = TRUE)\n## \n## Coefficients:\n##                     Value Std. Error t value\n## WrittenFrequency  0.07756    0.07510   1.033\n## Regularity1      -0.41104    0.12364  -3.325\n## NcountStem        0.05391    0.02088   2.581\n## \n## Intercepts:\n##                          Value    Std. Error t value \n## Dutch|DutchGerman         -3.6682   0.3622   -10.1271\n## DutchGerman|WestGermanic  -2.0193   0.1832   -11.0221\n## WestGermanic|Germanic     -1.0037   0.1369    -7.3306\n## Germanic|IndoEuropean      2.1527   0.1926    11.1783\n## \n## Residual Deviance: 644.6232 \n## AIC: 658.6232\n\n\n\n8.6.3 Ordinal model with category-specific effects\nModel with category-specific effects:\n\netymology_m12_2 &lt;- brm(\n  data = etymology,\n  family = cumulative,\n  EtymAge ~ 1 + WrittenFrequency + NcountStem + cs(Regularity),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  control = list(adapt_delta = 0.99),\n  file = \"models/etymology_m12_2.brm\"\n)\n\n\nsummary(etymology_m12_2)\n## Warning: Category specific effects for this family should be considered\n## experimental and may have convergence issues.\n##  Family: cumulative \n##   Links:\n## Warning: Category specific effects for this family should be considered\n## experimental and may have convergence issues.\n## mu = logit; disc = identity \n## Formula: EtymAge ~ 1 + WrittenFrequency + NcountStem + cs(Regularity) \n##    Data: etymology (Number of observations: 285) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## Intercept[1]            -5.15      1.05    -7.65    -3.60 1.00     1312\n## Intercept[2]            -2.47      0.30    -3.08    -1.92 1.00     2590\n## Intercept[3]            -1.41      0.21    -1.83    -1.00 1.00     2984\n## Intercept[4]             1.81      0.24     1.35     2.30 1.00     3307\n## WrittenFrequency         0.08      0.08    -0.07     0.23 1.00     3608\n## NcountStem               0.05      0.02     0.01     0.10 1.00     3590\n## Regularityregular[1]    -2.06      1.12    -4.62    -0.21 1.00     1361\n## Regularityregular[2]    -0.85      0.37    -1.58    -0.15 1.00     2418\n## Regularityregular[3]    -0.82      0.28    -1.36    -0.27 1.00     2592\n## Regularityregular[4]    -0.73      0.41    -1.52     0.06 1.00     3333\n##                      Tail_ESS\n## Intercept[1]              982\n## Intercept[2]             3017\n## Intercept[3]             3278\n## Intercept[4]             3116\n## WrittenFrequency         2544\n## NcountStem               2525\n## Regularityregular[1]      850\n## Regularityregular[2]     2642\n## Regularityregular[3]     2225\n## Regularityregular[4]     2900\n## \n## Further Distributional Parameters:\n##      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## disc     1.00      0.00     1.00     1.00   NA       NA       NA\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n(If we choose to ignore the warnings:) There are now category-specific values of the Regularity coefficient, as for the Intercept (cutoffs). Note that all the Regularity coefficients have 95% CIs that overlap. This model does not seem to be an improvement over the previous one, which we can check by comparing LOO:\n\netymology_m12_1 &lt;- add_criterion(etymology_m12_1, \"loo\")\netymology_m12_2 &lt;- add_criterion(etymology_m12_2, \"loo\")\n\n\nloo_compare(etymology_m12_1, etymology_m12_2)\n##                 elpd_diff se_diff\n## etymology_m12_1  0.0       0.0   \n## etymology_m12_2 -2.3       2.0\n\nThe simpler model is preferred.\n\n\n8.6.4 Frequentist multinomial model\nWe follow Levshina (2015) Chap. 13, using the mlogit package (Croissant 2020), to fit a frequentist multinoimal model to the regularity data.\nFirst, transform the data to the format required by mlogit():\n\nregularity_2 &lt;- mlogit.data(regularity, shape = \"wide\", choice = \"Auxiliary\")\n\nFit the model:\n\nregularity_m12_freq &lt;- mlogit(formula = Auxiliary ~ 1 | Regularity.num + WrittenFrequency, data = regularity_2, reflevel = \"hebben\")\n\nThe reflevel argument makes the sub-models compare the probabilties of the same Auxiliary levels as model regularity_m12_1.\n\nsummary(regularity_m12_freq)\n## \n## Call:\n## mlogit(formula = Auxiliary ~ 1 | Regularity.num + WrittenFrequency, \n##     data = regularity_2, reflevel = \"hebben\", method = \"nr\")\n## \n## Frequencies of alternatives:choice\n##   hebben     zijn  zijnheb \n## 0.824286 0.028571 0.147143 \n## \n## nr method\n## 6 iterations, 0h:0m:0s \n## g'(-H)^-1g = 2.23E-07 \n## gradient close to zero \n## \n## Coefficients :\n##                            Estimate Std. Error  z-value  Pr(&gt;|z|)    \n## (Intercept):zijn         -3.6740106  0.2886448 -12.7285 &lt; 2.2e-16 ***\n## (Intercept):zijnheb      -1.7706302  0.1113882 -15.8960 &lt; 2.2e-16 ***\n## Regularity.num:zijn      -1.6108619  0.5248941  -3.0689 0.0021483 ** \n## Regularity.num:zijnheb   -0.9632617  0.2537025  -3.7968 0.0001466 ***\n## WrittenFrequency:zijn     0.1398767  0.1227794   1.1393 0.2545979    \n## WrittenFrequency:zijnheb  0.0054712  0.0598898   0.0914 0.9272110    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Log-Likelihood: -364.24\n## McFadden R^2:  0.041447 \n## Likelihood ratio test : chisq = 31.499 (p.value = 2.4218e-06)\n\n\n\n\n\nBaayen, R. H. 2008. Analyzing linguistic data: A practical introduction to statistics using R. Cambridge: Cambridge University Press.\n\n\nBarreda, Santiago, and Noah Silbert. 2023. Bayesian Multilevel Models for Repeated Measures Data: A Conceptual and Practical Introduction in r. Taylor & Francis. https://santiagobarreda.com/bmmrmd/.\n\n\nBürkner, Paul-Christian, and Matti Vuorre. 2019. “Ordinal Regression Models in Psychology: A Tutorial.” Advances in Methods and Practices in Psychological Science 2 (1): 77–101.\n\n\nCroissant, Yves. 2020. “Estimation of Random Utility Models in R: The mlogit Package.” Journal of Statistical Software 95 (11): 1–41. https://doi.org/10.18637/jss.v095.i11.\n\n\nKurz, S. 2023. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nLevshina, Natalia. 2015. How to do linguistics with R: Data exploration and statistical analysis. John Benjamins.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nReddy, Siva, Diana McCarthy, and Suresh Manandhar. 2011. “An Empirical Study on Compositionality in Compound Nouns.” In Proceedings of 5th International Joint Conference on Natural Language Processing, 210–18.\n\n\nTabak, Wieke M, Robert Schreuder, and R Harald Baayen. 2005. “Lexical Statistics and Lexical Processing: Semantic Density, Information Complexity, Sex, and Irregularity in Dutch.” In Linguistic Evidence, edited by Stephan Kepser and Marga Reis, 529–56. De Gruyter Mouton.\n\n\nWinter, Bodo, and Paul-Christian Bürkner. 2021. “Poisson Regression for Linguists: A Tutorial Introduction to Modelling Count Data with Brms.” Language and Linguistics Compass 15 (11): e12439.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Models for discrete data: counts, scales, and frequencies</span>"
    ]
  },
  {
    "objectID": "week11.html#sec-brm3-dr",
    "href": "week11.html#sec-brm3-dr",
    "title": "7  Bayesian Hierarchical Models 3",
    "section": "7.3 Distributional regression",
    "text": "7.3 Distributional regression\nModels so far in this course always only involve the mean of a distribution, which is modeled as a function of predictors, by-subject variability, etc. But other parameters could vary as well; such distributional regression models are easily fitted using brms/Stan (vignette). We’ll consider models where the residual variability, \\(\\sigma\\) (“degree of noise”) is modeled, for linear regression.\nThis kind of model could let us address research questions involving (by-observation) variability. (Example: for VOT data, are voiced stops “more variable” than voiceless stops?)\nWe will fit a distributional extension of our neutralization model, following the steps shown in Nicenboim, Schad, and Vaishth (2024) Sec. 5.1.6.\nOne situation where we’d need such a model is when the research questions are about “noise”/variability—as in Ciaccio and Verı́ssimo (2022) (who also give a tutorial on these models using brms). Another example, from phonetics, is Sonderegger, Stuart-Smith, and Mielke (2023) (code here).\nBut, most of the time, our research questions will not be about this kind of variability. How might we decide we need such a model, even if the RQs don’t involve noise?\nAcross all data, a posterior predictive check of our neutralization model looks decent:4\n\npp_check(neutralization_m11_1, ndraws = 50, type = \"dens_overlay\")\n\n\n\n\n\n\n\n\nHowever, we know that subjects differ a lot in how they speak during a production experiment. Some will speak formally, others casually; some may have had more sleep than others; and so on. This could lead to the degree of “noise” differing by subject. To see if this could be happening, consider the same plot, by-subject:\n\nppc_dens_overlay_grouped(neutralization$vowel_dur,\n  yrep =\n    posterior_predict(neutralization_m11_1,\n      ndraws = 100\n    ),\n  group = neutralization$subject\n)\n\n\n\n\n\n\n\n\nIt does look like the degree of noise may vary by subject, e.g. 13, 14, 15. We can check this more directly by doing a PP check for the standard deviation, by subject:\n\npp_check(neutralization_m11_1,\n  type = \"stat_grouped\",\n  ndraws = 1000,\n  group = \"subject\",\n  stat = \"sd\"\n)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFor some subjects, the observed value of SD lies outside the model’s predicted values, suggesting allowing residual SD to differ by-subject could make sense.\nFor the sake of this example, we might also wonder whether the amount of noise differs by voicing:\n\nppc_dens_overlay_grouped(neutralization$vowel_dur,\n  yrep =\n    posterior_predict(neutralization_m11_1,\n      ndraws = 100\n    ),\n  group = neutralization$voicing_fact\n)\n\n\n\n\n\n\n\n\n(This could be part of the search for any difference in vowel_duration by voicing.) It seems unlikely from this plot, but we will include a term in the model to confirm.\nThis model will allow residual SD is allowed to differ by-subject, and by voicing. The SD for subject \\(j\\) is modeled as:\n\\[\\begin{align}\n\\sigma_j & = \\exp(\\sigma + \\beta^*_{voicing} + u_j) \\\\\nu_j & \\sim N(0, \\tau) \\\\\n\\end{align}\\]\nHere, \\(\\exp(\\sigma)\\) is the “average” residual SD; \\(\\beta^*_{voicing}\\) is the difference in residual SD between voicing=-0.5 and 0.5 (in log space), and \\(u_j\\) is the offset for subject \\(j\\).\nThe exponential parametrization is used so that \\(\\sigma_j\\) stays positive for all subjects. For priors, we use:\n\n\\(\\sigma \\sim N(0, \\log(50))\\) : because \\(\\sigma\\) is now inside an exponential, and its old prior had width 50.\n\\(\\beta^*_{voicing} \\sim N(0, \\log(50))\\): similar\n\\(\\tau \\sim \\exp(0.25)\\) : because log(50) is the scale, and 1/log(50) \\(\\approx\\) 0.25.\n\nReminder: if you are not comfortable determining weakly-informative priors, it’s always an option to just use brms’ default priors (see Section 6.2). This would correspond to just omitting the prior argument in the brm() call below.\n\nprior_6 &lt;- c(\n  prior(normal(150, 75), class = Intercept), # beta_0\n  prior(normal(0, 50), class = b),\n  prior(exponential(0.02), class = sd), # random-effect SDs\n  prior(lkj(1.5), class = cor), # random-effect correlation matrices\n  prior(normal(0, log(50)), class = Intercept, dpar = sigma),\n  prior(normal(0, log(50)), class = b, dpar = sigma),\n  prior(exponential(0.25),\n    class = sd, group = subject,\n    dpar = sigma\n  )\n)\n\nneutralization_m11_4 &lt;- brm(\n  data = neutralization,\n  brmsformula(\n    vowel_dur ~ voicing + place + vowel + prosodic_boundary +\n      (1 + voicing | subject) + (1 + voicing | item_pair),\n    sigma ~ 1 + voicing + (1 | subject)\n  ),\n  prior = prior_6,\n  save_pars = save_pars(all = TRUE),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  file = \"models/neutralization_m11_4.brm\"\n)\n\n\nsummary(neutralization_m11_4)\n##  Family: gaussian \n##   Links: mu = identity; sigma = log \n## Formula: vowel_dur ~ voicing + place + vowel + prosodic_boundary + (1 + voicing | subject) + (1 + voicing | item_pair) \n##          sigma ~ 1 + voicing + (1 | subject)\n##    Data: neutralization (Number of observations: 749) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~item_pair (Number of levels: 24) \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)              9.85      2.09     6.59    14.48 1.00     1699\n## sd(voicing)                2.34      1.65     0.09     6.09 1.00     1798\n## cor(Intercept,voicing)    -0.16      0.46    -0.91     0.76 1.00     4335\n##                        Tail_ESS\n## sd(Intercept)              2709\n## sd(voicing)                2320\n## cor(Intercept,voicing)     2730\n## \n## ~subject (Number of levels: 16) \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sd(Intercept)             27.46      5.64    19.05    41.61 1.01     1136\n## sd(voicing)                4.67      2.28     0.63     9.52 1.00     1534\n## sd(sigma_Intercept)        0.28      0.07     0.18     0.43 1.00     1627\n## cor(Intercept,voicing)     0.50      0.32    -0.27     0.94 1.00     3838\n##                        Tail_ESS\n## sd(Intercept)              1623\n## sd(voicing)                1743\n## sd(sigma_Intercept)        2635\n## cor(Intercept,voicing)     2602\n## \n## Regression Coefficients:\n##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept           156.70      7.12   142.69   171.32 1.01      800     1420\n## sigma_Intercept       2.93      0.08     2.78     3.07 1.00     1950     2513\n## voicing               9.41      1.95     5.70    13.41 1.00     2731     2583\n## place1               -9.59      2.78   -15.11    -4.16 1.00     2649     3009\n## place2                1.55      1.55    -1.55     4.55 1.00     2260     2599\n## vowel1                1.93      3.56    -5.17     9.04 1.00     2862     2436\n## vowel2              -16.90      2.04   -20.83   -12.78 1.00     2471     2795\n## vowel3               -3.59      1.31    -6.08    -0.99 1.00     2605     2389\n## vowel4               -6.53      1.05    -8.48    -4.40 1.00     2447     2645\n## prosodic_boundary    12.93      2.63     7.70    18.08 1.00     7406     3123\n## sigma_voicing        -0.01      0.06    -0.12     0.10 1.00     9021     2838\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nNote the new terms here:\n\nsigma_Intercept\nsigma_voicing\nsd(sigma_Intercept)\n\nThe 95% CI for sd(sigma_Intercept) is clearly above zero (speakers differ in the degree of noise), while the 95% CI for `sigma_voicing overlaps zero (residual SD doesn’t differ by voicing), as expected from our plots above.\nNew posterior predictive checks:\n\n## looks similar\npp_check(neutralization_m11_4, ndraws = 50, type = \"dens_overlay\")\n\n## any better?\nppc_dens_overlay_grouped(neutralization$vowel_dur,\n  yrep =\n    posterior_predict(neutralization_m11_4,\n      ndraws = 100\n    ),\n  group = neutralization$subject\n)\n\npp_check(neutralization_m11_4,\n  type = \"stat_grouped\",\n  ndraws = 1000,\n  group = \"subject\",\n  stat = \"sd\"\n)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe differences from the previous model are small, but we can see that the new model is slightly better. There is no no subject for whom the observed SD is outside the distribution of predicted SDs.\nThere would probably be larger changes (as in the (Nicenboim, Schad, and Vaishth 2024) example) if we had more data per subject.\n\nExercise 7.5 Perform a model comparison to check whether allowing \\(\\sigma\\) to vary, by (both) speaker and by voicing level, is justified. That is: does the data support using a distributional mixed-effects model rather than a normal (non-distributional) mixed-effects model?\n\n\n\n\n\nCiaccio, Laura Anna, and João Verı́ssimo. 2022. “Investigating Variability in Morphological Processing with Bayesian Distributional Models.” Psychonomic Bulletin & Review 29 (6): 2264–74.\n\n\nGronau, Quentin F., Henrik Singmann, and Eric-Jan Wagenmakers. 2020. “bridgesampling: An R Package for Estimating Normalizing Constants.” Journal of Statistical Software 92 (10): 1–29. https://doi.org/10.18637/jss.v092.i10.\n\n\nKurz, S. 2023. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nNicenboim, B., D. Schad, and S. Vaishth. 2024. An Introduction to Bayesian Data Analysis for Cognitive Science. https://vasishth.github.io/bayescogsci/book/. 5/2024 version.\n\n\nSonderegger, Morgan, Jane Stuart-Smith, and Jeff Mielke. 2023. “How Variable Are English Sibilants?” In Proceedings of the 20th International Congress of Phonetic Sciences, 3196–3200. Prague.\n\n\nWolock, T. M. 2020. “Distributional Regression Models: A Brms Tutorial.” https://www.tmwolock.com/index.php/2020/12/18/distributional-regression-models-brms-tutorial/.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Hierarchical Models 3</span>"
    ]
  },
  {
    "objectID": "week12.html#multinomial-models",
    "href": "week12.html#multinomial-models",
    "title": "8  Models for discrete data: counts, scales, and frequencies",
    "section": "8.5 Multinomial models",
    "text": "8.5 Multinomial models\n\n8.5.1 Without random effectgs: regularity\nFit a basic multinoimial model to the regularity data, using brms’ default priors.\nAs always, you should think more carefully about these priors for a real analysis. This is trickier for multinomial regression than for logistic regression, as usefully discussed by Barreda and Silbert (2023), Sec. 12.2.5.\n\nregularity_m12_1 &lt;-\n  brm(\n    data = regularity,\n    family = categorical(link = logit),\n    Auxiliary ~ Regularity.num + WrittenFrequency,\n    iter = 2000, warmup = 1000, cores = 4, chains = 4,\n    file = \"models/regularity_m12_1.brm\"\n  )\n\n\nsummary(regularity_m12_1)\n##  Family: categorical \n##   Links: muzijnheb = logit; muzijn = logit \n## Formula: Auxiliary ~ Regularity.num + WrittenFrequency \n##    Data: regularity (Number of observations: 700) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## muzijnheb_Intercept           -1.78      0.11    -2.00    -1.57 1.00     4533\n## muzijn_Intercept              -3.71      0.29    -4.35    -3.19 1.00     3129\n## muzijnheb_Regularity.num      -0.96      0.26    -1.45    -0.46 1.00     4050\n## muzijnheb_WrittenFrequency     0.01      0.06    -0.11     0.12 1.00     4240\n## muzijn_Regularity.num         -1.60      0.52    -2.65    -0.58 1.00     2974\n## muzijn_WrittenFrequency        0.14      0.12    -0.10     0.38 1.00     3699\n##                            Tail_ESS\n## muzijnheb_Intercept            2986\n## muzijn_Intercept               2460\n## muzijnheb_Regularity.num       3140\n## muzijnheb_WrittenFrequency     3209\n## muzijn_Regularity.num          2326\n## muzijn_WrittenFrequency        2962\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nDefine:\n\n\\(y\\) = Auxiliary (values: 1, 2, 3 = hebben, zijnheb, zijn)\n\\(p_1\\), \\(p_2\\), \\(p_3\\) are the probabilities of each Auxiliary value.\n\\(x_1\\) is WrittenFrequency\n\\(x_2\\) is Regularity.num\n\nThen the model, abstracting away from priors, is:\n\\[\\begin{align}\n\\log(\\frac{p_2}{p_1}) & = \\beta^{21}_0 + \\beta^{21}_1 x_1 + \\beta^{21}_2 x_2  \\\\\n\\log(\\frac{p_3}{p_1}) & = \\beta^{31}_0 + \\beta^{31}_1 x_1 + \\beta^{31}_2 x_2 \\\\\np_1 + p_2 + p_3 & = 1\n\\end{align}\\]\nThe “score” for each outcome is:\n\\[\\begin{align}\ns_1 & = 0 \\\\\ns_2 & = \\beta^{21}_0 + \\beta^{21}_1 x_1 + \\beta^{21}_2 x_2 \\\\\ns_3 & = \\beta^{31}_0 + \\beta^{31}_1 x_1 + \\beta^{31}_2 x_2\n\\end{align}\\]\nThese can be used to write the relationship between linear predictors (=scores) and probabilities, in “softmax” form:\n\\[\np_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^{3}\\exp(s_j)}\n\\]\nInterpretation of some coefficients:\n\nmuzijnheb_Intercept: \\(\\beta_{0}^{21}\\), the log-odds of Auxiliary = zijnheb vs. hebben, when \\(x_1 = x_2 = 0\\).\n\nBecause \\(x_1\\) and \\(x_2\\) are centered, this should be roughly the same as “average log-odds of Auxiliary = zijnheb vs. hebben [across all data]”.\n\n\n\npt_aux &lt;- prop.table(xtabs(~Auxiliary, data = regularity))\npt_aux\n## Auxiliary\n##     hebben    zijnheb       zijn \n## 0.82428571 0.14714286 0.02857143\nlog(pt_aux[2] / pt_aux[1]) # about the same as \"Estimate\" column: ~-1.7\n##   zijnheb \n## -1.723113\n\n\nmuzijn_Intercept: \\(\\beta_{0}^{31}\\) similar, for “average log-odds of Auxiliary = zijn vs. hebben [across all data]”\n\n\nlog(pt_aux[3] / pt_aux[1]) # about the same as \"Estimate\" column:\n##     zijn \n## -3.36211\n\n\nWe can also compare to the predicted probabilities, using the “scores” formulation from above:\n\n\n## these are the  \"Estimate\" values for muzijnheb_Intercept\n## and muzijn_Intercept\nscores &lt;- c(0, fixef(regularity_m12_1)[1, 1], fixef(regularity_m12_1)[2, 1])\n\n## this is the equation for p_i from above\nexp(scores) / sum(exp(scores))\n## [1] 0.83775206 0.14183738 0.02041056\n\nThese are similar to the empirical probabilities:\n\npt_aux\n## Auxiliary\n##     hebben    zijnheb       zijn \n## 0.82428571 0.14714286 0.02857143\n\n\nmuzijnheb_Regularity.num: \\(\\beta_{2}^{21}\\) the change in “log-odds of Auxiliary = zijnheb vs. hebben” between Regularity=regular and Regularity=irregular.\n\nThe muzijnheb_ notation is because this is a coefficient of the logit model of \\(\\mu\\) for zijnheb vs. reference level, hebben.\n\n\n\npt_aux_reg &lt;- prop.table(xtabs(~ Regularity + Auxiliary, data = regularity), margin = 1)\npt_aux_reg\n##            Auxiliary\n## Regularity      hebben    zijnheb       zijn\n##   irregular 0.67924528 0.24528302 0.07547170\n##   regular   0.86691312 0.11829945 0.01478743\n\nLog-odds of zijnheb vs. hebben:\n\nRegularity=irregular: log(0.245/0.769) = -1.14\nRegularity=regular: log(0.11829945/0.86691312) = -1.99\n\nChange in log-odds: -1.99 - -1.14 = -0.85\nThat’s similar to the muzijnheb_Regularity.num estimate. (It won’t be exactly the same, because the model also accounts for WrittenFrequency.)\nThis exercise hopefully demonstrates that interpreting the actual model coefficients is unintiitve. Instead it is better to understand fitted multinomial models by computing quantities of interest, and using these downstream—in effect plots, posterior examination, hypothesis tests, etc.\nThe equivalent frequentist model is shown in ?sec-extra-dd1-multinomial.\n\n8.5.1.1 Plotting effects\nMarginal effect of WrittenFrequency (averaging over Regularity.num):\n\nconditional_effects(regularity_m12_1, categorical = TRUE, effects = c(\"WrittenFrequency\"))\n\n\n\n\n\n\n\n\nThere is little frequency effect evident in this model, that also includes verb Regularity. (Note that regular verbs have higher frequency; presumably there would be a frequency effect in a model without Regularity.) This is consistent with the 95% CI for both _WrittenFrequency terms overlapping zero.\nMarginal effect of Regularity.num, treated as a numeric predictor (so you need to imagine there are just two values):\n\nconditional_effects(regularity_m12_1, categorical = TRUE, effects = c(\"Regularity.num\"))\n\n\n\n\n\n\n\n\nTo see marginal effects of Regularity treated as a factor requires more legwork. (Code commented out, and there is probably an easier way to do this.) Often it is easier to just refit your model using factors for discrete predictors, and use it to make prediction plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 8.5  \n\nUse hypothesis() to formally test whether either WrittenFrequency coefficient is larger than 0.\nMake a plot showing the marginal effect of Regularity, treated as a factor.7",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Models for discrete data: counts, scales, and frequencies</span>"
    ]
  },
  {
    "objectID": "week13.html",
    "href": "week13.html",
    "title": "9  Multivariate models",
    "section": "",
    "text": "9.1 Preliminaries\nLoad libraries we will need:\nlibrary(brms)\nlibrary(lme4)\nlibrary(arm)\nlibrary(tidyverse)\n\nlibrary(tidybayes)\n\nlibrary(bayestestR)\n\nlibrary(phonTools) # TODO\n\nlibrary(patchwork)\n\n# avoids bug where select from MASS is used\nselect &lt;- dplyr::select",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multivariate models</span>"
    ]
  },
  {
    "objectID": "week13.html#preliminaries",
    "href": "week13.html#preliminaries",
    "title": "9  Multivariate models",
    "section": "",
    "text": "Practical notes\n\n\n\n\n\n\nIf you have loaded rethinking, you need to detach it before using brms. See Kurz (2023) Sec. 4.3.1.\nI use the file argument when fitting brms models to make compiling this document easier (so the models don’t refit every time I compile). You may or may not want to do this for your own models. See file and file_refit arguments in ?brm.\nHere I set the file_refit option so “brms will refit the model if model, data or algorithm as passed to Stan differ from what is stored in the file.”\n\n\noptions(brms.file_refit = \"on_change\")\n\n\nI use chains = 4, cores = 4 when fitting brm models below—this means 4 chains, each to be run on one core on my laptop. cores = 4 may need to be adjusted for your computer. (You may have fewer or more cores; I have 8 cores, so this leaves 50% free.) You should figure out how to use multiple cores on your machine.\nMake numbers be printed only to 3 digits, for neater output:\n\n\noptions(digits = 3)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multivariate models</span>"
    ]
  },
  {
    "objectID": "week13.html#data",
    "href": "week13.html#data",
    "title": "9  Multivariate models",
    "section": "9.2 Data",
    "text": "9.2 Data\n\n\n\n\n\n9.2.1 American English vowels\nInstall the joeysvowels package (Stanley 2020), if you haven’t done so already:\n\nremotes::install_github(\"joeystanley/joeysvowels\")\n\n\nThe joeysvowels package provides a handful of datasets, some subsets of others, that contain formant measurements and other information about the vowels in my own speech. The purpose of the package is to make vowel data easily accessible for demonstrating code snippets when demonstrating working with sociophonetic data.\n\n\nlibrary(joeysvowels)\n\nmidpoints &lt;- mutate(midpoints, dur = end - start)\n\nWe’ll use the midpoints dataset:  these are measures of F1 and F2 at vowel midpoint for one speaker’s vowels from many words in a controlled context (surrounding coronal consonants): “odd”, “dad”, “sod”, “Todd”, and so on. Standard F1/F2 vowel plot of all data, with 95% ellipses:1\n\n\nCode\nmidpoints %&gt;% ggplot(aes(x = F2, y = F1)) +\n  geom_point(aes(color = vowel), size = 0.2) +\n  scale_x_reverse() +\n  scale_y_reverse() +\n  stat_ellipse(level = 0.95, aes(color = vowel))\n\n\n\n\n\n\n\n\n\nNote how vowel distributions differ both in location (in F1/F2 space) and shape: the direction and size of the ellipse.\nLet’s further restrict to just the THOUGHT and LOT vowels for a simple example:\n\ntwovowels &lt;- filter(midpoints, vowel %in% c(\"LOT\", \"THOUGHT\")) %&gt;% droplevels()\n\nRQs for this data could be:\n\nAre LOT and THOUGHT pronounced differently?\nIf so, how?\n\nThese two vowels are merged for many North American English speakers, but (by self-report) not for this speaker.\nTheir data for just these vowels looks like:\n\n\nCode\ntwovowels %&gt;% ggplot(aes(x = F2, y = F1)) +\n  geom_point(aes(color = vowel), size = 0.2) +\n  scale_x_reverse() +\n  scale_y_reverse() +\n  stat_ellipse(level = 0.95, aes(color = vowel))\n\n\n\n\n\n\n\n\n\n\n\n9.2.2 Canadian English voice quality\nThis is data from a project by Jeanne Brown, a PhD student at McGill.\n\nvq_data &lt;- readRDS(\"jb_creak_data.rds\")\n\nThis is a greatly simplified subset of the data from from Jeanne’s paper on acoustic measures of “creaky voice” in Canadian English-French bilingual speakers.\nThis subset is just:\n\nA couple acoustic measures\nEnglish speech only (the dataset also contains French)\nOnly one utterance position (66-99% through utterance)\n\nThere are 4884 observations.\nEvery row corresponds to two acoustic measures that correlate with creaky voice, measured for a single vowel, in a corpus of podcast speech:\n\nCPP: cepstral peak prominence\n\nContinuous, where a lower value is expected to correlate with creakiness.\n\nbad_f0_track: whether f0 could not be measured for this vowel\n\nBinary (0/1), where 1 is expected to correlate with creakiness.\n\n\nOther columns:\n\nSex, Sex_c: factor and (centered) numeric versions of speaker gender (higher = male)\nYOB, YOB_c: raw and normalized (mean/2 SD) year of birth of speaker.\ndev_rate: a measure of speaking rate, relative to the speaker’s mean.\nprev_seg, foll_seg: what kind of segment precede and follow the vowel:\n\nvowel, or various consonants (voiceless stop, etc.), or none (= word boundary)\n\n\nThe actual research questions are:\n\nDoes speaker Sex affect creakiness?\nDoes speaker YOB affect creakiness?\n\nJeanne’s paper examines these questions on one acoustic measure at a time. We’ll consider a couple additional questions, made possible by jointing modeling the two acoustic measures.\n\nDo speakers with higher CPP have a higher probability of bad_f0_track?\nDo contexts ?\n\nEither one would give insight into whether there is one underlying aspect of voice quality being captured by both measures.\nEmpirical plots:\n\np1 &lt;- vq_data %&gt;% group_by(Speaker) %&gt;% mutate(CPP = mean(CPP)) %&gt;% \n  ggplot(aes(x = Sex, y = CPP)) + geom_boxplot() + labs(y = \"Speaker average CPP\")\n\np2 &lt;- vq_data %&gt;% group_by(Speaker) %&gt;% mutate(CPP = mean(CPP)) %&gt;% \n  ggplot(aes(x = YOB, y = CPP)) + geom_smooth(method = 'lm') + geom_point() + labs(y = \"Speaker average CPP\")\n\np1 + p2\n## `geom_smooth()` using formula = 'y ~ x'\n\n\np3 &lt;- vq_data %&gt;% group_by(Speaker) %&gt;% mutate(bt = mean(bad_f0_track)) %&gt;% \n  ggplot(aes(x = Sex, y = bt)) + geom_boxplot() + labs(y = \"Speaker % bad f0 tracks\")\n\np4 &lt;- vq_data %&gt;% group_by(Speaker) %&gt;% mutate(bt = mean(bad_f0_track)) %&gt;% \n  ggplot(aes(x = YOB, y = bt)) + geom_smooth(method = 'lm') + geom_point() + labs(y = \"Speaker % bad f0 tracks\")\n\np3 + p4\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTODO: make similar plots by context",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multivariate models</span>"
    ]
  },
  {
    "objectID": "hw5.html#question-3-optional",
    "href": "hw5.html#question-3-optional",
    "title": "16  Homework 5",
    "section": "16.4 Question 3 (Optional)",
    "text": "16.4 Question 3 (Optional)\nThis question is worth up to 20% extra credit.\n(a). Make a plot of the model’s predicted vowel_dur, by voicing, for each speaker, divided up by population. That is, it should look like the empirical plot above (except the lines come from model predictions). Comparing to the empirical plot, how does the model prediction plot reflect your answer to the previous question?\n\nWe consider a second model, which fits separate by-speaker random effects for population=Austrian and German subjects. That is:\n\nThere are by-speaker intercepts, and `voicing slopes for German speakers, with one set of variances (+ one correlation)\nThere are by-speaker intercepts, and `voicing slopes for Austrian speakers, with another set of variances (+ one correlation)\n\n(b). Figure out how to fit this model, which should be called grawunder_m2. Do so.2\n(c). Which terms in the model address (3) and (4)? Explain.\n(d). Carry out hypothesis tests (using hypothesis) assessing (3) and (4) for this model.\n\n\n\n\nGrawunder, Sven. 2014. “Wie Schaukt a Pruag Aos?-Stabile Phonetische Unterschiede in Wortformen Nach Auslautverhärtung in Tirol.” In Sprechwissenschaft: Bestand, Prognose, Perspektive, 209–20. Peter Lang.\n\n\nNicenboim, Bruno, Timo B Roettger, and Shravan Vasishth. 2018. “Using Meta-Analysis for Evidence Synthesis: The Case of Incomplete Neutralization in German.” Journal of Phonetics 70: 39–55.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Homework 5</span>"
    ]
  },
  {
    "objectID": "week13.html#footnotes",
    "href": "week13.html#footnotes",
    "title": "9  Multivariate models",
    "section": "",
    "text": "If you’re not familiar with these kinds of plot: \\(x\\) and \\(y\\) axes go from larger to smaller.↩︎\nNote that these data actually come from different words (column word), so our model should include random effects. I haven’t included random effects just to get the models to fit faster, for pedagogical purposes, and because there are very few observations here anyway (127) relative to model complexity↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multivariate models</span>"
    ]
  },
  {
    "objectID": "week13.html#intrinsically-related-y_1-and-y_2",
    "href": "week13.html#intrinsically-related-y_1-and-y_2",
    "title": "9  Multivariate models",
    "section": "9.3 Intrinsically-related \\(y_1\\) and \\(y_2\\)",
    "text": "9.3 Intrinsically-related \\(y_1\\) and \\(y_2\\)\nA multivariate model is one where two or more response variables are modeled simultaneously. In these notes we assume there are just two variables being modeled, called \\(y_1\\) and \\(y_2\\).\nWhile the model doesn’t care how \\(y_1\\) and \\(y_2\\) are conceptually related, it’s useful to think about different cases:\n\n\\(y_1\\) and \\(y_2\\) are intrinsically related\n\nRQs typically about how factors jointly affect \\(y_1\\) and \\(y_2\\)\n\n\\(y_1\\) and \\(y_2\\) aren’t intrinsically related\n\nRQs can assess whether and how they’re related\n\n\nAn example of the first type is vowel data, which is commonly modeled with at least F1 (first formant) and F2 (second formant). Sometimes F3 and/or duration are included as well.\nWhile F1 and F2 are roughly related to articulatory parameters (jaw opening and tongue backness), this is a rough approximation, and any phonetician would agree that vowels “occur” in multi-dimensional cue space. This is the sense in which F1 and F2 are intrinsically-related.\n\n9.3.1 Independent models\nNonetheless, in analyzing formant data, researchers usually model F1 and F2 are as independent. Let’s do this for the twovowels data, accounting for differences between vowels in location and shape.\nTo keep our model relatively simple, we’ll assume that shape (parametrized by sigma) doesn’t differ by-word (our data is too sparse to estimate this anyway).\nThis would be two distributional linear mixed-effects models:\n\n## Include weak priors, so we can calculate Bayes Factors later\n##\n## Prior: 250 Hz is a large effect for formants\n## 15 Hz is a large effect for log\nprior_1 &lt;- c(\n  prior(normal(0, 250), class = b),\n  prior(normal(0, 15), class = b, dpar = sigma)\n)\n\n## formula for F1 model\nbf1 &lt;- bf(\n  F1 ~ vowel,\n  sigma ~ vowel\n)\n\n## formula for F2 model\nbf2 &lt;- bf(\n  F2 ~ vowel,\n  sigma ~ vowel\n)\n\n## Fitting with more iterations than usual, to get OK Bayes factors\n## really should be iter = 10k\ntv_f1_m1 &lt;- brm(\n  formula = bf1, \n  family = gaussian(), \n  data = twovowels, \n  prior = prior_1, \n  chains = 4, cores = 4, iter = 4000, \n  file = \"models/tv_f1_m1.brm\"\n)\n\n\ntv_f2_m1 &lt;- brm(\n  formula = bf2, \n  family = gaussian(), \n  data = twovowels, \n  prior = prior_1, \n  chains = 4, cores = 4, iter = 4000, \n  file = \"models/tv_f2_m1.brm\"\n  )\n\nModel results:\n\ntv_f1_m1\n##  Family: gaussian \n##   Links: mu = identity; sigma = log \n## Formula: F1 ~ vowel \n##          sigma ~ vowel\n##    Data: twovowels (Number of observations: 127) \n##   Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n##          total post-warmup draws = 8000\n## \n## Regression Coefficients:\n##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept            620.66      9.41   602.32   638.89 1.00     7834     5862\n## sigma_Intercept        4.24      0.09     4.07     4.43 1.00     9108     6190\n## vowelTHOUGHT         -40.85     11.79   -63.85   -16.97 1.00     8545     5870\n## sigma_vowelTHOUGHT    -0.15      0.13    -0.40     0.10 1.00     8927     6271\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\ntv_f2_m1\n##  Family: gaussian \n##   Links: mu = identity; sigma = log \n## Formula: F2 ~ vowel \n##          sigma ~ vowel\n##    Data: twovowels (Number of observations: 127) \n##   Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n##          total post-warmup draws = 8000\n## \n## Regression Coefficients:\n##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept           1152.83     10.50  1132.21  1173.36 1.00     9436     6252\n## sigma_Intercept        4.34      0.10     4.16     4.55 1.00     8138     5465\n## vowelTHOUGHT        -141.12     17.21  -175.02  -107.61 1.00     7131     6459\n## sigma_vowelTHOUGHT     0.39      0.13     0.13     0.64 1.00     8158     5818\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nBayes Factors:\n\nbf_pointnull(tv_f1_m1)\n## Sampling priors, please wait...\n## Warning: Bayes factors might not be precise.\n##   For precise Bayes factors, sampling at least 40,000 posterior samples is\n##   recommended.\n## Bayes Factor (Savage-Dickey density ratio)\n## \n## Parameter          |       BF\n## -----------------------------\n## (Intercept)        | 3.95e+66\n## sigma_Intercept    | 7.67e+67\n## vowelTHOUGHT       |    15.28\n## sigma_vowelTHOUGHT |    0.016\n## \n## * Evidence Against The Null: 0\nbf_pointnull(tv_f2_m1)\n## Sampling priors, please wait...\n## Warning: Bayes factors might not be precise.\n##   For precise Bayes factors, sampling at least 40,000 posterior samples is\n##   recommended.\n## Bayes Factor (Savage-Dickey density ratio)\n## \n## Parameter          |        BF\n## ------------------------------\n## (Intercept)        | 4.85e+131\n## sigma_Intercept    |  4.86e+55\n## vowelTHOUGHT       |  6.53e+08\n## sigma_vowelTHOUGHT |     0.696\n## \n## * Evidence Against The Null: 0\n\n(These are approximate because we didn’t fit the model for many iterations!)\nAnswers to our RQs, for F1 and F2 separately:\n\nYes, they’re different, in both F1 and F2\nvowel = THOUGHT:\n\nlower F1 and F2, much clearer for F2\nPossible more variable in F2 (95% CredI positive, but BF is inconclusive)\n\n\n\n9.3.1.1 Plotting model predictions\nLet’s plot model predictions as F1/F2 ellipses, for the LOT and THOUGHT vowels.\nFor 95% prediction ellipses (like 95% PIs, but in 2D), we first simulate data for each vowel for F1 and F2, separately.2\n\nnd &lt;- data.frame(\n  vowel = c(\"THOUGHT\", \"LOT\")\n)\n##\nf1_samples &lt;- tv_f1_m1 %&gt;%\n  add_predicted_draws(newdata = nd, ndraws = 1000) %&gt;%\n  dplyr::select(.draw, vowel, F1 = .prediction)\n## Adding missing grouping variables: `.row`\n\nf2_samples &lt;- tv_f2_m1 %&gt;%\n  add_predicted_draws(newdata = nd, ndraws = 1000) %&gt;%\n  dplyr::select(.draw, vowel, F2 = .prediction)\n## Adding missing grouping variables: `.row`\n\nThese just look like:\n\nf1_samples %&gt;% head()\n## # A tibble: 6 × 4\n## # Groups:   vowel, .row [1]\n##    .row .draw vowel      F1\n##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1     1     1 THOUGHT  621.\n## 2     1     2 THOUGHT  560.\n## 3     1     3 THOUGHT  551.\n## 4     1     4 THOUGHT  597.\n## 5     1     5 THOUGHT  643.\n## 6     1     6 THOUGHT  568.\n\nMerge F1 and F2 into one dataframe, by matching draws across the two dataframes of predictions:\n\nmerged_samples &lt;- inner_join(\n  f1_samples,\n  f2_samples, \n  by = c(\".draw\", \"vowel\")\n  ) %&gt;% ## drop irrelevant columns\n  select(vowel, F1, F2)\n\nF1/F2 plot with 95% prediction ellipses, over the empirical data:\n\n\nCode\nggplot(merged_samples, aes(x = F2, y = F1, color = vowel)) +\n  geom_point(alpha = 0.5, size = 0.5, data = twovowels) +\n  stat_ellipse(level = 0.95) +\n  theme_minimal() +\n  labs(x = \"F1 (Hz)\", y = \"F2 (Hz)\") +\n  scale_x_reverse() +\n  scale_y_reverse()\n\n\n\n\n\n\n\n\n\nWe can do the same procedure to produce 95% credible ellipses by replacing add_predicted_draws() with add_epred_draws() in the code above.\nThis gives:\n\n\nCode\nf1_samples &lt;- tv_f1_m1 %&gt;%\n  add_epred_draws(newdata = nd, ndraws = 1000) %&gt;%\n  select(.draw, vowel, F1 = .epred)\n## Adding missing grouping variables: `.row`\n\nf2_samples &lt;- tv_f2_m1 %&gt;%\n  add_epred_draws(newdata = nd, ndraws = 1000) %&gt;%\n  select(.draw, vowel, F2 = .epred)\n## Adding missing grouping variables: `.row`\n\nmerged_samples &lt;- inner_join(f1_samples, f2_samples, by = c(\".draw\", \"vowel\")) %&gt;% ## drop irrelevant columns\n  select(vowel, F1, F2)\n\nggplot(merged_samples, aes(x = F2, y = F1, color = vowel)) +\n  stat_ellipse(level = 0.95) +\n  theme_minimal() +\n  geom_point(alpha = 0.5, size = 0.5, data = twovowels) +\n  labs(x = \"F1 (Hz)\", y = \"F2 (Hz)\") +\n  scale_x_reverse() +\n  scale_y_reverse()\n\n\n\n\n\n\n\n\n\nThe second plot captures visually the result that LOT and THOUGHT are distinct in both F1 and F2: the ellipses are nowhere near overlapping on either axis.\n\nThis univariate method, where F1 and F2 are analyzed separately, is a perfectly OK way to analyze vowel formant data, but there are a few issues:\n\nOur RQs are about effects on F1 and F2 jointly (and potentially other acoustic cues, like F3 or duration)—but we are reporting on each one separately.\n\nAt best, the univariate approach is losing power.\n\nThe models assume that F1 and F2 are uncorrelated—visually, that the F1/F2 ellipses are not tilted in F1/F2 space. We can see that’s not the case from the empirical plot, especially for vowel = THOUGHT.\n\n\n\n\n9.3.2 Multivariate model\nWe can address these issues by fitting a multivariate model.\n\n## when all response variables have the same\n## model structure, can use this shortcut\nbf3 &lt;- bf(mvbind(F1, F2) ~ vowel, sigma ~ vowel) +\n  set_rescor(TRUE)\n\n## set a very weak prior, so we can calculate Bayes Factors later.\nprior_2 &lt;- c(\n  prior(normal(0, 250), resp = F1, class = b),\n  prior(normal(0, 250), resp = F2, class = b),\n  prior(normal(0, 15), class = b, resp = F1, dpar = sigma),\n  prior(normal(0, 15), class = b, resp = F2, dpar = sigma)\n)\n\n## here we save parameters so we'll be able to calculate\n## Bayes Factors versus subset models later.\ntv_joint_m1 &lt;- brm(\n  formula = bf3,\n  family = gaussian(), \n  data = twovowels, \n  chains = 4, cores = 4, iter = 2000, \n  prior = prior_2, \n  save_pars = save_pars(all = TRUE), \n  file = \"models/tv_joint_m1.brm\"\n)\n\nSome notes:\n\nset_rescor(TRUE): allows the residuals of F1 and F2 to be correlated.\n\nThis makes sense in the current case (F1 and F2 are intrinsically related), but in other cases (i.e., mediation analyses) won’t necessarily make sense.\n\nIt’s also not possible if one of the models for \\(y_1\\) or \\(y_2\\) doesn’t have residuals, as we’ll see later.\n\nThe family argument together with the fact that formula refers to multiple response variables is enough for brms to determine that this is a multivariate linear regression model.\n\nSummary:\n\ntv_joint_m1\n##  Family: MV(gaussian, gaussian) \n##   Links: mu = identity; sigma = log\n##          mu = identity; sigma = log \n## Formula: F1 ~ vowel \n##          sigma ~ vowel\n##          F2 ~ vowel \n##          sigma ~ vowel\n##    Data: twovowels (Number of observations: 127) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## F1_Intercept            620.81      9.48   601.78   639.01 1.00     5663\n## sigma_F1_Intercept        4.27      0.10     4.08     4.46 1.00     6424\n## F2_Intercept           1152.61     10.40  1132.49  1172.91 1.00     5860\n## sigma_F2_Intercept        4.36      0.10     4.19     4.56 1.00     5386\n## F1_vowelTHOUGHT         -41.01     11.96   -64.68   -18.06 1.00     5870\n## sigma_F1_vowelTHOUGHT    -0.18      0.13    -0.43     0.07 1.00     6934\n## F2_vowelTHOUGHT        -140.50     16.94  -174.17  -108.07 1.00     5160\n## sigma_F2_vowelTHOUGHT     0.35      0.12     0.10     0.58 1.00     5993\n##                       Tail_ESS\n## F1_Intercept              3272\n## sigma_F1_Intercept        3104\n## F2_Intercept              3330\n## sigma_F2_Intercept        3249\n## F1_vowelTHOUGHT           2995\n## sigma_F1_vowelTHOUGHT     2895\n## F2_vowelTHOUGHT           2983\n## sigma_F2_vowelTHOUGHT     3326\n## \n## Residual Correlations: \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(F1,F2)    -0.26      0.08    -0.42    -0.10 1.00     6181     3241\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nWhat some regression coefficients mean:\n\nF1_Intercept: F1 for vowel = LOT\nsigma_F1_Intercept: \\(\\log(\\sigma)\\) for vowel = LOT\nsigma_F1_vowelTHOUGHT: difference in \\(\\log(\\sigma)\\) between vowel = THOUGHT and LOT.\n\nAnother new term is rescor(F1,F2): the correlation between the residuals of F1 and F2. Its interpretation is, “how correlated are F1 and F2, after accounting for other factors [here: word, vowel]”.\n\nExercise 9.1 For each of the following coefficients: consider the direction of the model’s estimate, as well as whether the 95% CredI overlaps zero. What does this reflect, visually, in the empirical plot of the data in Section 9.2.1?\n\nF2_Intercept\nF1_vowelTHOUGHT\nrescor(F1,F2)\nsigma_F2_vowelTHOUGHT\n\n\n\n9.3.2.1 Plotting model predictions\nTo get 95% prediction ellipses, we again simulate data for each vowel:\n\nsamples &lt;- tv_joint_m1 %&gt;%\n  add_predicted_draws(newdata = nd, ndraws = 1000) %&gt;%\n  ## need to turn data from \"long\" to \"wide\" format for plotting\n  pivot_wider(names_from = .category, values_from = .prediction)\n\nThese draws look like:\n\nsamples %&gt;% head()\n## # A tibble: 6 × 7\n## # Groups:   vowel, .row [1]\n##   vowel    .row .chain .iteration .draw    F1    F2\n##   &lt;chr&gt;   &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 THOUGHT     1     NA         NA     1  588. 1077.\n## 2 THOUGHT     1     NA         NA     2  567. 1193.\n## 3 THOUGHT     1     NA         NA     3  474. 1047.\n## 4 THOUGHT     1     NA         NA     4  637.  951.\n## 5 THOUGHT     1     NA         NA     5  630.  917.\n## 6 THOUGHT     1     NA         NA     6  494.  884.\n\nF1/F2 plot with 95% prediction ellipses, over the empirical data:\n\nggplot(samples, aes(x = F2, y = F1, color = vowel)) +\n  stat_ellipse(level = 0.95) +\n  theme_minimal() +\n  geom_point(alpha = 0.5, size = 0.5, data = twovowels) +\n  labs(x = \"F1 (Hz)\", y = \"F2 (Hz)\") +\n  scale_x_reverse() +\n  scale_y_reverse()\n\n\n\n\n\n\n\n\nComparing to the same plot for the univariate model:\n\nLooks much closer to the empirical plot, compared to model predictions in Section 9.3.1.1.\nCaptures the fact that F1 and F2 are not independent, for either LOT or THOUGHT (= ellipses are tilted).\n\n\n\n9.3.2.2 Model comparison\nThe multivariate model also allows us to directly address the question of whether vowel matters, by re-fitting the model without vowel and then comparing the full and subset models.\n\nbf4 &lt;- bf(mvbind(F1, F2) ~ 1, sigma ~ 1) +\n  set_rescor(TRUE)\n\n## note: using default prior, prior_2 wouldn't work\ntv_joint_m2 &lt;- brm(\n  formula = bf4, ,\n  family = gaussian(), data = twovowels, chains = 4, cores = 4, iter = 3000, save_pars = save_pars(all = TRUE), file = \"models/tv_joint_m2.brm\"\n)\n\nModel comparison using PSIS-LOO:\n\ntv_joint_m1 &lt;- add_criterion(tv_joint_m1, criterion = \"loo\", moment_match = TRUE)\ntv_joint_m2 &lt;- add_criterion(tv_joint_m2, criterion = \"loo\", moment_match = TRUE)\n\nloo_compare(tv_joint_m1, tv_joint_m2)\n##             elpd_diff se_diff\n## tv_joint_m1   0.0       0.0  \n## tv_joint_m2 -37.5       8.1\n\nModel comparison using a Bayes Factor:\n\nbayesfactor_models(tv_joint_m2, tv_joint_m1)\n## Warning: Bayes factors might not be precise.\n##   For precise Bayes factors, sampling at least 40,000 posterior samples is\n##   recommended.\n## Computation of Marginal Likelihood: estimating marginal likelihood,\n##   please wait...\n## Computation of Marginal Likelihood: estimating marginal likelihood,\n##   please wait...\n## Bayes Factors for Model Comparison\n## \n##     Model       BF\n## [2]       2.91e+11\n## \n## * Against Denominator: [1]\n## *   Bayes Factor Type: marginal likelihoods (bridgesampling)\n\nLOO and BF addresses RQ1 with a single number: LOT and THOUGHT differ in F1/F2, and the difference is (highly) meaningful.\n\nExercise 9.2 Make an F1/F2 plot with 95% confidence ellipses for each vowel, for the joint model (tv_joint_m1).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multivariate models</span>"
    ]
  },
  {
    "objectID": "week13.html#assessing-relatedness-of-y_1-and-y_2",
    "href": "week13.html#assessing-relatedness-of-y_1-and-y_2",
    "title": "9  Multivariate models",
    "section": "9.4 Assessing relatedness of \\(y_1\\) and \\(y_2\\)",
    "text": "9.4 Assessing relatedness of \\(y_1\\) and \\(y_2\\)\nWe fit a multivariate model of bad_f0_track and CPP for the vq_data dataset, accounting for:\n\nBoth \\(y_1\\) and \\(y_2\\) can depend on Sex, YOB_c (RQs 1, 2)\nSpeakers can vary in \\(y_1\\) and \\(y_2\\)\nThe way speakers vary may be correlated between \\(y_1\\) and \\(y_2\\) (RQ 3)\nContexts can vary in \\(y_1\\) and \\(y_2\\), i.e., prev_seg and foll_seg\nThe way contexts vary may be correlated between \\(y_1\\) and \\(y_2\\) (RQ 4)\n\nModel formula:\n\n## just a basic prior for random-effect correlation matrices\nprior_1 &lt;- c(\n  prior(lkj(1.5), class=cor) \n)\n\n\ncpp_model &lt;- brmsformula(\n  CPP ~  Sex_c + YOB_c+dev_rate  +\n    (1 | p | Speaker) +\n    (1 | q | prev_seg) +\n    (1 | r | foll_seg),\n  family = gaussian()\n)\n\nbad_f0_track_model &lt;-  brmsformula(bad_f0_track ~  Sex_c + YOB_c+dev_rate  +\n  (1 | p | Speaker) +\n  (1 | q | prev_seg) +\n  (1 | r | foll_seg), family = bernoulli()\n) \n\njoint_model &lt;- cpp_model + bad_f0_track_model + set_rescor(FALSE)\n\nThe full model is:\n\njoint_model\n## CPP ~ Sex_c + YOB_c + dev_rate + (1 | p | Speaker) + (1 | q | prev_seg) + (1 | r | foll_seg) \n## bad_f0_track ~ Sex_c + YOB_c + dev_rate + (1 | p | Speaker) + (1 | q | prev_seg) + (1 | r | foll_seg)\n\nIn this formula, note the | p | |q|, etc. in the random effects. This allows for correlations across random effects for different response variables. For example, (1 | p | Speaker): allows for correlation between by-speaker random intercepts for cpp and bad_track_f0. Its interpretation is, “how correlated are a speaker’s values for these two acoustic cues, after accounting for their Sex and YOB?”\nThere are four such terms “tying” the two models (for CPP and bad_f0_track) together.\nFit the model:\n\ncpp_badtrack_joint_m1 &lt;- brm(formula = formula_4, \n                             data = PS_sub2 ,\n                             chains = 4, cores = 4, iter = 2000,\n                             prior = prior_1, \n                             save_pars = save_pars(all = TRUE),\n                             file = 'models/cpp_badtrack_joint_m1.brm')\n\nModel summary:\n\ncpp_badtrack_joint_m1\n##  Family: MV(gaussian, bernoulli) \n##   Links: mu = identity; sigma = identity\n##          mu = logit \n## Formula: CPP ~ Sex_c + YOB_c + dev_rate + (1 | p | Speaker) + (1 | q | prev_seg) + (1 | r | foll_seg) \n##          bad_f0_track ~ Sex_c + YOB_c + dev_rate + (1 | p | Speaker) + (1 | q | prev_seg) + (1 | r | foll_seg) \n##    Data: PS_sub2 (Number of observations: 4884) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Multilevel Hyperparameters:\n## ~foll_seg (Number of levels: 9) \n##                                         Estimate Est.Error l-95% CI u-95% CI\n## sd(CPP_Intercept)                           0.77      0.25     0.43     1.36\n## sd(badf0track_Intercept)                    0.63      0.21     0.35     1.15\n## cor(CPP_Intercept,badf0track_Intercept)    -0.74      0.22    -0.97    -0.17\n##                                         Rhat Bulk_ESS Tail_ESS\n## sd(CPP_Intercept)                       1.00     1118     1968\n## sd(badf0track_Intercept)                1.00     1338     1923\n## cor(CPP_Intercept,badf0track_Intercept) 1.00     1313     1681\n## \n## ~prev_seg (Number of levels: 9) \n##                                         Estimate Est.Error l-95% CI u-95% CI\n## sd(CPP_Intercept)                           1.07      0.28     0.66     1.76\n## sd(badf0track_Intercept)                    1.22      0.32     0.75     2.00\n## cor(CPP_Intercept,badf0track_Intercept)    -0.94      0.08    -1.00    -0.69\n##                                         Rhat Bulk_ESS Tail_ESS\n## sd(CPP_Intercept)                       1.00     1173     1743\n## sd(badf0track_Intercept)                1.00     1241     1583\n## cor(CPP_Intercept,badf0track_Intercept) 1.00     1347     2259\n## \n## ~Speaker (Number of levels: 49) \n##                                         Estimate Est.Error l-95% CI u-95% CI\n## sd(CPP_Intercept)                           1.75      0.20     1.40     2.17\n## sd(badf0track_Intercept)                    0.71      0.09     0.56     0.91\n## cor(CPP_Intercept,badf0track_Intercept)    -0.07      0.15    -0.36     0.23\n##                                         Rhat Bulk_ESS Tail_ESS\n## sd(CPP_Intercept)                       1.01      622      987\n## sd(badf0track_Intercept)                1.00     1206     2052\n## cor(CPP_Intercept,badf0track_Intercept) 1.00     1063     1706\n## \n## Regression Coefficients:\n##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## CPP_Intercept           20.51      0.52    19.51    21.54 1.01      692\n## badf0track_Intercept    -0.36      0.49    -1.30     0.60 1.00      708\n## CPP_Sex_c               -0.99      0.51    -1.97     0.02 1.00      668\n## CPP_YOB_c                1.15      0.53     0.08     2.20 1.01      631\n## CPP_dev_rate            -0.15      0.04    -0.23    -0.08 1.00     4965\n## badf0track_Sex_c         1.14      0.22     0.70     1.57 1.01     1040\n## badf0track_YOB_c        -0.32      0.22    -0.77     0.09 1.00     1232\n## badf0track_dev_rate     -0.08      0.03    -0.15    -0.03 1.00     5514\n##                      Tail_ESS\n## CPP_Intercept            1085\n## badf0track_Intercept     1237\n## CPP_Sex_c                1283\n## CPP_YOB_c                1137\n## CPP_dev_rate             2877\n## badf0track_Sex_c         1511\n## badf0track_YOB_c         1686\n## badf0track_dev_rate      3238\n## \n## Further Distributional Parameters:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma_CPP     2.80      0.03     2.74     2.86 1.00     5244     2597\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nA new type of term here are the random-effect correlations corresponding to different models.\ncor(CPP_Intercept,badf0track_Intercept) is the correlation between by-speaker random intercepts for cpp and bad_track_f0.\n\nExercise 9.3  \n\nWhich terms in the model address RQs 1 and 2? What answers does it suggest?\nWhich terms in the model address RQs 3 and 4? What answers do they suggest?\nConsider the effect of foll_seg. Which following contexts give the most and least creakiness (= higher bad_f0_track, lower CPP)? Can you think of why these make (phonetic) sense?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarreda, Santiago, and Noah Silbert. 2023. Bayesian Multilevel Models for Repeated Measures Data: A Conceptual and Practical Introduction in r. Taylor & Francis. https://santiagobarreda.com/bmmrmd/.\n\n\nBürkner, Paul-Christian. 2024. “Estimating Multivariate Models with Brms.” https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html.\n\n\nKurz, S. 2023. Statistical Rethinking (Second Edition) with Brms, Ggplot2, and the Tidyverse. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nSmith, I., M. Sonderegger, and The SPADE Consortium. 2024. “Modelled Multivariate Overlap: A method for measuring vowel merger.” In Proceedings of Interspeech 2024, 457–61.\n\n\nStanley, Joey. 2020. Joeysvowels: Datasets Based on My Speech. https://github.com/joeystanley/joeysvowels.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multivariate models</span>"
    ]
  },
  {
    "objectID": "FDA_edited.html",
    "href": "FDA_edited.html",
    "title": "13  Functional principal components analysis",
    "section": "",
    "text": "13.1 Preliminaries\nLoad libraries we will need:\nlibrary(tidyverse)\nlibrary(fda)\nlibrary(fdapace)\nlibrary(MFPCA)\nlibrary(mgcv)\n\nselect &lt;- dplyr::select\n\n# Install this package if need be\n# devtools::install_github('uasolo/landmarkregUtils')\nlibrary(landmarkregUtils)\nWe use several new packages for FPCA and multivariate FPCA:",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Functional principal components analysis</span>"
    ]
  },
  {
    "objectID": "FDA_edited.html#preliminaries",
    "href": "FDA_edited.html#preliminaries",
    "title": "13  Functional principal components analysis",
    "section": "",
    "text": "fda (Ramsay 2024)\nfdapace (Zhou et al. 2024)\nMFPCA (Happ and Greven 2018; Happ-Kurz 2022)\n\n\n13.1.1 Data\n\n13.1.1.1 Danish /t/ releases\nWe’ll be working with two publicly available datasets, replicating parts of the analysis from the associated papers (with some extensions). The first is from Puggaard-Rode (2023): this is a study of the release of the voiceless coronal stop /t/ in Danish, which is variably aspirated or affricated. Each datapoint is the spectrum a /t/ release, where amplitude (in decibels) is a function of frequency (in Hz) measured at 37 frequency bands.\nLet’s load the data and do some pre-processing. (The data can be found here, under the heading for Ch. 6. Much of the code in this section is taken with minimal modifications from the Quarto notebook associated with the paper.):\n\n# Read in the data\naffric &lt;- read_delim('fda_data/affric.csv', delim = ';', locale = readr::locale(encoding = 'ISO-8859-1'))\nparticipant_info &lt;- read_csv('fda_data/participant_info.csv', locale = readr::locale(encoding = 'ISO-8859-1'))\n\ndanish_stops &lt;- left_join(affric, participant_info)\n\n# Convert character vectors to factors\nfor (col in c(\n  'height', 'round', 'pal', 'stress', 'gender', 'parish', 'backness')){\n  danish_stops[[col]] &lt;- as.factor(danish_stops[[col]])\n}\n\n# Collapse backness distinction to two levels\ndanish_stops &lt;- danish_stops %&gt;%\n  mutate(bkn2l = case_when(\n    backness == 'front' | backness == 'central' ~ 'nbk',\n    TRUE ~ 'bk')) %&gt;%\n  mutate(bkn2l = as.factor(bkn2l))\n\n# Read in the multitaper spectra\nspectra &lt;- read_csv('fda_data/yind_log.csv')\n\n# Filter to only /t/\ndanish_t &lt;- danish_stops %&gt;%\n  filter(stop == 't') %&gt;%\n  mutate(new_id = seq.int(nrow(filter(danish_stops, stop == 't'))))\n\nHere, we simply restrict the domain of the function to the range slightly to help avoid spurious effects: intrusive voicing from a previous or following segment can appear below 500 Hz, and the effects of aspiration/affrication above 8000 Hz are not likely to be of interest.\n\n# Filter the spectra dataframe to only /t/\nt_spectra &lt;- danish_t %&gt;%\n  inner_join(spectra) %&gt;%\n  filter(.index &gt; 500 & .index &lt; 8000) %&gt;% # Filter to a sensible frequency range\n  select(-c(.obs, ...1)) %&gt;%\n  rename(.obs = new_id)\n\n\n\n13.1.1.2 New Zealand English near–square merger\nThe second dataset comes from Gubian et al. (2019), a study of a change in progress in New Zealand English which is causing the diphthongs /ɪə/ (lexical set near) and /eə/ (lexical set square) to merge. Here, each observation is a pair of formant tracks (F1 and F2), measured at 11 evenly spaced points: formant frequency (in Lobanov 1971 normalized units) is a function of time (in units of relative duration).\nThe data can be found in this GitHub repository, which is also the source of much of the code used here.\n\n# Load the data\nnze &lt;- read_csv('fda_data/NZE.csv')\n\n# Convert character vectors to factors\nnze &lt;- nze %&gt;% \n  mutate(across(c(Diphthong, Region, Age, Sex, Speaker, Word, Phrase, Age2), ~as.factor(.x)))\n\nnze$Age &lt;- factor(nze$Age, levels = c('Young', 'MidAge', 'Old'))\nnze$Age2 &lt;- factor(nze$Age2, levels = c('Younger', 'older'))\n\n# Add a column for the measurement point\nnze$time &lt;- rep(seq(0, 1, length.out = 11), nrow(nze)/11)\n\n# Add a column for token duration\nnze &lt;- nze %&gt;% \n  mutate(duration = max(time_ms), .by = tokenId)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Functional principal components analysis</span>"
    ]
  },
  {
    "objectID": "FDA_edited.html#basics-of-fpca",
    "href": "FDA_edited.html#basics-of-fpca",
    "title": "13  Functional principal components analysis",
    "section": "13.2 Basics of FPCA",
    "text": "13.2 Basics of FPCA\n\n13.2.1 Functional data in linguistics\nSpeech production and perception and both fundamentally time-dynamic processes. Think, for example, of how formants or pitch change across vowels or words. This dynamic information is an important cue to all sorts of contrasts. There are also other kinds of functions in linguistic data: we’ve already seen the example of spectra, which are functions in the frequency domain.\n\nExercise 13.1 What are some other examples of functional linguistic data that come to mind?\n\nMost linguistic analyses rely solely on point measures (e.g., formants at vowel midpoint), which may or may not be good representations of the trajectories. In some cases, there may even be considerable disagreement about what a reasonable point measure would be (this is notably the case for fricative spectra, for example). Functional data can be challenging to work with directly, however, since they are inherently high-dimensional. This forms part of the motivation for functional principal components analysis (FPCA), which allows arbitrary functions to be represented with only a few parameters.\n\n\n13.2.2 Functional principal components analysis (FPCA)\nPrincipal components analysis is used to project a set of points (usually in a high-dimensional space) into a new coordinate space, such that:\n\nData are centered on the mean of each dimension,\nThe first dimension (principal component 1, or PC1) is the single dimension which accounts for the largest possible proportion of variance in the data. All dimensions are ranked in decreasing order of proportion of variance explained.\nAll dimensions are orthogonal to one another.\n\nIt is often described as (moving the origin and) ‘rotating the axes’. A somewhat less intuitive way of thinking about it (which may, however, make the analogy with FPCA clearer) is this: each point in the original space is being decomposed into a weighted sum of the vectors describing the mean and each of the principal components. FPCA similarly allows a set of functions to be represented as a linear combination of the mean and of a series of orthogonal functions, each capturing a diminishing proportion of variance in the data.\nBoth PCA and FPCA allow for several different kinds of research questions to be investigated. One common use case is for subsequent regression analysis, where PC scores can be used in place of or in addition to other measures. For example, we can fit a linear regression model to examine which variables predict PC scores along a given dimension, or we can fit a logistic regression model to investigate how well PC scores predict a certain contrast of interest. Alternatively, the dimensions of variation (the principal components themselves) and their relative importance may be of interest.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Functional principal components analysis</span>"
    ]
  },
  {
    "objectID": "FDA_edited.html#affrication-of-t-releases-in-danish",
    "href": "FDA_edited.html#affrication-of-t-releases-in-danish",
    "title": "13  Functional principal components analysis",
    "section": "14.1 Affrication of /t/ releases in Danish",
    "text": "14.1 Affrication of /t/ releases in Danish\nDanish /t/ is variably aspirated (realized as [tʰ]) or affricated (realized as [t^s]). Per Puggaard-Rode (2023),\n\nThe acoustics of alveolar frication and aspiration are well-understood. In alveolar frication, a jet of air impinges on a hard surface (the upper front teeth) immediately in front of the coronal constriction, resulting in noise in a broad range of high frequencies (mainly above 4 kHz) (Stevens 1993). In aspiration, lower frequency turbulence noise in a narrower frequency range (mainly below 1 kHz) is generated at and near the glottis (Stevens 1998). This noise is filtered through the supraglottal cavity, where the resonance frequencies of the following vowel significantly affect the resulting noise.\n\nLet’s take a look at the dataset:\n\nt_spectra %&gt;% \n  glimpse()\n## Rows: 188,885\n## Columns: 21\n## $ parish    &lt;fct&gt; Agerskov, Agerskov, Agerskov, Agerskov, Agerskov, Agerskov, …\n## $ token     &lt;chr&gt; \"tæ2\", \"tæ2\", \"tæ2\", \"tæ2\", \"tæ2\", \"tæ2\", \"tæ2\", \"tæ2\", \"tæ2…\n## $ cog       &lt;dbl&gt; 4305, 4305, 4305, 4305, 4305, 4305, 4305, 4305, 4305, 4305, …\n## $ vot       &lt;dbl&gt; 63.3, 63.3, 63.3, 63.3, 63.3, 63.3, 63.3, 63.3, 63.3, 63.3, …\n## $ pal       &lt;fct&gt; negpal, negpal, negpal, negpal, negpal, negpal, negpal, negp…\n## $ height    &lt;fct&gt; mid, mid, mid, mid, mid, mid, mid, mid, mid, mid, mid, mid, …\n## $ backness  &lt;fct&gt; front, front, front, front, front, front, front, front, fron…\n## $ round     &lt;fct&gt; negrd, negrd, negrd, negrd, negrd, negrd, negrd, negrd, negr…\n## $ stop      &lt;chr&gt; \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", …\n## $ stress    &lt;fct&gt; str, str, str, str, str, str, str, str, str, str, str, str, …\n## $ parishno  &lt;dbl&gt; 3074, 3074, 3074, 3074, 3074, 3074, 3074, 3074, 3074, 3074, …\n## $ tape_id   &lt;dbl&gt; 395, 395, 395, 395, 395, 395, 395, 395, 395, 395, 395, 395, …\n## $ birthyear &lt;dbl&gt; 1883, 1883, 1883, 1883, 1883, 1883, 1883, 1883, 1883, 1883, …\n## $ gender    &lt;fct&gt; male, male, male, male, male, male, male, male, male, male, …\n## $ year      &lt;dbl&gt; 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, …\n## $ dialect   &lt;chr&gt; \"Søn\", \"Søn\", \"Søn\", \"Søn\", \"Søn\", \"Søn\", \"Søn\", \"Søn\", \"Søn…\n## $ rec_age   &lt;dbl&gt; 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, …\n## $ bkn2l     &lt;fct&gt; nbk, nbk, nbk, nbk, nbk, nbk, nbk, nbk, nbk, nbk, nbk, nbk, …\n## $ .obs      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ .index    &lt;dbl&gt; 601.6667, 802.5000, 1003.3333, 1204.1667, 1405.0000, 1605.83…\n## $ .value    &lt;dbl&gt; -0.60640837, -0.55430433, -0.48678129, -0.21229655, 0.273846…\n\nThe main variables of interest here are:\n\n.obs: A unique identifier for each spectrum,\n.index: The center frequency of the spectral bin associated with a given measurement,\n.value: The amplitude measure at a given spectral bin,\nvot: The voice onset time (in ms) of the stop\ntoken: A phonetic transcription for the syllable that the token comes from, used to determine:\n\n\n\npal: Whether the stop is palatalized or not.\nheight: The height of the following vowel (high, mid, or low)\nbkn2l: The backness of the following vowel (back or non-back)\nround: The roundness of the following vowel (rounded or unrounded)\nstress: Whether the token comes from a stressed or unstressed syllable\n\n\n\nparish: The place of origin of the speaker who produced the token. There’s only a single speaker from each parish, so this also uniquely identifies each speaker.\n\nLet’s see what the average spectrum looks like:\n\nt_spectra %&gt;% \n  ggplot(aes(x = .index, y = .value)) +\n  geom_smooth()\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nExercise 14.1 Make some exploratory plots to examine how different variables affect the shape of the spectrum. Why do you think those variables have an effect?\n\n\n14.1.1 Fitting the FPCA and unpacking the output\nWe’ll be using the function FPCA() from the fdapace to fit the FPCA. This function takes two lists of vectors as inputs (Ly and Lt, containing the y and x values, respectively), where each vector corresponds to one functional observation. We can use the convenience function MakeFPCAInputs() in fdapace to wrangle our long form dataframe of spectra into this format.\nOptions for the FPCA can be passed as a named list to the optns parameter of FPCA(). Setting methodMuCovEst = 'smooth' will enable smoothing of the functions before fitting (which is often useful in limiting spurious effects due to noise in the data): methodBwCov = 'GCV' will use generalized cross-validation to determine the correct degree of smoothness. Finally, the FVEthreshold (FVE = fraction of variance explained), indeirectly determines the number of FPCs: enough dimensions to account for at least that percentage of the variation in the data will be computed (default is 0.99, but we’ll use 0.95 here.)\nFor more options, see the fdapace vignette or the function’s documentation, by running the command ?FPCA().\n\n# Make the FPCA inputs\nt_input &lt;- with(t_spectra,\n                MakeFPCAInputs(IDs = .obs, # A unique ID for each observation\n                               tVec = .index, # The x value of the measure (in this case, the frequency bin)\n                               yVec = .value) # The y value of the measure (in this case, the amplitude value)\n  )\n\n# Run the FPCA\nfpca_t &lt;- with(t_input,\n               FPCA(Ly, Lt,\n                    optns = list(dataType = 'Dense',\n                                 methodMuCovEst = 'smooth', # Smooth the data (not the default for dense data)\n                                 methodBwCov = 'GCV', # Estimate the degree of smoothing required via generalized cross-validation\n                                 FVEthreshold = 0.95)) # Find a solution with enough principal components to explain 95% of the variance\n)\n\nfpca_t %&gt;% plot()\n\n\n\n\n\n\n\n\nRunning the plot function on the FPCA object produces some useful summary metrics:\nDesign plot: This shows how individual measurements are spaced across the x axis (called ‘time grid’ in the plot, but actually the frequency domain in our example). It serves as a sanity check that the inputted data look the way they’re supposed to: here, we have many observations along the frequency dimension (37 bins) and they’re all equally spaced, so our data is said to be dense. FPCA also works with sparse data, but fitting can be more complex or prone to problems.\nMean function: As the name suggests, this is the average of all (smoothed) observations. (You can check for yourself that this indeed looks like the mean spectrum we plotted earlier with ggplot2.) This is stored in the FPCA object as $mu.\nScree plot: This will be familiar to you if you’ve used plain PCA before. The bars on this plot give the FVE by each given PC, and the connected red dots give the running sum of the FVE up to a given PC. The total FVE and the list of cumulative FVE values are stored in the FPCA object as $FVE and $cumFVE, respectively. If we want to calculate the FVE associated with a particular PC, we can use this function:\n\n# A convenience function for getting the FVE for a given set of principal components (by default, for all FPCs)\nFVE &lt;- function(fpca_obj, fpcs = 1:fpca_obj$selectK) {\n  return(c(fpca_obj$cumFVE[1], diff(fpca_obj$cumFVE))[fpcs])\n}\n\nfpca_t %&gt;% FVE()\n## [1] 0.58418897 0.18240035 0.09251821 0.05992529 0.03934014\n\nWe see that the majority of the variance is capture by a single dimension (FPC1). FVE rapidly drops off FPC2, to the point that FPC4 and 5 aren’t contributing much. When several FPCs have similarly low FVE like this, it’s often a sign that they’re picking up on noise rather than signal.\nA plot of the first three eigenfunctions: This is the meat and potatoes of the FPCA, showing the shape of the three functions which have been determined to explain the largest proportion of variance and which are used to calculate principal component scores. It can be hard to interpret the eigenfunctions just by looking at them: it’s thus often useful to visualize their modifying effect on the mean function.\nTo do this, we simply multiply the eigenfunction by some arbitrary value and add it to the mean. To get a good sense of the range of variability found in our data, we can use quantiles of PC scores along that dimension calculated from the data: for example, if we choose the first and third quartiles, it would mean that half of the observations vary along that PC within the range described by those two curves. The following convenience function makes it easy to do just that, allowing the user to specify which FPCs are of interest and which quantiles to use.\n\n# A convenience function for visualizing the effects of FPCs, adapted from Puggaard-Rode (2023)\nplot_fpc_effects &lt;- function(fpca_obj, fpcs = 1:fpca_obj$selectK, range = c(0, 1)) {\n  vals &lt;- data.frame()\n  \n  for (fpc in fpcs) {\n    vals &lt;- vals %&gt;% \n      rbind(data.frame(fpc = paste0('FPC', fpc),\n                       measurement = fpca_obj$obsGrid, mu = fpca_obj$mu,\n                       highq = fpca_obj$mu + quantile(fpca_obj$xiEst[, fpc], range[2]) * fpca_obj$phi[, fpc],\n                       lowq = fpca_obj$mu + quantile(fpca_obj$xiEst[, fpc], range[1]) * fpca_obj$phi[, fpc]))\n  }\n \n  vals &lt;- vals %&gt;% \n    mutate(fpc = fpc %&gt;% as.factor()) %&gt;% \n    pivot_longer(cols = c(mu, highq, lowq))\n\n  plt &lt;- vals %&gt;% \n    ggplot(aes(x = measurement, y = value, colour = name)) +\n    geom_line(linewidth = 1) +\n    facet_wrap(~fpc, ncol = 3) +\n    xlab('Measurement') +\n    ylab('Peak (ERB)') + \n    scale_colour_manual(values = c('red', 'blue', 'gray'),\n                       name = 'Score',\n                       labels = c('Low', 'High', 'Mean')) +\n    theme(legend.position = 'bottom')\n    \n  return(plt)\n}\n\nfpca_t %&gt;% \n  plot_fpc_effects(fpcs = 1:3, range = c(0.25, 0.75))\n\n\n\n\n\n\n\n\nHere, the first quantile (low value) is plotted in red, the third quantile (high value) in blue, and the mean in grey. Note that the sign of any given dimension is arbitrary, and can be flipped without affecting the fit (this may sometimes facilitate interpretation).\nFrom these plots, we see that FPC1 primarily affects the spectral peak: peak frequency varies monotonically (low values = lower frequency, high values = higher frequency), but peak amplitude does not (amplitude is highest at low values, drops towards the mean and increases again at high values). FPC2 simultaneously affects peak amplitude and peak prominence (low = higher amplitude and more prominent peak, high = lower amplitude peak and more of a plateau). Conversely, FPC3 captures joint change in peak frequency and peak prominence (low = higher frequency, less prominent peak, high = lower frequency, more prominent peak).\n\nExercise 14.2 Try running the FPCA again, but this time without smoothing the functions (set methodMuCovEst = 'cross-sectional' in the optns() parameter). How does this change the results?\n\n\n\n14.1.2 Extracting and interpreting FPC scores\nFPC scores are contained in a matrix, stored in the FPCA object as xiEst. We can make pulling them out and adding them back to our original dataframe a little easier with the following function:\n\n# A convenience function to pull the FPC scores out of the FPCA object and store them in the original dataframe\nextract_fpc_scores &lt;- function(df, fpca_obj, fpcs = 1:fpca_obj$selectK) {\n  for (fpc in fpcs) {\n    df &lt;- df %&gt;% \n      mutate('fpc{fpc}' := fpca_obj$xiEst[,fpc]) # Creates a column `fpcx`, where x = fpc, for the scores\n  }\n  \n    return(df)\n}\n\ndanish_t &lt;- danish_t %&gt;%\n  extract_fpc_scores(fpca_t)\n\n\nExercise 14.3 Building off what was learned previously, make some exploratory plots to determine which variables are predictive of FPC1, FPC2 and FPC3 scores.\n\n\nExercise 14.4 Try and fit a simple, reasonable linear model for FPC1, FPC2, and FPC3 scores, using the predictors which seemed important in the previous question. (Separate univariate models are fine for our purposes, although a single multivariate model would in principle be better.)\n\n\n\n14.1.3 Reconstructing functions from FPC scores\nLet’s now try to reconstruct the original spectra using the mean function and the first three principal components.\n\n# Join in the extracted FPC scores\nt_spectra &lt;- t_spectra %&gt;% \n  left_join(danish_t %&gt;% select(new_id:fpc5), by = c('.obs' = 'new_id'))\n\n# Create a dataframe with the mean and the principal components\nfpcs &lt;- with(fpca_t,\n             data.frame(.index = obsGrid,\n                        mu = mu,\n                        FPC1 = phi[, 1],\n                        FPC2 = phi[, 2],\n                        FPC3 = phi[, 3])) %&gt;% \n  mutate(index = floor(.index)) %&gt;% # Gets around a floating point precision issue\n  select(-.index)\n\nt_spectra &lt;- t_spectra %&gt;% \n  mutate(index = floor(.index)) %&gt;% # Gets around a floating point precision issue\n  left_join(fpcs, by = 'index')\n\n# Generate the reconstructed measures\nt_spectra &lt;- t_spectra %&gt;% \n  mutate(reconstructed = mu + fpc1 * FPC1 + fpc2 * FPC2 + fpc3 * FPC3)\n\n# Take a small subsample of curves for visualization\nset.seed(76)\n\ntrajs &lt;- t_spectra$.obs %&gt;% sample(9) \nt_spectra_samples &lt;- t_spectra %&gt;% \n  filter(.obs %in% trajs)\n\n# Plot the observed values vs reconstructed curves\nt_spectra_samples %&gt;% \n  ggplot(aes(x = index)) +\n  geom_point(aes(y = .value), colour = 'red', shape = 1) +\n  geom_line(aes(y = reconstructed), colour = 'black') +\n  facet_wrap(~.obs) +\n  theme(strip.text = element_blank())\n\n\n\n\n\n\n\n\nThe measured frequency values are represented by the red dots, whereas the reconstructed spectra are represented by the solid black line. As expected, the first three PCs offer a fair, but not perfect, approximation of the original data.\n\n\n14.1.4 Introducing controls\nThe results of the previous FPCA tell us what the most important dimensions of variation are across the entire dataset. However, we’ve used heavily imbalanced corpus data, which may skew the results. We may, moreover, be particularly interested in certain kinds of variability—for example, inter-speaker variability, rather than variability due to the phonological environment.\nIn these cases, we can fit a model to control for certain variables, make marginal predictions for the given variable of interest, and run the FPCA on those predictions rather than on the empirical data. Here’s a reasonable GAMM for the data, controlling for gender, the VOT of the stop, and the following vowel:\n\n# Some more data wrangling\nt_spectra &lt;- t_spectra %&gt;% \n  mutate(start.event = ((row_number() - 1 %% 37) == 0),\n         token = token %&gt;% as.factor(),\n         gender.ord = gender %&gt;% as.ordered(),\n         vot.std = vot %&gt;% arm::rescale())\n\ncontrasts(t_spectra$gender.ord) &lt;- contr.treatment(n = 2)\n\n# Fit a first model without an AR1 error model\ndanish_mod1 &lt;- t_spectra %&gt;% \n  bam(.value ~ gender.ord +\n        s(.index, bs = 'cr') + s(.index, by = gender.ord, bs = 'cr') +\n        s(vot.std, bs = 'cr') + ti(.index, vot.std, bs = 'cr') +\n        s(.index, parish, bs = 'fs', xt = 'cr', k = 5, m = 1) +\n        s(.index, token, bs = 'fs', xt = 'cr', k = 5, m = 1),\n      data = ., method = 'fREML', discrete = TRUE)\n\ndanish_mod2 &lt;- t_spectra %&gt;% \n  bam(.value ~ gender.ord +\n        s(.index, bs = 'cr') + s(.index, by = gender.ord, bs = 'cr') +\n        s(vot.std, bs = 'cr') + ti(.index, vot.std, bs = 'cr') +\n        s(.index, parish, bs = 'fs', xt = 'cr', k = 5, m = 1) +\n        s(.index, token, bs = 'fs', xt = 'cr', k = 5, m = 1),\n      data = ., method = 'fREML', discrete = TRUE,\n      rho = itsadug::start_value_rho(danish_mod1), AR.start = .$start.event)\n\n\ndanish_mod2 %&gt;% \n  write_rds(file = 'fda_data/danish_mod2.rds')\n\nThe model has been fitted in advance in order to save time, so let’s load it now.\n\ndanish_mod2 &lt;- read_rds('fda_data/danish_mod2.rds')\n\ndanish_mod2 %&gt;% summary()\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## .value ~ gender.ord + s(.index, bs = \"cr\") + s(.index, by = gender.ord, \n##     bs = \"cr\") + s(vot.std, bs = \"cr\") + ti(.index, vot.std, \n##     bs = \"cr\") + s(.index, parish, bs = \"fs\", xt = \"cr\", k = 5, \n##     m = 1) + s(.index, token, bs = \"fs\", xt = \"cr\", k = 5, m = 1)\n## \n## Parametric coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.256054   0.011237  22.787   &lt;2e-16 ***\n## gender.ord.L 0.009974   0.010799   0.924    0.356    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                              edf   Ref.df      F  p-value    \n## s(.index)                  8.774    8.896 35.109  &lt; 2e-16 ***\n## s(.index):gender.ordmale   7.758    8.433  8.541  &lt; 2e-16 ***\n## s(vot.std)                 7.467    8.401  3.826 7.94e-05 ***\n## ti(.index,vot.std)        12.217   13.601 24.379  &lt; 2e-16 ***\n## s(.index,parish)         940.227 1062.000 42.661  &lt; 2e-16 ***\n## s(.index,token)          167.855  249.000 12.770  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =   0.49   Deviance explained = 49.3%\n## fREML = -1.1695e+05  Scale est. = 0.079677  n = 188885\n\nWe then use the mgcv::predict.bam() function to make by-speaker predictions for the smooth over frequency, marginalizing over all other predictors (including gender.ord), which are then stored in the dataframe parish_df. These will allow us to examine the dimensions of residual inter-speaker variability (i.e., that which is not explained by gender). (You don’t need to worry too much about the details of the code here. This probably isn’t the cleanest way to make these predictions.)\n\n# Create a dataframe for predictions\nparish_df &lt;- t_spectra %&gt;% \n  distinct(parish, .index) %&gt;% \n  mutate(vot.std = 0)\n\n# Make predictions\nparish_df$gender.ord &lt;- 'male'\n\nparish_pred_male &lt;- danish_mod2 %&gt;% \n  predict.bam(parish_df,\n              terms = c('(Intercept)',\n                        'gender.ord',\n                        's(.index)',\n                        's(.index):gender.ordmale',\n                        's(vot.std)',\n                        's(.index,parish)'),\n              newdata.guaranteed = TRUE,\n              discrete = FALSE)\n\nparish_df$gender.ord &lt;- 'female'\n\nparish_pred_female &lt;- danish_mod2 %&gt;% \n  predict.bam(parish_df,\n              terms = c('(Intercept)',\n                        'gender.ord',\n                        's(.index)',\n                        's(.index):gender.ordmale',\n                        's(vot.std)',\n                        's(.index,parish)'),\n              newdata.guaranteed = TRUE,\n              discrete = FALSE)\n\nparish_df$pred &lt;- (parish_pred_male + parish_pred_female)/2\n\nparish_df &lt;- parish_df %&gt;% \n  select(-gender.ord)\n\nWe’re now ready to run the FPCA on the by-speaker predictions in parish_df.\n\nExercise 14.5 Using the previous example as a template, carry out FPCA on the by-speaker predictions. (Note that there’s no need to smooth the functions this time, since the GAMM has already done this for us. Extract the FVE for each of the first three principal components, and create a plot to visualize the effects of the eigenfunctions on the mean. How do the results compare to those from the original FPCA?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Functional principal components analysis</span>"
    ]
  },
  {
    "objectID": "FDA_edited.html#nearsquare-merger-in-new-zealand-english",
    "href": "FDA_edited.html#nearsquare-merger-in-new-zealand-english",
    "title": "13  Functional principal components analysis",
    "section": "14.2 near–square merger in New Zealand English",
    "text": "14.2 near–square merger in New Zealand English\nLet’s take a look at the dataset:\n\nnze %&gt;% \n  glimpse()\n## Rows: 46,442\n## Columns: 14\n## $ tokenId   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n## $ Diphthong &lt;fct&gt; I@, I@, I@, I@, I@, I@, I@, I@, I@, I@, I@, e@, e@, e@, e@, …\n## $ time_ms   &lt;dbl&gt; 0.00000, 10.00000, 20.00000, 30.00000, 40.00000, 50.00000, 6…\n## $ Region    &lt;fct&gt; Hamilton, Hamilton, Hamilton, Hamilton, Hamilton, Hamilton, …\n## $ Age       &lt;fct&gt; MidAge, MidAge, MidAge, MidAge, MidAge, MidAge, MidAge, MidA…\n## $ Sex       &lt;fct&gt; female, female, female, female, female, female, female, fema…\n## $ Speaker   &lt;fct&gt; h007, h007, h007, h007, h007, h007, h007, h007, h007, h007, …\n## $ Word      &lt;fct&gt; years, years, years, years, years, years, years, years, year…\n## $ F1        &lt;dbl&gt; -0.80419952, -0.80092751, -0.79765550, -0.78238614, -0.74094…\n## $ F2        &lt;dbl&gt; 0.9653225, 0.9622437, 0.9591649, 0.9043064, 0.8886326, 0.780…\n## $ Phrase    &lt;fct&gt; nonfinal, nonfinal, nonfinal, nonfinal, nonfinal, nonfinal, …\n## $ Age2      &lt;fct&gt; Younger, Younger, Younger, Younger, Younger, Younger, Younge…\n## $ time      &lt;dbl&gt; 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 0.0, …\n## $ duration  &lt;dbl&gt; 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, …\n\nThe most important variables here are:\n\ntokenId: A unique identifier for each token,\ntime: The timepoint (in relative duration) that the measurement comes from,\nF1, F2: The formant values,\nDiphthong: The underlying vowel that the token comes from (I@ = [ɪə], e@ = [eə]),\nAge2: The age group the spaker belongs to (old or young).\nPhrase: Whether the token is phrase-final or not (relevant because sound changes are often more advanced phrase-finally than non-finally).\n\nLet’s look at a sample of 25 speakers and see how they vary in terms of the realization of the near–square contrast:\n\nset.seed(26)\n\nspeakers &lt;- unique(nze$Speaker) %&gt;% \n  sample(25)\n\nnze_sub &lt;- nze %&gt;% \n  filter(Speaker %in% speakers) %&gt;% \n  mutate(Diphthong = Diphthong %&gt;% as.factor())\n\nnze_sub_long &lt;- nze_sub %&gt;% \n  pivot_longer(cols = c(F1, F2),\n               names_to = 'formant',\n               values_to = 'formant_freq')\n\nnze_sub_long %&gt;% \n  ggplot(aes(x = time, y = formant_freq, colour = Diphthong, group = interaction(Diphthong, formant))) +\n  geom_smooth() +\n  facet_wrap(~Speaker, ncol = 5) +\n  theme(strip.text = element_blank(),\n        legend.position = 'bottom')\n## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nWe immediate see that there is variation in terms of whether a given speaker:\n\nHas maintained the contrast or not—and if so, over what proportion of the time interval the contrast can be seen,\nMarks the contrast with just F1, just F2, or both.\n\n\n14.2.1 Multivariate FPCA\nWe thus need a dynamic metric that accounts for the patterns of covariation in F1 and F2. Multidimensional FPCA (MFPCA) does just that, simultaneously determining the functions which capture the most variance for each dimension. Although there’s a different set of PCs for each dimension, there is a single set of PC scores which affect both dimensions jointly.\nWe’ll be using the package MFPCA and its MFPCA() function here. It works somewhat differently from fdapace::FPCA(), notably requiring the input functions to be in a different format. We’ll use the long2irregFunData() function from Michele Gubian’s package landmarkregUtils, as well as the function funData::as.funData(), to covert our long dataframe to the correct format. We need to create two separate funData objects, which we will then combine into a single multiFunData object.\n\n# Create the inputs\nnze_f1 &lt;- long2irregFunData(nze,\n                                id = 'tokenId',\n                                time = 'time',\n                                value = 'F1') %&gt;% \n  as.funData()\n\nnze_f2 &lt;- long2irregFunData(nze,\n                                id = 'tokenId',\n                                time = 'time',\n                                value = 'F2') %&gt;% \n  as.funData()\n\nnze_inputs &lt;- multiFunData(list(nze_f1, nze_f2))\n\nThe options for MFPCA() are also specified in a slightly different way. Instead of passing a desired FVE threshhold, we supply the desired number of PCs.\n\n# Run the MFPCA\nnze_mfpca &lt;- MFPCA(nze_inputs,\n                   M = 3,\n                   uniExpansions = list(list(type = 'uFPCA'), list(type = 'uFPCA')))\n\n# Get summary plots\nnze_mfpca %&gt;% \n  plot(combine = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescribe the effects of each of the principal components.\n\n\nExercise 14.6 Try and fit two univariate FPCAs on the nze data, one for F1 and one for F2. How do they differ from the compare to the MFPCA?\n\n\n\n14.2.2 Extracting FPC scores\nHere’s a convience function for getting the FPC scores out of the model, very similar to the one for univariate FPCA:\n\n# Create a dataframe with one row per observation\nnze_tokens &lt;- nze %&gt;% \n  distinct(tokenId, Diphthong, Region, Age, Sex, Speaker, Word, Phrase, Age2, duration)\n\n# A convenience function similar to `extract_fpc_scores` above\nextract_mfpc_scores &lt;- function(df, fpca_obj, fpcs = 1:(fpca_obj$normFactors %&gt;% length())) {\n  for (fpc in fpcs) {\n    df &lt;- df %&gt;% \n      mutate('fpc{fpc}' := fpca_obj$scores[,fpc]) # Creates a column `fpcx`, where x = fpc, for the scores\n  }\n  \n    return(df)\n}\n\n# Extract scores\nnze_tokens &lt;- nze_tokens %&gt;% \n  extract_mfpc_scores(nze_mfpca)\n\n\nExercise 14.7 Which FPCs contribute the most to the near–square distinction in NZE? Do we see evidence of a merger in progress?\n\n\n\n\n\nGubian, Michele, Jonathan Harrington, Mary Stevens, Florian Schiel, and Paul Warren. 2019. “Tracking the New Zealand English NEAR/SQUARE Merger Using Functional Principal Components Analysis.” In Proceedings of Interspeech 2019, 296–300. https://doi.org/10.21437/Interspeech.2019-2115.\n\n\nHapp, Clara, and Sonja Greven. 2018. “Multivariate Functional Principal Component Analysis for Data Observed on Different (Dimensional) Domains.” Journal of the American Statistical Association 113: 649–59. https://doi.org/10.1080/01621459.2016.1273115.\n\n\nHapp-Kurz, Clara. 2022. MFPCA: Multivariate Functional Principal Component Analysis for Data Observed on Different Dimensional Domains. https://github.com/ClaraHapp/MFPCA.\n\n\nLobanov, Boris. 1971. “Classification of Russian Vowels Spoken by Different Speakers.” Journal of the Acoustical Society of America 49 (2B): 606–8. https://pubs.aip.org/asa/jasa/article-abstract/49/2B/606/747097/Classification-of-Russian-Vowels-Spoken-by.\n\n\nPuggaard-Rode, Rasmus. 2023. “The /t/ Release in Jutland Danish. Decomposing the Spectrum with Functional PCA.” In Proceedings of the 20th International Congress of Phonetic Sciences, 3262–66. Prague: Guarant International. https://rpuggaardrode.github.io/icphs2023/.\n\n\nRamsay, James. 2024. Fda: Functional Data Analysis. https://CRAN.R-project.org/package=fda.\n\n\nZhou, Yidong, Han Chen, Su I Iao, Poorbita Kundu, Hang Zhou, Satarupa Bhattacharjee, Cody Carroll, et al. 2024. fdapace: Functional Data Analysis and Empirical Dynamics. https://CRAN.R-project.org/package=fdapace.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Functional principal components analysis</span>"
    ]
  },
  {
    "objectID": "FDA_edited.html#references",
    "href": "FDA_edited.html#references",
    "title": "13  Functional principal components analysis",
    "section": "14.3 References",
    "text": "14.3 References\n\n\n\n\nGubian, Michele, Jonathan Harrington, Mary Stevens, Florian Schiel, and Paul Warren. 2019. “Tracking the New Zealand English NEAR/SQUARE Merger Using Functional Principal Components Analysis.” In Proceedings of Interspeech 2019, 296–300. https://doi.org/10.21437/Interspeech.2019-2115.\n\n\nHapp, Clara, and Sonja Greven. 2018. “Multivariate Functional Principal Component Analysis for Data Observed on Different (Dimensional) Domains.” Journal of the American Statistical Association 113: 649–59. https://doi.org/10.1080/01621459.2016.1273115.\n\n\nHapp-Kurz, Clara. 2022. MFPCA: Multivariate Functional Principal Component Analysis for Data Observed on Different Dimensional Domains. https://github.com/ClaraHapp/MFPCA.\n\n\nLobanov, Boris. 1971. “Classification of Russian Vowels Spoken by Different Speakers.” Journal of the Acoustical Society of America 49 (2B): 606–8. https://pubs.aip.org/asa/jasa/article-abstract/49/2B/606/747097/Classification-of-Russian-Vowels-Spoken-by.\n\n\nPuggaard-Rode, Rasmus. 2023. “The /t/ Release in Jutland Danish. Decomposing the Spectrum with Functional PCA.” In Proceedings of the 20th International Congress of Phonetic Sciences, 3262–66. Prague: Guarant International. https://rpuggaardrode.github.io/icphs2023/.\n\n\nRamsay, James. 2024. Fda: Functional Data Analysis. https://CRAN.R-project.org/package=fda.\n\n\nZhou, Yidong, Han Chen, Su I Iao, Poorbita Kundu, Hang Zhou, Satarupa Bhattacharjee, Cody Carroll, et al. 2024. fdapace: Functional Data Analysis and Empirical Dynamics. https://CRAN.R-project.org/package=fdapace.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Functional principal components analysis</span>"
    ]
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "10  Generalized Additive Models (GAMs)",
    "section": "",
    "text": "10.1 Preliminaries\nWe load a couple of libraries: mgcv is the main library for fitting GAMs; itsadug is mainly for plotting (and has a number of other convenience functions). The tidyverse is for everything else!\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(itsadug)\nNew packages for us are:\nData import:\ntemp &lt;- read.delim(\"https://osf.io/download/7ut2r/\")\nprice_bin &lt;- readRDS(file = url(\"https://osf.io/download/vs6jk/\"))\n\n## you can also download the data files from Piazza, and\n## load them in as follows from the current directory:\n# temp &lt;- read_tsv(\"global_temperature.txt\")\n# price_bin &lt;- readRDS(\"price_bin.rds\")\nSubset of older speakers from price_bin data, which we’ll need below:\nprice_bin_older &lt;- price_bin %&gt;%\n  filter(age == \"older\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generalized Additive Models (GAMs)</span>"
    ]
  },
  {
    "objectID": "week6.html#subset-older-speakers-only",
    "href": "week6.html#subset-older-speakers-only",
    "title": "10  LING 683 Fall 2024: Week 6",
    "section": "12.1 Subset: older speakers only",
    "text": "12.1 Subset: older speakers only\nHere’s the GAM we looked at in the slides (slide 23 on), fitted to the subset of the price_bin data from older speakers:\n\nprice_bin_older_gam &lt;-\n  bam(f2 ~ s(measurement_no, bs = \"cr\", k = 11),\n    data = price_bin_older\n  )\n\nHere’s the summary that we went through:\n\nsummary(price_bin_older_gam)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f2 ~ s(measurement_no, bs = \"cr\", k = 11)\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 1538.289      1.338    1149   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                     edf Ref.df     F p-value    \n## s(measurement_no) 8.053   9.22 588.4  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.161   Deviance explained = 16.1%\n## fREML = 1.9393e+05  Scale est. = 50787     n = 28365\n\nAnd here’s a simple plot of the smooth, using the plot_smooth() function from the itsadug package.\n\nplot_smooth(price_bin_older_gam, view = \"measurement_no\")\n## Summary:\n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * NOTE : No random effects in the model to cancel.\n## \n\n\n\n\n\n\n\n\n\nExercise 12.1 I’d like to invite you to explore this smooth.\n\nWhat happens to the smooth if you change the value of k, either increasing or decreasing it? Does the code still work? What does the resulting smooth look like when you plot it using plot_smooth()?\nTry using different types of basis functions. One of them can be accessed using bs=\"tp\" (a thin-plate smooth); another one using bs=\"ps\". Do these change the shape of the smooth?\n\n\nExample code for Exercise 12.1 to un-hide if you need help:\n\n\nCode\na.\nprice_bin_older_gam_k6 &lt;-\n  bam(f2 ~ s(measurement_no, bs = \"cr\", k = 6),\n    data = price_bin_older\n  )\nplot_smooth(price_bin_older_gam_k6, view = \"measurement_no\")\n\nb.\nprice_bin_older_gam_tp6 &lt;-\n  bam(f2 ~ s(measurement_no, bs = \"tp\", k = 6),\n    data = price_bin_older\n  )\nplot_smooth(price_bin_older_gam_tp6, view = \"measurement_no\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LING 683 Fall 2024: Week 6</span>"
    ]
  },
  {
    "objectID": "week6.html#sec-gamms1-full-dataset",
    "href": "week6.html#sec-gamms1-full-dataset",
    "title": "10  LING 683 Fall 2024: Week 6",
    "section": "12.2 Full dataset",
    "text": "12.2 Full dataset\nAnd now we look at the full data set, capturing the difference between the older vs. younger groups using a difference smooth. Here’s the model that we explored in the slides (slide 37 on):\n\nprice_bin$age_o &lt;- as.ordered(price_bin$age)\ncontrasts(price_bin$age_o) &lt;- \"contr.treatment\"\n\nprice_bin_gam &lt;-\n  bam(\n    f2 ~ age_o +\n      s(measurement_no, bs = \"cr\", k = 11) +\n      s(measurement_no, bs = \"cr\", k = 11, by = age_o),\n    data = price_bin\n  )\n\nThe model summary.\n\nsummary(price_bin_gam)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f2 ~ age_o + s(measurement_no, bs = \"cr\", k = 11) + s(measurement_no, \n##     bs = \"cr\", k = 11, by = age_o)\n## \n## Parametric coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  1539.117      1.329 1158.04   &lt;2e-16 ***\n## age_oyounger -119.471      2.096  -56.99   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                                  edf Ref.df      F p-value    \n## s(measurement_no)              8.518  9.457 584.32  &lt;2e-16 ***\n## s(measurement_no):age_oyounger 5.895  7.130  73.17  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.253   Deviance explained = 25.3%\n## fREML = 3.2386e+05  Scale est. = 50069     n = 47419\n\nAnd a plot!\n\nplot_smooth(price_bin_gam, view = \"measurement_no\", plot_all = \"age_o\")\n## Summary:\n##  * age_o : factor; set to the value(s): older, younger. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * NOTE : No random effects in the model to cancel.\n## \n\n\n\n\n\n\n\n\nIt’s also possible to plot the difference smooth alone (using plot_diff() from itsadug).\n\nplot_diff(price_bin_gam,\n  view = \"measurement_no\",\n  comp = list(age_o = c(\"older\", \"younger\"))\n)\n## Summary:\n##  * measurement_no : numeric predictor; with 100 values ranging from 0.000000 to 100.000000. \n##  * NOTE : No random effects in the model to cancel.\n## \n## \n## measurement_no window(s) of significant difference(s):\n##  0.000000 - 100.000000\n\n\n\n\n\n\n\n\n\nExercise 12.2 The data set also contains a variable called following_voiceless, which captures the voicing of the following segment. We expect that this vowel will be realised differently when followed by a voiceless segment; but is that the case?\n\nSet up following_voiceless as an ordered factor with treatment coding (like we did for age_o above).\nFit a model with a difference smooth (again, analogous to the one above).\nPlot the results.\n\n\nSolutions to Exercise 12.2:\n\n\nCode\nprice_bin$foll_v_o &lt;- as.ordered(price_bin$following_voiceless)\ncontrasts(price_bin$foll_v_o) &lt;- \"contr.treatment\"\n\nprice_bin_gam_fv &lt;-\n  bam(\n    f2 ~ foll_v_o + age_o +\n      s(measurement_no, bs = \"cr\", k = 11) +\n      s(measurement_no, bs = \"cr\", k = 11, by = age_o) +\n      s(measurement_no, bs = \"cr\", k = 11, by = foll_v_o),\n    data = price_bin, method = \"ML\"\n  )\n\nsummary(price_bin_gam_fv)\n\nplot_smooth(price_bin_gam_fv, view = \"measurement_no\", plot_all = \"foll_v_o\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LING 683 Fall 2024: Week 6</span>"
    ]
  },
  {
    "objectID": "week6.html#working-with-mgcv-models",
    "href": "week6.html#working-with-mgcv-models",
    "title": "10  LING 683 Fall 2024: Week 6",
    "section": "15.1 Working with mgcv models",
    "text": "15.1 Working with mgcv models\nitsadug is the package most commonly used by (psycho)linguists to work with fitted GA(M)Ms. itsadug has good functionality, but it’s important to not be limited by what any one package can do. Many packages can make predictions and prediction plots from mgcv models (fitted with bam() or gam()), such as ggeffects, modelbased, gratia, or emmeans. Different things you’ll want to do will be easier in different packages.\nAn example using ggeffects (used throughout RMLD: Sonderegger (2023)), which by default makes predictions on the response scale—here, probabilities, rather than log-odds.\n\nlibrary(ggeffects)\n\npreds &lt;- ggpredict(m1, terms = \"year\")\n\nplot(preds) +\n  labs(y = \"Predicted Probability\", x = \"Year\") + ylim(0,1)\n## Scale for y is already present.\n## Adding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\nHow similar does this look to your plot from Exercise 14.1?\nTo exemplify using emmeans with mgcv models, let’s fit a model to data from all three word senses.\n\nFit this model as m2, using the monitor dataframe. (Hint: this is like the price_bin_gam model above.) Don’t worry about autocorrelation here.\nSolution:\n\n\nCode\nmonitor$sense &lt;- as.ordered(monitor$sense)\ncontrasts(monitor$sense) &lt;- \"contr.treatment\"\n\n\nm2 &lt;- bam(cbind(count, n-count) ~ sense + s(year) + s(year, by = sense), data = monitor, family = binomial)\n\n\n\nModel predictions:\n\npreds &lt;- ggpredict(m2, terms = c(\"year\", \"sense\"))\n\nplot(preds) +\n  labs(y = \"Predicted Probability\", x = \"Year\", title = \"\") + ylim(0,1)\n## Scale for y is already present.\n## Adding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\nThis emmeans code finds the pairwise differences between senses, averaging across years:\n\nlibrary(emmeans)\n## Welcome to emmeans.\n## Caution: You lose important information if you filter this package's results.\n## See '? untidy'\n\nemm &lt;- emmeans(m2, ~sense)\n\npairs(emm)\n##  contrast            estimate     SE    df  t.ratio p.value\n##  newspaper - observe    -2.96 0.0168 42636 -176.011  &lt;.0001\n##  newspaper - ship        1.74 0.0277 42636   63.043  &lt;.0001\n##  observe - ship          4.71 0.0273 42636  172.165  &lt;.0001\n## \n## Results are given on the log odds ratio (not the response) scale. \n## P value adjustment: tukey method for comparing a family of 3 estimates\n\n\nExercise 15.2 What are these pairwise differences when year = 2010? (This requires figuring out how to get emmeans to make predictions about one predictor while another is held constant at a given value.)\n\n\n\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.\n\n\nSóskuthy, Márton. 2017. “Generalised Additive Mixed Models for Dynamic Analysis in Linguistics: A Practical Introduction.” arXiv Preprint arXiv:1703.05339.\n\n\n———. 2021. “Evaluating Generalised Additive Mixed Modelling Strategies for Dynamic Speech Analysis.” Journal of Phonetics 84: 101017.\n\n\nvan Rij, Jacolien, Martijn Wieling, R. Harald Baayen, and Hedderik van Rijn. 2022. itsadug: Interpreting Time Series and Autocorrelated Data Using GAMMs.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction with R. Second. Boca Raton, FL: Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>LING 683 Fall 2024: Week 6</span>"
    ]
  },
  {
    "objectID": "week6.html#footnotes",
    "href": "week6.html#footnotes",
    "title": "10  Generalized Additive Models (GAMs)",
    "section": "",
    "text": "mgcv also has extensive support for models beyond the exponential family, such as negative-binomial models.↩︎\nTo do this, you could switch to using a gamm() model with a correlation argument instead of a bam() model. This will fit much more slowly.↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generalized Additive Models (GAMs)</span>"
    ]
  },
  {
    "objectID": "bayesian_gamms.html",
    "href": "bayesian_gamms.html",
    "title": "12  Bayesian GAMs",
    "section": "",
    "text": "13 Preliminaries\nLoad libraries we’ll need, all familiar from Chapter 10 and Chapter 11 on GA(M)Ms.\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(brms)\nlibrary(itsadug)\nlibrary(tidybayes)\nlibrary(modelr)\n\nlibrary(patchwork)\nImport the pm_young subset of the price_bin data, used in Chapter 11:\npm_young &lt;- readRDS(file = url(\"https://osf.io/download/xw7va/\"))\nToday we will mostly analyze today from just one participant, whose data is stored in s43:\ns43 &lt;- pm_young %&gt;% \n  filter(speaker==\"s-43\") %&gt;%\n  group_by(id) %&gt;%\n  mutate(traj_start = measurement_no == min(measurement_no)) %&gt;%\n  ungroup()\nWe restrict to a single speaker because Bayesian GAMs take a long time to fit.\nOur research question will be the same as in Sec. 3 of Week 6: is F1 different between PRICE and MOUTH?\nEmpirical plot for this speaker:\nCode\nrutgers_theme &lt;- theme_minimal() +\n  theme(panel.grid = element_blank(),\n        axis.line = element_line(size=0.8),\n        axis.ticks = element_line(size=0.8),\n        axis.text = element_text(size=14, colour=\"black\"),\n        axis.title = element_text(size=16, face=\"bold\"))\n## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n## ℹ Please use the `linewidth` argument instead.\n\nggplot(s43, aes(x=measurement_no, y=f1, col=vowel)) +\n  geom_line(aes(group=id), alpha=0.05) +\n  geom_smooth() +\n  xlab(\"% vowel duration\") +\n  ylab(\"F1 (Hz)\") +\n  scale_colour_manual(values=c(\"orange\",\"purple\")) +\n  scale_x_continuous(breaks=seq(20,80,30),\n                     labels=paste0(seq(20,80,30), \"%\")) +\n  rutgers_theme +\n  theme(axis.text.x=element_text(size=14,angle=60, hjust=1),\n        panel.grid.major.y=element_line(size=0.1, colour=\"grey\"))\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian GAMs</span>"
    ]
  },
  {
    "objectID": "bayesian_gamms.html#motivation",
    "href": "bayesian_gamms.html#motivation",
    "title": "12  Bayesian GAMs",
    "section": "14.1 Motivation",
    "text": "14.1 Motivation\nIt is worth first thinking through: why would we want to fit a Bayesian GAM to this kind of data? As we’ll see, Bayesian GAMs take much longer to fit and require more knoweldge than regular GAMs. What are the potential benefits?\n\nExercise 14.1 Group discussion of this point. (don’t peek at the commented out text!)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian GAMs</span>"
    ]
  },
  {
    "objectID": "bayesian_gamms.html#smooth-terms-in-brms",
    "href": "bayesian_gamms.html#smooth-terms-in-brms",
    "title": "12  Bayesian GAMs",
    "section": "14.2 Smooth terms in brms",
    "text": "14.2 Smooth terms in brms\nA nice aspect of brms is that its model specification syntax is familiar from other functions: basic regression model syntax is the same as lm(), random effects are specified similarly to g/lmer(), and smooth terms are specified similarly to gam() or bam().\nFor example, consider a single vowel (PRICE) from speaker S43:\n\n## subset to just the PRICE data (discarding vowel = MOUTH):\ns43_price &lt;- s43 %&gt;% filter(vowel==\"price\")\n\nTo fit a frequentist GAM for this data (not accounting for grouping by vowel token, for now):\n\n# frequentist gam\ns43_price_gam &lt;- bam(f1 ~ s(measurement_no, bs=\"cr\", k=11),\n                     data=s43_price)\n\nFit the equivalent Bayesian GAM (this will take longer):\n\ns43_price_m1 &lt;- brm(f1 ~ s(measurement_no, bs=\"cr\", k=11),\n                    control=list(adapt_delta=0.99),\n                    iter = 2500, \n                     data=s43_price, chains = 4, cores = 4, file = 'models/s43_price_m1.brm')\n\n(I had to play with the iter and adapt_delta: if you use the default brm() settings, you’ll get divergent transitions.)\nThe Bayesian GAM takes longer to fit: a minute or two, versus instantaneous.\nFitted F1 time trajectory for the two models, bam() (left) and brm() (right):\n\nCode\nplot_smooth(s43_price_gam, view=\"measurement_no\")\n## Summary:\n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * NOTE : No random effects in the model to cancel.\n## \nconditional_effects(s43_price_m1) %&gt;%\n  plot()\n\n\n\n\n\n\n\n\n\n\n\nThese look very similar.\nThe elegance of brms’ syntax (we basically just had to change bam() to brm()) hides larger differences between the two models under the hood.\nCompare their model summaries:\nGAM:\n\nsummary(s43_price_gam)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f1 ~ s(measurement_no, bs = \"cr\", k = 11)\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  752.486      1.672   450.1   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                     edf Ref.df     F p-value    \n## s(measurement_no) 6.439  7.763 101.7  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.362   Deviance explained = 36.5%\n## fREML = 7736.7  Scale est. = 3893.7    n = 1393\n\nBayesian GAM:\n\nsummary(s43_price_m1)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: f1 ~ s(measurement_no, bs = \"cr\", k = 11) \n##    Data: s43_price (Number of observations: 1393) \n##   Draws: 4 chains, each with iter = 2500; warmup = 1250; thin = 1;\n##          total post-warmup draws = 5000\n## \n## Smoothing Spline Hyperparameters:\n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sds(smeasurement_no_1)     3.07      1.06     1.66     5.61 1.00     1010\n##                        Tail_ESS\n## sds(smeasurement_no_1)     1840\n## \n## Regression Coefficients:\n##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept           752.49      1.68   749.23   755.73 1.00     5133     3488\n## smeasurement_no_1     0.05      0.42    -0.76     0.87 1.00     4327     3330\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma    62.44      1.20    60.16    64.87 1.00     5104     3289\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nWe know how to read a bam() model summary, but what should we do with this new one from brm()?\nbrm() shows different quantities, corresponding to a different way of representing smooths—it exploits the very cool equivalence between smooths and random effects, which allows a GAM to be fitted using machinery for fitting a mixed-effects model. This is well described in Mahr’s tutorial, and explained more succinctly in Franke’s chapter.\nThe model summary in brm() does not show estimated degrees of freedom like bam(), but a “standard deviation” parameter instead. Each smooth corresponds to one such parameter (in the model above, sds(smeasurement_no_1)), which is shown in the “Smoothing Spline Hyperparameters” block.\nTo understand this parameter, which we’ll call \\(\\sigma\\), it’s important to know that bam() and brm() represent smooths differently. In bam(), the “smooth term” describes the entire curve, parametrized as a set of \\(k\\) basis functions. In contrast, brm() separates the basis functions into:\n\nA “linear”, or “fixed effect” component (= 1 basis function)\nA “wiggly” or “random effect” components (= \\(k-1\\) basis functions)\n\nThis is why there are two terms in the brm() output for measurement_no, compared to one in the bam() output. (We return to the linear component below.)\nThe higher the magnitude of the wiggly coefficients, the more wiggly the overall smooth. Since the standard deviation of the wiggly coefficients (\\(\\sigma\\)) is basically an overall summary of their magnitude, \\(\\sigma\\) tells us roughly the same information as the EDF parameter in bam().\nIf there is strong evidence that \\(\\sigma\\) is different from 0, we can conclude that we are looking at a non-linear effect. Evaluating this is like evaluating whether a random-effect term is non-zero (covered in TODO): an approximate method is to examine the posterior of \\(\\sigma\\) to see if the 95% CredI is away from 0. Let’s plot the posterior distribution of \\(\\sigma\\):\n\nmcmc_plot(s43_price_m1, variable=\"^sds\", regex=TRUE, type=\"dens\") + xlim(0,10)\n\n\n\n\n\n\n\n\nThe bulk of the distribution is well above 0, suggesting that there is non-linearity here.\nA more rigorous method would be a model comparison with a model containing a linear effect of measurement_no, using using PSIS-LOO or a Bayes Factor. The results should (intuitively) line up with the hypothesis test shown under “Approximate significance of smooth terms” in the s43_price_gam summary.\n\nExercise 14.2 Do this model comparison, using PSIS-LOO or a Bayes Factor.\n\nReturning to the “fixed” or “linear” part of the smooth—smeasurement_no_1 in the model summary. This term is hard to interpret on its own. Conceptually, it should be the linear approximation to the curve, but in practice, this is only a rough equivalence because its estimate is tied up with the estimate of the “wiggly” part of the smooth (is my understanding). In model s43_price_m1, the linear term has a 95% CredI centered near 0, which makes sense given the empirical plot—a linear approximation would be (roughly) a flat line.\nSeeing strong evidence that the coefficient for the linear part of a smooth is different from 0 would at least suggest that the relationship is not a flat line, but checking that rigorously would require a better method (e.g. comparison with an F1 ~ 1 model).\nThe upshot is that, like in GAMs, the actual parameters shown in the model summary corresponding to smooth terms are hard to interpret. It’s just a matter of whether there are two terms (brm()) or one (bam()) per smooth.\n\n14.2.1 Priors for smooth terms\nWe’re supposed to think about priors when fitting Bayesian models. But because the model terms corresponding to smooths are unintuitive, it’s hard to define priors for them. For example, for smeasurement_no_1 (a.k.a. \\(\\sigma\\)), which parametrizes “the wiggliness of the curve”, we probably want a prior like we’ve used for other “standard deviation parameters” (e.g. random-effect SDs) in Bayesian models (half-\\(t\\), half-Cauchy, or exponential), but what should the scale of this prior be?\nA solution suggested here by GAMM guru Gavin Simpson is:\n\nBut you can pop a gaussian or t prior on the swt_1 term, for example, and another separate prior on the sds term, and then simulate from the prior distribution of the model (with prior_only = TRUE) and plot the prior draws using conditional_smooths() to see samples of smooths drawn from your prior. You should be looking at the amount of wigglines of the prior smooths to see if that about on average matches plausible range of wiggliness for the non-linear relationships you envisage based on your prior knowledge. You can also look at the values on the y axis to see if the implied effect sizes are reasonable given your prior expectations or not. Then you can adjust the priors you use until you effect sizes roughly in line with expectations and the same for the wigglinesses of the smooths implied by the prior.\n\nThat is: use prior predictive simulation!2\nFor example, in our case: the priors of the fitted model are:\n\ns43_price_m1_priors &lt;- prior_summary(s43_price_m1)\ns43_price_m1_priors\n##                    prior     class                                 coef group\n##                   (flat)         b                                           \n##                   (flat)         b                    smeasurement_no_1      \n##  student_t(3, 760, 63.8) Intercept                                           \n##    student_t(3, 0, 63.8)       sds                                           \n##    student_t(3, 0, 63.8)       sds s(measurement_no, bs = \"cr\", k = 11)      \n##    student_t(3, 0, 63.8)     sigma                                           \n##  resp dpar nlpar lb ub       source\n##                             default\n##                        (vectorized)\n##                             default\n##                   0         default\n##                   0    (vectorized)\n##                   0         default\n\nThat is:\n\nThe intercept (row 1) and linear component (row 2: smeasurement_no_1) have flat priors, as is the default for fixed effects in brms.\nThe smooth component (coef = s(measurement_no, bs = \"cr\", k = 11)) has a Student-\\(t\\) distribution with scale estimated from the SD of the data (here, measurement_no), which is the default for standard deviation terms in brms.\n\nTo do prior predictive simulation, we can’t have any flat priors, so we’ll change them to be reasonable based on domain knowledge. We’ll also change the prior for the smooth component to be even weaker. The logic is: since we don’t really understand what these parameters mean, we’ll first use very weak priors, then “rein them in” until our prior predictive simulation gives reasonable results.\nAs a first try:\n\nReasonable values for the intercept are about 0-2000 Hz\nA large linear effect would be 10 (measurement_no ranges from 0-100, so that corresponds to a 1000 hz change)\nKeep the smooth component’s prior:\n\n\nprior1 &lt;- prior(normal(1000, 500), class = \"Intercept\") + prior(normal(0,10), class = b) + prior(student_t(3, 0, 63.8), class = sds)\n\nPrior predictive simulation with this prior:\n\ns43_price_brm_nopost &lt;- brm(s43_price_m1$formula,\n                            data=s43_price_m1$data,\n                            prior=prior1,\n                            file = 'models/s43_price_brm_nopost',\n                            sample_prior=\"only\")\n## \n## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 4e-05 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.4 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.024 seconds (Warm-up)\n## Chain 1:                0.013 seconds (Sampling)\n## Chain 1:                0.037 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 2e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.026 seconds (Warm-up)\n## Chain 2:                0.012 seconds (Sampling)\n## Chain 2:                0.038 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 1e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.029 seconds (Warm-up)\n## Chain 3:                0.012 seconds (Sampling)\n## Chain 3:                0.041 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 1e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.023 seconds (Warm-up)\n## Chain 4:                0.012 seconds (Sampling)\n## Chain 4:                0.035 seconds (Total)\n## Chain 4:\n\n## Examine just the smooth\nconditional_smooths(s43_price_brm_nopost, spaghetti=T, ndraws=20)\n\n\n## Examine the predicted F1 ~ measuerment_no curves\ns43 %&gt;%\n    data_grid(measurement_no = seq_range(measurement_no, n = 100)) %&gt;%\n    add_epred_draws(s43_price_brm_nopost, ndraws = 100) %&gt;%\n    ggplot(aes(x = measurement_no, y = F1)) +\n    geom_line(aes(y = .epred, group = .draw), alpha = .1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote the scale of the \\(y\\)-axis—way too large.\nLet’s try a more restrictive prior on the smooth term:\n\nprior2 &lt;- prior(normal(1000, 500), class = \"Intercept\") + prior(normal(0,10), class = b) + prior(student_t(3, 0, 20), class = sds)\n\n\ns43_price_brm_nopost_2 &lt;- brm(s43_price_m1$formula,\n                            data=s43_price_m1$data,\n                            prior=prior2,\n                            file = 'models/s43_price_brm_nopost_2',\n                            sample_prior=\"only\")\n## Compiling Stan program...\n## Start sampling\n## \n## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 3.6e-05 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.026 seconds (Warm-up)\n## Chain 1:                0.013 seconds (Sampling)\n## Chain 1:                0.039 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 1e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.028 seconds (Warm-up)\n## Chain 2:                0.013 seconds (Sampling)\n## Chain 2:                0.041 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 2e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.026 seconds (Warm-up)\n## Chain 3:                0.014 seconds (Sampling)\n## Chain 3:                0.04 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 4e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.028 seconds (Warm-up)\n## Chain 4:                0.014 seconds (Sampling)\n## Chain 4:                0.042 seconds (Total)\n## Chain 4:\nconditional_smooths(s43_price_brm_nopost_2, spaghetti=T, ndraws=20)\n\n\ns43 %&gt;%\n    data_grid(measurement_no = seq_range(measurement_no, n = 100)) %&gt;%\n    add_epred_draws(s43_price_brm_nopost_2, ndraws = 20) %&gt;%\n    ggplot(aes(x = measurement_no, y = F1)) +\n    geom_line(aes(y = .epred, group = .draw), alpha = .1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice the difference in scale along the \\(y\\)-axis. There is a comparable amount of wiggliness here relative to what we know F1 curves in the real world look like—though perhaps we’d like to put an even stronger prior on wiggliness.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis still isn’t a great prior, but hopefully we’ve shown what the process of using posterior predictive simulation to refine the prior would look like—even when the actual model parameters for the smooths are difficult to interpret.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian GAMs</span>"
    ]
  },
  {
    "objectID": "bayesian_gamms.html#autoregression-terms",
    "href": "bayesian_gamms.html#autoregression-terms",
    "title": "12  Bayesian GAMs",
    "section": "14.3 Autoregression terms",
    "text": "14.3 Autoregression terms\nWhat if we wanted to control for correlations within trajectories? It’s easy – and, actually, brms is much more flexible in how we do it. We can finally test the question of whether we need an AR1 or an AR2 model:\n\ns43_price_brm_ar1 &lt;- brm(f1 ~ s(measurement_no, bs=\"cr\", k=11) + ar(p=1, gr=id),\n                     data=s43_price, control=list(adapt_delta=0.99),\n                     file = 'models/s43_price_brm_ar1',\n                     cores=4, chains = 4)\ns43_price_brm_ar2 &lt;- brm(f1 ~ s(measurement_no, bs=\"cr\", k=11) + ar(p=2, gr=id),\n                     data=s43_price, control=list(adapt_delta=0.99),\n                     file = 'models/s43_price_brm_ar2',\n                     cores=4, chains = 4)\n\nModel summaries:\n\nsummary(s43_price_brm_ar1)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: f1 ~ s(measurement_no, bs = \"cr\", k = 11) + ar(p = 1, gr = id) \n##    Data: s43_price (Number of observations: 1393) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Smoothing Spline Hyperparameters:\n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sds(smeasurement_no_1)     2.69      0.85     1.54     4.81 1.00     1138\n##                        Tail_ESS\n## sds(smeasurement_no_1)     1662\n## \n## Correlation Structures:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## ar[1]     0.65      0.02     0.60     0.69 1.00     4817     2899\n## \n## Regression Coefficients:\n##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept           753.02      3.33   746.56   759.71 1.00     4445     2245\n## smeasurement_no_1     0.07      0.60    -1.10     1.25 1.00     4125     3073\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma    50.80      0.97    48.96    52.77 1.00     4562     2711\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\nsummary(s43_price_brm_ar2)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: f1 ~ s(measurement_no, bs = \"cr\", k = 11) + ar(p = 2, gr = id) \n##    Data: s43_price (Number of observations: 1393) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Smoothing Spline Hyperparameters:\n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## sds(smeasurement_no_1)     2.76      0.90     1.54     4.93 1.00      884\n##                        Tail_ESS\n## sds(smeasurement_no_1)     1399\n## \n## Correlation Structures:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## ar[1]     0.61      0.03     0.55     0.66 1.00     3477     3082\n## ar[2]     0.08      0.03     0.01     0.14 1.00     3366     2763\n## \n## Regression Coefficients:\n##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept           753.10      3.47   746.18   759.84 1.00     4274     3044\n## smeasurement_no_1     0.07      0.59    -1.09     1.26 1.00     3420     2942\n## \n## Further Distributional Parameters:\n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma    50.74      0.95    48.99    52.67 1.00     4019     2918\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nconditional_effects(s43_price_m1) %&gt;%\n  plot()\nconditional_effects(s43_price_brm_ar1) %&gt;%\n  plot()\nconditional_effects(s43_price_brm_ar2) %&gt;%\n  plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that these models handle autocorrelation much more rigorously than those fitted with bam(), since the rho component(s), i.e., ar[1] at lag 1 and ar[2] at lag 2, are estimated directly from the data at the same time as the model.\nWe can also use model comparison to actually test “is it important to account for autocorrelation?” and “if so, is an AR1 correlation structure sufficient?”\n\n\n\n\n\n\n\n\nExercise 14.3  \n\nExamine the model prediction plots just above. What effect has incorporating AR1 structure had on model predictions? AR2 structure?\nUse model comparison (via PSIS-LOO), with the three last models fitted above, to test: “is it important to account for autocorrelation?” and “if so, is an AR1 correlation structure sufficient?”\nDetermine a rho value for our bam() model, s43_price_gam. How does it compare to the 95% CredI of this parameter in s43_price_brm_ar1?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian GAMs</span>"
    ]
  },
  {
    "objectID": "bayesian_gamms.html#real-data",
    "href": "bayesian_gamms.html#real-data",
    "title": "12  Bayesian GAMs",
    "section": "16.1 Real data",
    "text": "16.1 Real data\nTo fit a model to a real dataset, with multiple speakers, we have to reckon with the fact (I think) that that you can’t include “random intercepts” or “factor smooths” in a brms GAMM. From here:\n\nUnfortunately, you cannot use splines as “random” or “varying” effects, because the penalized splines from mgcv which brms relies on are already implemented as a varying effect.\nSo if you want a smooth term that is at the same time a “random” effect, you need to work directly with non-penalized splines. That’s IMHO usually not a big deal, since you rarely want to allow the per-individual spline to be very complex (you can’t learn them reliably anyway), so a low-degree-of-freedom spline should work just well. So something like:\ny ~ Treatment + s(t, by = Treatment) + (1 + bs(t, df = 2)|individual) Could be sensible. (this requires the splines package. You may also want to use ns but I would expect the results to be quite comparable)\n\nThe model below attempts to use this tip to fit a Bayesian version of our GAMM for the full pm_young dataset (all speakers), from Week 7:\n\nprice_bin_gam_rsmooth &lt;- bam(f2 ~ age_o +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=age_o) +\n                       s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11),\n                     data=price_bin,\n                     AR.start=traj_start, rho=rho_est)\n\nIt takes a very long time to fit the following model: (Don’t run this code! It takes 1-2 hours on my laptop)\n\npm_young$vowel_o &lt;- as.ordered(pm_young$vowel)\ncontrasts(pm_young$vowel_o) &lt;- \"contr.treatment\"\n\npm_young$speaker_f &lt;-factor(pm_young$speaker)\n\nprior2 &lt;- prior(normal(1000, 500), class = \"Intercept\") + prior(normal(0,10), class = b) + prior(student_t(3, 0, 20), class = sds)\n\n\n\nprice_all_m2 &lt;- brm(f1 ~ vowel_o +\n                      s(measurement_no, bs=\"cr\", k=11) + \n                      s(measurement_no, bs=\"cr\", k=11, by=vowel_o) +\n                      + (1 + bs(measurement_no, df = 2)*vowel_o | speaker_f )  +\n                      ar(p=1, gr=id),\n                    prior = prior2,\n                    data=pm_young, \n                    file = 'models/price_all_m2',\n                    cores=4, chains = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction with R. Second. Boca Raton, FL: Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian GAMs</span>"
    ]
  },
  {
    "objectID": "bayesian_gamms.html#footnotes",
    "href": "bayesian_gamms.html#footnotes",
    "title": "12  Bayesian GAMs",
    "section": "",
    "text": "Also useful, and online: these slides by Simon Wood.↩︎\nI’d add to this advice: examine the predicted trajectories, if that is more intuitive. However, conditional_smooths() is clever about things like removing the effect of autocorrelation terms, so it may be a good idea to stick with it unless you’re sure your predicted trajectories are correct.↩︎",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian GAMs</span>"
    ]
  },
  {
    "objectID": "week6.html#preliminaries",
    "href": "week6.html#preliminaries",
    "title": "10  Generalized Additive Models (GAMs)",
    "section": "",
    "text": "mgcv (Wood 2017), for fitting generalized additive (mixed) models (GA(M)Ms).\nitsadug (van Rij et al. 2022), convenience functions for fitting and interpreting GAMMs using mgcv.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generalized Additive Models (GAMs)</span>"
    ]
  },
  {
    "objectID": "week6.html#new-zealand-diphthong-data",
    "href": "week6.html#new-zealand-diphthong-data",
    "title": "10  Generalized Additive Models (GAMs)",
    "section": "10.2 New Zealand diphthong data",
    "text": "10.2 New Zealand diphthong data\n\n10.2.1 Subset: older speakers only\nHere’s the GAM we looked at in the slides (slide 23 on), fitted to the subset of the price_bin data from older speakers:\n\nprice_bin_older_gam &lt;-\n  bam(f2 ~ s(measurement_no, bs = \"cr\", k = 11),\n    data = price_bin_older\n  )\n\nHere’s the summary that we went through:\n\nsummary(price_bin_older_gam)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f2 ~ s(measurement_no, bs = \"cr\", k = 11)\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 1538.289      1.338    1149   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                     edf Ref.df     F p-value    \n## s(measurement_no) 8.053   9.22 588.4  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.161   Deviance explained = 16.1%\n## fREML = 1.9393e+05  Scale est. = 50787     n = 28365\n\nAnd here’s a simple plot of the smooth, using the plot_smooth() function from the itsadug package.\n\nplot_smooth(price_bin_older_gam, view = \"measurement_no\")\n## Summary:\n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * NOTE : No random effects in the model to cancel.\n## \n\n\n\n\n\n\n\n\n\nExercise 10.1 I’d like to invite you to explore this smooth.\n\nWhat happens to the smooth if you change the value of k, either increasing or decreasing it? Does the code still work? What does the resulting smooth look like when you plot it using plot_smooth()?\nTry using different types of basis functions. One of them can be accessed using bs=\"tp\" (a thin-plate smooth); another one using bs=\"ps\". Do these change the shape of the smooth?\n\n\nExample code for Exercise 10.1 to un-hide if you need help:\n\n\nCode\na.\nprice_bin_older_gam_k6 &lt;-\n  bam(f2 ~ s(measurement_no, bs = \"cr\", k = 6),\n    data = price_bin_older\n  )\nplot_smooth(price_bin_older_gam_k6, view = \"measurement_no\")\n\nb.\nprice_bin_older_gam_tp6 &lt;-\n  bam(f2 ~ s(measurement_no, bs = \"tp\", k = 6),\n    data = price_bin_older\n  )\nplot_smooth(price_bin_older_gam_tp6, view = \"measurement_no\")\n\n\n\n\n10.2.2 Full dataset\nAnd now we look at the full data set, capturing the difference between the older vs. younger groups using a difference smooth. Here’s the model that we explored in the slides (slide 37 on):\n\nprice_bin$age_o &lt;- as.ordered(price_bin$age)\ncontrasts(price_bin$age_o) &lt;- \"contr.treatment\"\n\nprice_bin_gam &lt;-\n  bam(\n    f2 ~ age_o +\n      s(measurement_no, bs = \"cr\", k = 11) +\n      s(measurement_no, bs = \"cr\", k = 11, by = age_o),\n    data = price_bin\n  )\n\nThe model summary.\n\nsummary(price_bin_gam)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f2 ~ age_o + s(measurement_no, bs = \"cr\", k = 11) + s(measurement_no, \n##     bs = \"cr\", k = 11, by = age_o)\n## \n## Parametric coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  1539.117      1.329 1158.04   &lt;2e-16 ***\n## age_oyounger -119.471      2.096  -56.99   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                                  edf Ref.df      F p-value    \n## s(measurement_no)              8.518  9.457 584.32  &lt;2e-16 ***\n## s(measurement_no):age_oyounger 5.895  7.130  73.17  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.253   Deviance explained = 25.3%\n## fREML = 3.2386e+05  Scale est. = 50069     n = 47419\n\nAnd a plot!\n\nplot_smooth(price_bin_gam, view = \"measurement_no\", plot_all = \"age_o\")\n## Summary:\n##  * age_o : factor; set to the value(s): older, younger. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * NOTE : No random effects in the model to cancel.\n## \n\n\n\n\n\n\n\n\nIt’s also possible to plot the difference smooth alone (using plot_diff() from itsadug).\n\nplot_diff(price_bin_gam,\n  view = \"measurement_no\",\n  comp = list(age_o = c(\"older\", \"younger\"))\n)\n## Summary:\n##  * measurement_no : numeric predictor; with 100 values ranging from 0.000000 to 100.000000. \n##  * NOTE : No random effects in the model to cancel.\n## \n## \n## measurement_no window(s) of significant difference(s):\n##  0.000000 - 100.000000\n\n\n\n\n\n\n\n\n\nExercise 10.2 The data set also contains a variable called following_voiceless, which captures the voicing of the following segment. We expect that this vowel will be realised differently when followed by a voiceless segment; but is that the case?\n\nSet up following_voiceless as an ordered factor with treatment coding (like we did for age_o above).\nFit a model with a difference smooth (again, analogous to the one above).\nPlot the results.\n\n\nSolutions to Exercise 10.2:\n\n\nCode\nprice_bin$foll_v_o &lt;- as.ordered(price_bin$following_voiceless)\ncontrasts(price_bin$foll_v_o) &lt;- \"contr.treatment\"\n\nprice_bin_gam_fv &lt;-\n  bam(\n    f2 ~ foll_v_o + age_o +\n      s(measurement_no, bs = \"cr\", k = 11) +\n      s(measurement_no, bs = \"cr\", k = 11, by = age_o) +\n      s(measurement_no, bs = \"cr\", k = 11, by = foll_v_o),\n    data = price_bin, method = \"ML\"\n  )\n\nsummary(price_bin_gam_fv)\n\nplot_smooth(price_bin_gam_fv, view = \"measurement_no\", plot_all = \"foll_v_o\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generalized Additive Models (GAMs)</span>"
    ]
  },
  {
    "objectID": "week6.html#global-temperature-data",
    "href": "week6.html#global-temperature-data",
    "title": "10  Generalized Additive Models (GAMs)",
    "section": "10.3 Global Temperature Data",
    "text": "10.3 Global Temperature Data\nLet us now play around with the global temperature data. It’s stored in a data frame called temp. The relevant columns we’re interested in are Year and median, where median actually has the temperature values (it’s “median” because the temperatures from before the modern age are based on estimates from many different models; there are a range of possible estimates for each year, so the median gives you the “best guess”).\nHere’s a plot of the raw data:\n\nplot(median ~ Year, data = temp, pch = 15, cex = 0.2)\n\n\n\n\n\n\n\n\nAnd here is a GAM fit to this data to get you started, along with a prediction plot.\n\ntemp_gam &lt;- bam(median ~ s(Year, bs = \"cr\", k = 50), data = temp)\nplot(median ~ Year, data = temp, pch = 15, cex = 0.2)\nplot_smooth(temp_gam, view = \"Year\", add = T, n.grid = 2000, rug = F)\n## Summary:\n##  * Year : numeric predictor; with 2000 values ranging from 1.000000 to 2017.000000. \n##  * NOTE : No random effects in the model to cancel.\n## \n\n\n\n\n\n\n\n\nFor reference, here’s how the GAM plot shown in the slides (slide 17) was created:\n\ntemp_gam_slides &lt;- bam(median ~ s(Year, bs = \"ad\", k = 50), data = temp, rho = 0.7)\nplot(median ~ Year, data = temp, pch = 15, cex = 0.2)\nplot_smooth(temp_gam_slides, view = \"Year\", add = T, n.grid = 2000, rug = F)\n## Summary:\n##  * Year : numeric predictor; with 2000 values ranging from 1.000000 to 2017.000000. \n##  * NOTE : No random effects in the model to cancel.\n## \n\n\n\n\n\n\n\n\nNotes on this model:\n\nThe bs=\"ad\" argument sets up a so-called adaptive smoother that can vary the smoothing parameter as a function of the time variable, i.e. certain date ranges can have more or less wiggly estimates than others. Standard smoothers (e.g. bs=\"cr\" or bs=\"tp\") can’t do this.\nThe argument rho=0.7 at the end adds an autoregressive error model, which deals with short-term dependencies between neighbouring data points. In this case, this AR error model can help us remove some of the short-term fluctuations from the data.\n\n\nExercise 10.3  \n\nPlay around with different k values to see how they affect your GAM smoother.\nPlay around with different smoother types ( e.g. bs=\"cr\" or bs=\"tp\") to see how they affect your GAM smooth.\nWhat happens if you leave out the autoregressive error model?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generalized Additive Models (GAMs)</span>"
    ]
  },
  {
    "objectID": "week6.html#autocorrelation-in-the-new-zealand-diphthong-data",
    "href": "week6.html#autocorrelation-in-the-new-zealand-diphthong-data",
    "title": "10  Generalized Additive Models (GAMs)",
    "section": "10.4 Autocorrelation in the New Zealand diphthong data",
    "text": "10.4 Autocorrelation in the New Zealand diphthong data\nConsider the price_bin_gam model from Section 10.2.2.\nWe can plot autocorrelation in the residuals as follows.\n\nacf(resid_gam(price_bin_gam), lag.max=10)\n\n\n\n\n\n\n\n\nHere’s how to include an AR1 error model in a GAM.\n\n# first, we need to have an indicator variable\n# that tells our gam where each trajectory\n# starts; also, the data set has to be set up\n# so that adjacent measurements are also\n# adjacent within the data set (which is already\n# the case here)\nprice_bin &lt;- price_bin %&gt;%\n  group_by(id) %&gt;%\n  mutate(traj_start=measurement_no == min(measurement_no)) %&gt;%\n  ungroup()\n\n# we obtain the autocorrelation at lag 1 within\n# our data set\nrho_est &lt;- start_value_rho(price_bin_gam)\n\n# we run the same model, but with two extra parameters:\n# - AR.start is the indicator variable that shows\n#   the start of each trajectory in the data set\n# - rho is roughly the degree of autocorrelation we\n#   wish to remove\nprice_bin_gam_AR &lt;- \n  bam(f2 ~ age_o +\n        s(measurement_no, bs=\"cr\", k=11) +\n        s(measurement_no, bs=\"cr\", k=11, by=age_o),\n      data=price_bin,\n      AR.start=traj_start, rho=rho_est)\n\nSummarize this model:\n\nsummary(price_bin_gam_AR)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f2 ~ age_o + s(measurement_no, bs = \"cr\", k = 11) + s(measurement_no, \n##     bs = \"cr\", k = 11, by = age_o)\n## \n## Parametric coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  1540.028      2.355   654.0   &lt;2e-16 ***\n## age_oyounger -120.433      3.717   -32.4   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                                  edf Ref.df      F p-value    \n## s(measurement_no)              9.524  9.908 347.59  &lt;2e-16 ***\n## s(measurement_no):age_oyounger 7.441  8.846  62.23  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.253   Deviance explained = 25.3%\n## fREML = 3.0449e+05  Scale est. = 39129     n = 47419\n\nComparing the two models (without vs. with AR1) graphically.\n\nplot_smooth(price_bin_gam, view=\"measurement_no\", plot_all=\"age_o\")\n## Summary:\n##  * age_o : factor; set to the value(s): older, younger. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * NOTE : No random effects in the model to cancel.\n## \nplot_smooth(price_bin_gam_AR, view=\"measurement_no\", plot_all=\"age_o\")\n## Summary:\n##  * age_o : factor; set to the value(s): older, younger. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * NOTE : No random effects in the model to cancel.\n## \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow plotting the autocorrelation for this revised model. Note that resid_gam() has to be used for models that include an AR1 error model.\n\nacf(resid_gam(price_bin_gam_AR), lag.max=10)\n\n\n\n\n\n\n\n\n\nExercise 10.4 Determine the best rho value for the global temperature model above. (Is it 0.7, the value we used?)\n\n\n10.4.1 GAMs beyond linear regression\nGAMs are generalized additive models in the same sense that GLMs are “generalized”: they can fit any model from the “exponential family”, including logistic regression and Poisson regression.1\nTo show an example, let’s load a new dataset, showing how the word “monitor” was used historically:\n\nmonitor &lt;- read.csv(\"monitor_simplified.csv\")\n\nThis dataset was generously provided by Gaurav Kamath, PhD student in Linguistics, from a much larger dataset he’s using in current research.\nEach row of this data corresponds to a single speech, by one parlimetarian, in the US Congress. It lists the number of times the word “monitor” was used in the speech in the senses of “ship” (“The U.S.S. Monitor docked”), “observe” (“We’ve got to monitor this”), or “newspaper” (“Christian Science Monitor”). These different meanings of the same (orthographic) word are called senses. Gaurav’s project examines how word senses change over time.\nColumns of the dataframe:\n\nspeech_id: unique ID for the speech\nyear the speech was delivered\nsense: word sense\ncount: number of uses of this sense\nn: number of uses across all senses\n\nThus, n is the same for rows 1-3, then rows 4-6, etc.\nPlot of proportion of uses of each sense over time:\n\nggplot(monitor, aes(x = year, y = count/n, color = sense)) +\ngeom_smooth() +  \nlabs(x = \"Year\", y = \"Proportion (count/n)\", color = \"Sense\") + \ncoord_cartesian(ylim = c(0,1))\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nHow “monitor” is used has changed dramatically over time.\nLet’s model the rise and fall of the sense “newspaper”. First, subset to just this data:\n\nmonitor_newspaper &lt;- filter(monitor, sense == 'newspaper')\n\nWe’d like to model the probability of using sense “newspaper”. We can use a binomial model here, which is the same as logistic regression but for aggregated data. Instead of e.g. row 1 of monitor_newspaper being 3 rows with “usage = 1” (corresponding to count = 3) and 37 rows with “usage = 0” (corresponding to n = 40 minus count = 3).\nFit this model:\n\nm1 &lt;- bam(cbind(count, n-count) ~ s(year), data = monitor_newspaper, family = binomial)\n\n\nExercise 10.5  \n\nPlot the predicted smooth from this model using plot_smooth().\nYour plot should have a similar shape to the empirical plot above, but different numbers on the y-axis. Why is this?\n\n\nIt’s worth mentioning that including autocorrelation via an argument to bam() is not possible for generalized models (such as logistic regression).2\n\n\n10.4.2 Working with mgcv models\nitsadug is the package most commonly used by (psycho)linguists to work with fitted GA(M)Ms. itsadug has good functionality, but it’s important to not be limited by what any one package can do. Many packages can make predictions and prediction plots from mgcv models (fitted with bam() or gam()), such as ggeffects, modelbased, gratia, or emmeans. Different things you’ll want to do will be easier in different packages.\nAn example using ggeffects (used throughout RMLD: Sonderegger (2023)), which by default makes predictions on the response scale—here, probabilities, rather than log-odds.\n\nlibrary(ggeffects)\n\npreds &lt;- ggpredict(m1, terms = \"year\")\n\nplot(preds) +\n  labs(y = \"Predicted Probability\", x = \"Year\") + ylim(0,1)\n## Scale for y is already present.\n## Adding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\nHow similar does this look to your plot from Exercise 10.4?\nTo exemplify using emmeans with mgcv models, let’s fit a model to data from all three word senses.\n\nFit this model as m2, using the monitor dataframe. (Hint: this is like the price_bin_gam model above.) Don’t worry about autocorrelation here.\nSolution:\n\n\nCode\nmonitor$sense &lt;- as.ordered(monitor$sense)\ncontrasts(monitor$sense) &lt;- \"contr.treatment\"\n\n\nm2 &lt;- bam(cbind(count, n-count) ~ sense + s(year) + s(year, by = sense), data = monitor, family = binomial)\n\n\n\nModel predictions:\n\npreds &lt;- ggpredict(m2, terms = c(\"year\", \"sense\"))\n\nplot(preds) +\n  labs(y = \"Predicted Probability\", x = \"Year\", title = \"\") + ylim(0,1)\n## Scale for y is already present.\n## Adding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\nThis emmeans code finds the pairwise differences between senses, averaging across years:\n\nlibrary(emmeans)\n## Welcome to emmeans.\n## Caution: You lose important information if you filter this package's results.\n## See '? untidy'\n\nemm &lt;- emmeans(m2, ~sense)\n\npairs(emm)\n##  contrast            estimate     SE    df  t.ratio p.value\n##  newspaper - observe    -2.96 0.0168 42636 -176.011  &lt;.0001\n##  newspaper - ship        1.74 0.0277 42636   63.043  &lt;.0001\n##  observe - ship          4.71 0.0273 42636  172.165  &lt;.0001\n## \n## Results are given on the log odds ratio (not the response) scale. \n## P value adjustment: tukey method for comparing a family of 3 estimates\n\n\nExercise 10.6 What are these pairwise differences when year = 2010? (This requires figuring out how to get emmeans to make predictions about one predictor while another is held constant at a given value.)\n\n\n\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.\n\n\nSóskuthy, Márton. 2017. “Generalised Additive Mixed Models for Dynamic Analysis in Linguistics: A Practical Introduction.” arXiv Preprint arXiv:1703.05339.\n\n\n———. 2021. “Evaluating Generalised Additive Mixed Modelling Strategies for Dynamic Speech Analysis.” Journal of Phonetics 84: 101017.\n\n\nvan Rij, Jacolien, Martijn Wieling, R. Harald Baayen, and Hedderik van Rijn. 2022. itsadug: Interpreting Time Series and Autocorrelated Data Using GAMMs.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction with R. Second. Boca Raton, FL: Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generalized Additive Models (GAMs)</span>"
    ]
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "11  Generalized additive mixed-effects models (GAMMs)",
    "section": "",
    "text": "11.1 Preliminaries\nLoad libraries we’ll need. Most are the same as Chapter 10:\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(itsadug)\n\nlibrary(gratia)\nlibrary(ggeffects)\nlibrary(patchwork)\nNew library for today, which we’ll use to expand our repertoire of plotting tools beyond itsadug (motivated in Section 10.4.2): gratia (Simpson 2024).\nImport the price_bin data from last time, as well as two new versions of this data that we’ll need below.\nprice_bin &lt;- readRDS(file = url(\"https://osf.io/download/vs6jk/\"))\npm_young &lt;- readRDS(file = url(\"https://osf.io/download/xw7va/\"))\nprice_cont &lt;- readRDS(file = url(\"https://osf.io/download/t3udn/\"))\n## you can also download these files from Piazza, and\n## load them in from the current directory, e.g.\n# price_bin &lt;- readRDS(\"price_bin.rds\")\nRe-do code from last chapter:\n## needed to fit a model with a difference smooth\nprice_bin$age_o &lt;- as.ordered(price_bin$age)\ncontrasts(price_bin$age_o) &lt;- \"contr.treatment\"\n\n## needed to fit the AR1 model\nprice_bin &lt;- price_bin %&gt;%\n  group_by(id) %&gt;%\n  mutate(traj_start=measurement_no == min(measurement_no)) %&gt;%\n  ungroup()\n\n## model with difference smooth\nprice_bin_gam &lt;-\n  bam(\n    f2 ~ age_o +\n      s(measurement_no, bs = \"cr\", k = 11) +\n      s(measurement_no, bs = \"cr\", k = 11, by = age_o),\n    data = price_bin\n  )\n\nrho_est &lt;- start_value_rho(price_bin_gam)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized additive mixed-effects models (GAMMs)</span>"
    ]
  },
  {
    "objectID": "week7.html#random-intercepts",
    "href": "week7.html#random-intercepts",
    "title": "11  Generalized additive mixed-effects models (GAMMs)",
    "section": "13.1 Random intercepts",
    "text": "13.1 Random intercepts\n\nprice_bin$speaker_f &lt;- factor(price_bin$speaker)\n\nprice_bin_gam_rint &lt;- bam(f2 ~ age_o +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=age_o) +\n                       s(speaker_f, bs=\"re\"),\n                     data=price_bin,\n                     AR.start=traj_start, rho=rho_est)\n\nPredicted effect of measurement_no for each age group:\n\nplot_smooth(price_bin_gam_rint, view=\"measurement_no\", plot_all=\"age_o\", rm.ranef=T,print.summary = FALSE)\n\n\n\n\n\n\n\n\n(Note the print.summary option here, to get rid of all the usual output that accompanies plot_smooth().)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized additive mixed-effects models (GAMMs)</span>"
    ]
  },
  {
    "objectID": "week7.html#random-slopes",
    "href": "week7.html#random-slopes",
    "title": "11  Generalized additive mixed-effects models (GAMMs)",
    "section": "13.2 Random slopes",
    "text": "13.2 Random slopes\nFit a model with by-speaker random intercepts and a random slope of measurement_no.\n\nprice_bin_gam_rslope &lt;- bam(f2 ~ age_o +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=age_o) +\n                       s(speaker_f, bs=\"re\") + s(speaker_f, measurement_no, bs=\"re\"),\n                     data=price_bin,\n                     AR.start=traj_start, rho=rho_est)\n\nPredicted effect of measurement_no for each age group:\n\nplot_smooth(price_bin_gam_rslope, view=\"measurement_no\", plot_all=\"age_o\", rm.ranef=T)\n## Summary:\n##  * age_o : factor; set to the value(s): older, younger. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(speaker_f),s(speaker_f,measurement_no)\n## \n\n\n\n\n\n\n\n\nImportant: it is usually not a good idea to use random slopes instead of random smooths!1\nHowever, you do often need random slope terms in GAMMs for parametric effects. These are equivalent to, and just as important as, random slope terms in GLMMS. For example, to allow the effect of age_o to differ by word, we’d include the term `s(wordform, age_o, bs = ‘re’).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized additive mixed-effects models (GAMMs)</span>"
    ]
  },
  {
    "objectID": "week7.html#random-smooths",
    "href": "week7.html#random-smooths",
    "title": "11  Generalized additive mixed-effects models (GAMMs)",
    "section": "13.3 Random smooths",
    "text": "13.3 Random smooths\nFit a model with a by-speaker random smooth for measurement_no:\n\nprice_bin_gam_rsmooth &lt;- bam(f2 ~ age_o +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=age_o) +\n                       s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11),\n                     data=price_bin,\n                     AR.start=traj_start, rho=rho_est)\n## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated\n## 1-d smooths of same variable.\n\nPredicted effect:\n\nplot_smooth(price_bin_gam_rsmooth, view=\"measurement_no\", plot_all=\"age_o\", rm.ranef=T)\n## Summary:\n##  * age_o : factor; set to the value(s): older, younger. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \n\n\n\n\n\n\n\n\nThis model takes a while to fit. There is a more efficient implementation of the bam() function that you can enable by setting the value of the discrete parameter to TRUE:\n\nprice_bin_gam_rsmooth_2 &lt;- bam(f2 ~ age_o +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=age_o) +\n                       s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11),\n                     data=price_bin,\n                     AR.start=traj_start, rho=rho_est,\n                     discrete=T)\n## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated\n## 1-d smooths of same variable.\n\n\nplot_smooth(price_bin_gam_rsmooth_2, view=\"measurement_no\", plot_all=\"age_o\", rm.ranef=T)\n## Summary:\n##  * age_o : factor; set to the value(s): older, younger. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \n\n\n\n\n\n\n\n\nSetting discrete = TRUE is usually a good approximation to the slower model fitted with the default (discrete = FALSE), but technically this should be checked before reporting your final model.\nModel summary:\n\nsummary(price_bin_gam_rsmooth)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f2 ~ age_o + s(measurement_no, bs = \"cr\", k = 11) + s(measurement_no, \n##     bs = \"cr\", k = 11, by = age_o) + s(measurement_no, speaker_f, \n##     bs = \"fs\", m = 1, k = 11)\n## \n## Parametric coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   1586.50      25.68  61.783  &lt; 2e-16 ***\n## age_oyounger  -159.59      36.38  -4.387 1.15e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                                    edf  Ref.df     F p-value    \n## s(measurement_no)                7.994   8.215 25.38  &lt;2e-16 ***\n## s(measurement_no):age_oyounger   5.316   5.611 11.72  &lt;2e-16 ***\n## s(measurement_no,speaker_f)    345.533 436.000 14.11  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =   0.46   Deviance explained = 46.4%\n## fREML = 3.0204e+05  Scale est. = 34633     n = 47419",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized additive mixed-effects models (GAMMs)</span>"
    ]
  },
  {
    "objectID": "week7.html#by-speaker-predictions",
    "href": "week7.html#by-speaker-predictions",
    "title": "11  Generalized additive mixed-effects models (GAMMs)",
    "section": "13.4 By-speaker predictions",
    "text": "13.4 By-speaker predictions\nPlots of the predicted measurement_no effect by speaker for each model, not accounting for age_o. This will give us a sense of what different random effect structures do.\n(To make these properly, the model predictions would ned to account for each speaker’s age. I leave this as an exercise.)\nRandom intercept:\n\nplot_smooth(price_bin_gam_rint, view = \"measurement_no\", \n            cond = list(speaker_f = unique(price_bin$speaker_f)), \n            plot_all = \"speaker_f\", \n            rm.ranef = FALSE, se = 0, legend_plot = FALSE,\n            main = \"Smooths of measurement_no by speaker\")\n## Warning in plot_smooth(price_bin_gam_rint, view = \"measurement_no\", cond =\n## list(speaker_f = unique(price_bin$speaker_f)), : speaker_f in cond and in\n## plot_all. Not all levels are being plotted.\n## Summary:\n##  * age_o : factor; set to the value(s): older. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor with 40 values; set to the value(s): s-101, s-104, s-133, s-134, s-140, s-141, s-170, s-171, s-180, s-2, ...\n\n\n\n\n\n\n\n\nRandom intercept + slope:\n\nplot_smooth(price_bin_gam_rslope, view = \"measurement_no\", \n            cond = list(speaker_f = unique(price_bin$speaker_f)), \n            plot_all = \"speaker_f\", \n            rm.ranef = FALSE, se = 0, legend_plot = FALSE,\n            main = \"Smooths of measurement_no by speaker\")\n## Warning in plot_smooth(price_bin_gam_rslope, view = \"measurement_no\", cond =\n## list(speaker_f = unique(price_bin$speaker_f)), : speaker_f in cond and in\n## plot_all. Not all levels are being plotted.\n## Summary:\n##  * age_o : factor; set to the value(s): older. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor with 40 values; set to the value(s): s-101, s-104, s-133, s-134, s-140, s-141, s-170, s-171, s-180, s-2, ...\n\n\n\n\n\n\n\n\nRandom smooths:\n\nplot_smooth(price_bin_gam_rsmooth, view = \"measurement_no\", \n            cond = list(speaker_f = unique(price_bin$speaker_f)), \n            plot_all = \"speaker_f\", \n            rm.ranef = FALSE, se = 0, legend_plot = FALSE,\n            main = \"Smooths of measurement_no by speaker\")\n## Warning in plot_smooth(price_bin_gam_rsmooth, view = \"measurement_no\", cond =\n## list(speaker_f = unique(price_bin$speaker_f)), : speaker_f in cond and in\n## plot_all. Not all levels are being plotted.\n## Summary:\n##  * age_o : factor; set to the value(s): older. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor with 40 values; set to the value(s): s-101, s-104, s-133, s-134, s-140, s-141, s-170, s-171, s-180, s-2, ...\n\n\n\n\n\n\n\n\nIt can also be useful to inspect the random effects themselves. Here are just the random smooths, using inspect_random() from itsadug:\n\ninspect_random(price_bin_gam_rsmooth, select=3)\n## Summary:\n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor with 40 values; set to the value(s): s-101, s-104, s-133, s-134, s-140, s-141, s-170, s-171, s-180, s-2, ...\n\n\n\n\n\n\n\n\n(select = 3 picks out the third smooth in the model – you can find the correct number by counting from the top of the smooth estimates part of the model summary; the random smooths are the third smooth for this specific model.)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized additive mixed-effects models (GAMMs)</span>"
    ]
  },
  {
    "objectID": "week7.html#comparison",
    "href": "week7.html#comparison",
    "title": "11  Generalized additive mixed-effects models (GAMMs)",
    "section": "13.5 Comparison",
    "text": "13.5 Comparison\nHere’s what the predicted effect looks like for all five models:\n\nExercise 13.1 (Interpretation)  \n\nConsider the three by-speaker prediction plots above. How do they differ?\nHow do these differences result from the three models’ different random effect structures?\nConsider the predicted measurement_no effect for the five models:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat differences do you see?\n\nWhat do you think it means here that adding any random effect makes so much difference, but it doesn’t matter much what kind of random effect?\n\n\n\nExercise 13.2 (Adding random smooths for previous / following environment) There are two variables in the data set that code the previous and following phone. Here’s what they look like:\n\ntable(price_bin$previous)\n## \n##         _    b    d    f    g    h    J    k    l    m    n none    p    P    r \n##   63   33 1056  630 2929  369 1035  968  776 8172 2870 4043 1639  708   10 7294 \n##    s    S    t    T    v    w    z \n## 2068   71 5859   20  308 6458   40\ntable(price_bin$following)\n## \n##           _     {     @     5     b     d     D     e     f     g     h     i \n##    63    41    66  1932    31   145  4152   246    22   967    21   117    51 \n##     I     k     l     m     n  none     p     Q     r     s     t     v     w \n##   500  5902  2756  4743  5342  3093   273    30   250  2115 10823  2458    10 \n##     z \n##  1270\n\nWe typically use random effects for full words (or items / stimuli) rather than previous / following environment, but there are simply too many unique words in this data set, and specifying previous / following environments achieves essentially the same goal (see Sonderegger (2023), Sec. 10.2.1 for explanation in the context of mixed-effects models). Your task is to set up separate random smooths for both and add them to your model.\n\nFit this model. (Remember: the grouping variable for random smooths must be a factor!)\nDoes this change affect your model output?\nUse inspect_smooth() to look at the previous / following smooths. If you know a bit about phonetics: do these smooths look the way you’d expect them to?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized additive mixed-effects models (GAMMs)</span>"
    ]
  },
  {
    "objectID": "week7.html#random-reference-difference-smooths",
    "href": "week7.html#random-reference-difference-smooths",
    "title": "11  Generalized additive mixed-effects models (GAMMs)",
    "section": "13.6 Random reference-difference smooths",
    "text": "13.6 Random reference-difference smooths\nSóskuthy (2021) points out that for \\(p\\)-values and SEs of smooths for an “average speaker” (or average word, etc.) to be correctly calculated, we need a random slope equivalent for the within-speaker following voiceless effect, i.e. a random effect that allows the shape of the following voiceless effect to vary within speakers. Sóskuthy calls this a “reference-difference smooth”.\nThis sounds complicated, but it’s directly analagous to how we use random slopes in GLMMs. If we want a correct estimate of predictor \\(x\\), which is a factor with two levels, and the effect of \\(x\\) can (conceptually) vary between speakers, then we need a term that allows speakers to vary in this way—this is the by-speaker random slope of \\(x\\).\nAdd a reference-difference smooth to our example:\n\nprice_bin$foll_v_o &lt;- as.ordered(price_bin$following_voiceless)\ncontrasts(price_bin$foll_v_o) &lt;- \"contr.treatment\"\n\nprice_bin_gam_fv_rs &lt;- bam(f2 ~ foll_v_o +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=foll_v_o) +\n                       s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11) +\n                       s(measurement_no, speaker_f, by=foll_v_o, bs=\"fs\", m=1, k=11),\n                     data=price_bin,\n                     AR.start=traj_start, rho=rho_est,\n                     discrete=T)\n## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated\n## 1-d smooths of same variable.\n\nModel summary:\n\nsummary(price_bin_gam_fv_rs)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f2 ~ foll_v_o + s(measurement_no, bs = \"cr\", k = 11) + s(measurement_no, \n##     bs = \"cr\", k = 11, by = foll_v_o) + s(measurement_no, speaker_f, \n##     bs = \"fs\", m = 1, k = 11) + s(measurement_no, speaker_f, \n##     by = foll_v_o, bs = \"fs\", m = 1, k = 11)\n## \n## Parametric coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  1492.963     22.242  67.125  &lt; 2e-16 ***\n## foll_v_oTRUE   31.042      8.594   3.612 0.000304 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                                              edf  Ref.df      F p-value    \n## s(measurement_no)                          8.749   8.947 69.703  &lt;2e-16 ***\n## s(measurement_no):foll_v_oTRUE             6.852   7.599 25.850  &lt;2e-16 ***\n## s(measurement_no,speaker_f)              350.158 438.000 12.222  &lt;2e-16 ***\n## s(measurement_no,speaker_f):foll_v_oTRUE 216.850 426.000  1.524  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.487   Deviance explained = 49.3%\n## fREML = 3.0159e+05  Scale est. = 33655     n = 47419\n\nAnd a plot of the two smooths.\n\nplot_smooth(price_bin_gam_fv_rs, view=\"measurement_no\",\n            plot_all=\"foll_v_o\", rm.ranef=T)\n## Summary:\n##  * foll_v_o : factor; set to the value(s): FALSE, TRUE. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f),s(measurement_no,speaker_f):foll_v_oTRUE\n## \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBroader context\n\n\n\n\n\nYou may notice something slightly odd about the plotted smooths: the confidence interval around the second one (following voiceless = TRUE) is a bit wider than the confidence interval around the first one. This is a known issue with plots of models with reference-difference smooths, which Morgan thinks is related to the fact that there are no correlations between different random-effect terms included in GAMMs (unlike GLMMs). Based on the simulations in Sóskuthy (2021), the model estimates should be OK – so this is likely an issue with the prediction function for GAMMs.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized additive mixed-effects models (GAMMs)</span>"
    ]
  },
  {
    "objectID": "week7.html#footnotes",
    "href": "week7.html#footnotes",
    "title": "11  Generalized additive mixed-effects models (GAMMs)",
    "section": "",
    "text": "To see why, compare the second and third by-speaker prediction plots below. Exceptions are when by-speaker differences are well-approximated as linear (= equivalent to random smooths), or it’s computationally infeasible to include random smooths and you want to at least account for some by-speaker (etc.) variability in trajectory shape.↩︎\nHint: this model should not contain an additional random smooth.↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized additive mixed-effects models (GAMMs)</span>"
    ]
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "Advanced Quantitative Methods for Linguistic Data",
    "section": "Contributors",
    "text": "Contributors\nThe authors are:\n\nChapters 1-9: Sonderegger\nChapters 10-11: Sóskuthy\nChapter 12: Sóskuthy & Sonderegger\nChapter 13: Lipari\nChapter 14: Doucette\n\nWe use datasets from (incomplete list):\n\nJoey Stanley: in 9  Multivariate models\nJeanne Brown: in 9  Multivariate models\nGaurav Kamath: in 10.4.1 GAMs beyond linear regression\nMichele Gubian: in 13  Functional principal components analysis\nRasmus Puggaard-Rode: in 13  Functional principal components analysis",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Advanced Quantitative Methods for Linguistic Data",
    "section": "Description",
    "text": "Description\nThe last update to these notes was on the “Published” date above.\n\nThese notes include:\n\nApplications to linguistic data of concepts from the reading\nPractical illustration of topics from the reading\nExercises: to be done in class, or on your own time",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "week7.html#preliminaries",
    "href": "week7.html#preliminaries",
    "title": "11  Generalized additive mixed-effects models (GAMMs)",
    "section": "",
    "text": "“Graceful ‘ggplot’-based graphics and utility functions for working with generalized additive models (GAMs) fitted using the ‘mgcv’ package.”\n\n\n\n\n\n\nCode new factor needed for a model with a difference smooth for age\nFit a first model to the New Zealand diphthong data (price_bin) to determine rho_est for fitting an AR1 model next.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized additive mixed-effects models (GAMMs)</span>"
    ]
  },
  {
    "objectID": "week7.html#random-effects",
    "href": "week7.html#random-effects",
    "title": "11  Generalized additive mixed-effects models (GAMMs)",
    "section": "11.2 Random effects",
    "text": "11.2 Random effects\nImportant practical note: factors used in GAMM models for random effects need to be coded as factors. Otherwise you’ll get an opaque error, like\nError in `names(dat) &lt;- object$term`:! 'names' attribute [1] \nmust be the same length as the vector [0]\". \nTry fitting this model:\n\nbam(f2 ~ age_o +\n      s(measurement_no, bs=\"cr\", k=11) +\n      s(measurement_no, bs=\"cr\", k=11, by=age_o) +\n      s(speaker, bs=\"re\"),\n    data=price_bin,\n    AR.start=traj_start, rho=rho_est)\n\nThe issue is that speaker is by default coded as a character vector when the price_bin data was imported, rather than a factor. Because this is now the default behavior in R by functions for data import (e.g. read.csv()), this issue comes up often.\nNow: here’s how you can specify the three different types of random effects in a GAMM, exemplified using the New Zealand diphthong data.\n\n11.2.1 Random intercepts\n\nprice_bin$speaker_f &lt;- factor(price_bin$speaker)\n\nprice_bin_gam_rint &lt;- bam(f2 ~ age_o +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=age_o) +\n                       s(speaker_f, bs=\"re\"),\n                     data=price_bin,\n                     AR.start=traj_start, rho=rho_est)\n\nPredicted effect of measurement_no for each age group:\n\nplot_smooth(price_bin_gam_rint, view=\"measurement_no\", plot_all=\"age_o\", rm.ranef=T,print.summary = FALSE)\n\n\n\n\n\n\n\n\n(Note the print.summary option here, to get rid of all the usual output that accompanies plot_smooth().)\n\n\n11.2.2 Random slopes\nFit a model with by-speaker random intercepts and a random slope of measurement_no.\n\nprice_bin_gam_rslope &lt;- bam(f2 ~ age_o +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=age_o) +\n                       s(speaker_f, bs=\"re\") + s(speaker_f, measurement_no, bs=\"re\"),\n                     data=price_bin,\n                     AR.start=traj_start, rho=rho_est)\n\nPredicted effect of measurement_no for each age group:\n\nplot_smooth(price_bin_gam_rslope, view=\"measurement_no\", plot_all=\"age_o\", rm.ranef=T)\n## Summary:\n##  * age_o : factor; set to the value(s): older, younger. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(speaker_f),s(speaker_f,measurement_no)\n## \n\n\n\n\n\n\n\n\nImportant: it is usually not a good idea to use random slopes instead of random smooths!1\nHowever, you do often need random slope terms in GAMMs for parametric effects. These are equivalent to, and just as important as, random slope terms in GLMMS. For example, to allow the effect of age_o to differ by word, we’d include the term `s(wordform, age_o, bs = ‘re’).\n\n\n11.2.3 Random smooths\nFit a model with a by-speaker random smooth for measurement_no:\n\nprice_bin_gam_rsmooth &lt;- bam(f2 ~ age_o +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=age_o) +\n                       s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11),\n                     data=price_bin,\n                     AR.start=traj_start, rho=rho_est)\n## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated\n## 1-d smooths of same variable.\n\nPredicted effect:\n\nplot_smooth(price_bin_gam_rsmooth, view=\"measurement_no\", plot_all=\"age_o\", rm.ranef=T)\n## Summary:\n##  * age_o : factor; set to the value(s): older, younger. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \n\n\n\n\n\n\n\n\nThis model takes a while to fit. There is a more efficient implementation of the bam() function that you can enable by setting the value of the discrete parameter to TRUE:\n\nprice_bin_gam_rsmooth_2 &lt;- bam(f2 ~ age_o +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=age_o) +\n                       s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11),\n                     data=price_bin,\n                     AR.start=traj_start, rho=rho_est,\n                     discrete=T)\n## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated\n## 1-d smooths of same variable.\n\n\nplot_smooth(price_bin_gam_rsmooth_2, view=\"measurement_no\", plot_all=\"age_o\", rm.ranef=T)\n## Summary:\n##  * age_o : factor; set to the value(s): older, younger. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \n\n\n\n\n\n\n\n\nSetting discrete = TRUE is usually a good approximation to the slower model fitted with the default (discrete = FALSE), but technically this should be checked before reporting your final model.\nModel summary:\n\nsummary(price_bin_gam_rsmooth)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f2 ~ age_o + s(measurement_no, bs = \"cr\", k = 11) + s(measurement_no, \n##     bs = \"cr\", k = 11, by = age_o) + s(measurement_no, speaker_f, \n##     bs = \"fs\", m = 1, k = 11)\n## \n## Parametric coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   1586.50      25.68  61.783  &lt; 2e-16 ***\n## age_oyounger  -159.59      36.38  -4.387 1.15e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                                    edf  Ref.df     F p-value    \n## s(measurement_no)                7.994   8.215 25.38  &lt;2e-16 ***\n## s(measurement_no):age_oyounger   5.316   5.611 11.72  &lt;2e-16 ***\n## s(measurement_no,speaker_f)    345.533 436.000 14.11  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =   0.46   Deviance explained = 46.4%\n## fREML = 3.0204e+05  Scale est. = 34633     n = 47419\n\n\n\n11.2.4 By-speaker predictions\nPlots of the predicted measurement_no effect by speaker for each model, not accounting for age_o. This will give us a sense of what different random effect structures do.\n(To make these properly, the model predictions would ned to account for each speaker’s age. I leave this as an exercise.)\nRandom intercept:\n\nplot_smooth(price_bin_gam_rint, view = \"measurement_no\", \n            cond = list(speaker_f = unique(price_bin$speaker_f)), \n            plot_all = \"speaker_f\", \n            rm.ranef = FALSE, se = 0, legend_plot = FALSE,\n            main = \"Smooths of measurement_no by speaker\")\n## Warning in plot_smooth(price_bin_gam_rint, view = \"measurement_no\", cond =\n## list(speaker_f = unique(price_bin$speaker_f)), : speaker_f in cond and in\n## plot_all. Not all levels are being plotted.\n## Summary:\n##  * age_o : factor; set to the value(s): older. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor with 40 values; set to the value(s): s-101, s-104, s-133, s-134, s-140, s-141, s-170, s-171, s-180, s-2, ...\n\n\n\n\n\n\n\n\nRandom intercept + slope:\n\nplot_smooth(price_bin_gam_rslope, view = \"measurement_no\", \n            cond = list(speaker_f = unique(price_bin$speaker_f)), \n            plot_all = \"speaker_f\", \n            rm.ranef = FALSE, se = 0, legend_plot = FALSE,\n            main = \"Smooths of measurement_no by speaker\")\n## Warning in plot_smooth(price_bin_gam_rslope, view = \"measurement_no\", cond =\n## list(speaker_f = unique(price_bin$speaker_f)), : speaker_f in cond and in\n## plot_all. Not all levels are being plotted.\n## Summary:\n##  * age_o : factor; set to the value(s): older. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor with 40 values; set to the value(s): s-101, s-104, s-133, s-134, s-140, s-141, s-170, s-171, s-180, s-2, ...\n\n\n\n\n\n\n\n\nRandom smooths:\n\nplot_smooth(price_bin_gam_rsmooth, view = \"measurement_no\", \n            cond = list(speaker_f = unique(price_bin$speaker_f)), \n            plot_all = \"speaker_f\", \n            rm.ranef = FALSE, se = 0, legend_plot = FALSE,\n            main = \"Smooths of measurement_no by speaker\")\n## Warning in plot_smooth(price_bin_gam_rsmooth, view = \"measurement_no\", cond =\n## list(speaker_f = unique(price_bin$speaker_f)), : speaker_f in cond and in\n## plot_all. Not all levels are being plotted.\n## Summary:\n##  * age_o : factor; set to the value(s): older. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor with 40 values; set to the value(s): s-101, s-104, s-133, s-134, s-140, s-141, s-170, s-171, s-180, s-2, ...\n\n\n\n\n\n\n\n\nIt can also be useful to inspect the random effects themselves. Here are just the random smooths, using inspect_random() from itsadug:\n\ninspect_random(price_bin_gam_rsmooth, select=3)\n## Summary:\n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor with 40 values; set to the value(s): s-101, s-104, s-133, s-134, s-140, s-141, s-170, s-171, s-180, s-2, ...\n\n\n\n\n\n\n\n\n(select = 3 picks out the third smooth in the model – you can find the correct number by counting from the top of the smooth estimates part of the model summary; the random smooths are the third smooth for this specific model.)\n\n\n11.2.5 Comparison\nHere’s what the predicted effect looks like for all five models:\n\nExercise 11.1 (Interpretation)  \n\nConsider the three by-speaker prediction plots above. How do they differ?\nHow do these differences result from the three models’ different random effect structures?\nConsider the predicted measurement_no effect for the five models:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat differences do you see?\n\nWhat do you think it means here that adding any random effect makes so much difference, but it doesn’t matter much what kind of random effect?\n\n\n\nExercise 11.2 (Adding random smooths for previous / following environment) There are two variables in the data set that code the previous and following phone. Here’s what they look like:\n\ntable(price_bin$previous)\n## \n##         _    b    d    f    g    h    J    k    l    m    n none    p    P    r \n##   63   33 1056  630 2929  369 1035  968  776 8172 2870 4043 1639  708   10 7294 \n##    s    S    t    T    v    w    z \n## 2068   71 5859   20  308 6458   40\ntable(price_bin$following)\n## \n##           _     {     @     5     b     d     D     e     f     g     h     i \n##    63    41    66  1932    31   145  4152   246    22   967    21   117    51 \n##     I     k     l     m     n  none     p     Q     r     s     t     v     w \n##   500  5902  2756  4743  5342  3093   273    30   250  2115 10823  2458    10 \n##     z \n##  1270\n\nWe typically use random effects for full words (or items / stimuli) rather than previous / following environment, but there are simply too many unique words in this data set, and specifying previous / following environments achieves essentially the same goal (see Sonderegger (2023), Sec. 10.2.1 for explanation in the context of mixed-effects models). Your task is to set up separate random smooths for both and add them to your model.\n\nFit this model. (Remember: the grouping variable for random smooths must be a factor!)\nDoes this change affect your model output?\nUse inspect_smooth() to look at the previous / following smooths. If you know a bit about phonetics: do these smooths look the way you’d expect them to?\n\n\n\n\n11.2.6 Random reference-difference smooths\nSóskuthy (2021) points out that for \\(p\\)-values and SEs of smooths for an “average speaker” (or average word, etc.) to be correctly calculated, we need a random slope equivalent for the within-speaker following voiceless effect, i.e. a random effect that allows the shape of the following voiceless effect to vary within speakers. Sóskuthy calls this a “reference-difference smooth”.\nThis sounds complicated, but it’s directly analagous to how we use random slopes in GLMMs. If we want a correct estimate of predictor \\(x\\), which is a factor with two levels, and the effect of \\(x\\) can (conceptually) vary between speakers, then we need a term that allows speakers to vary in this way—this is the by-speaker random slope of \\(x\\).\nAdd a reference-difference smooth to our example:\n\nprice_bin$foll_v_o &lt;- as.ordered(price_bin$following_voiceless)\ncontrasts(price_bin$foll_v_o) &lt;- \"contr.treatment\"\n\nprice_bin_gam_fv_rs &lt;- bam(f2 ~ foll_v_o +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=foll_v_o) +\n                       s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11) +\n                       s(measurement_no, speaker_f, by=foll_v_o, bs=\"fs\", m=1, k=11),\n                     data=price_bin,\n                     AR.start=traj_start, rho=rho_est,\n                     discrete=T)\n## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated\n## 1-d smooths of same variable.\n\nModel summary:\n\nsummary(price_bin_gam_fv_rs)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f2 ~ foll_v_o + s(measurement_no, bs = \"cr\", k = 11) + s(measurement_no, \n##     bs = \"cr\", k = 11, by = foll_v_o) + s(measurement_no, speaker_f, \n##     bs = \"fs\", m = 1, k = 11) + s(measurement_no, speaker_f, \n##     by = foll_v_o, bs = \"fs\", m = 1, k = 11)\n## \n## Parametric coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  1492.963     22.242  67.125  &lt; 2e-16 ***\n## foll_v_oTRUE   31.042      8.594   3.612 0.000304 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                                              edf  Ref.df      F p-value    \n## s(measurement_no)                          8.749   8.947 69.703  &lt;2e-16 ***\n## s(measurement_no):foll_v_oTRUE             6.852   7.599 25.850  &lt;2e-16 ***\n## s(measurement_no,speaker_f)              350.158 438.000 12.222  &lt;2e-16 ***\n## s(measurement_no,speaker_f):foll_v_oTRUE 216.850 426.000  1.524  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.487   Deviance explained = 49.3%\n## fREML = 3.0159e+05  Scale est. = 33655     n = 47419\n\nAnd a plot of the two smooths.\n\nplot_smooth(price_bin_gam_fv_rs, view=\"measurement_no\",\n            plot_all=\"foll_v_o\", rm.ranef=T)\n## Summary:\n##  * foll_v_o : factor; set to the value(s): FALSE, TRUE. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f),s(measurement_no,speaker_f):foll_v_oTRUE\n## \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBroader context\n\n\n\n\n\nYou may notice something slightly odd about the plotted smooths: the confidence interval around the second one (following voiceless = TRUE) is a bit wider than the confidence interval around the first one. This is a known issue with plots of models with reference-difference smooths, which Morgan thinks is related to the fact that there are no correlations between different random-effect terms included in GAMMs (unlike GLMMs). Based on the simulations in Sóskuthy (2021), the model estimates should be OK – so this is likely an issue with the prediction function for GAMMs.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized additive mixed-effects models (GAMMs)</span>"
    ]
  },
  {
    "objectID": "week7.html#extended-example-f1-in-price-vs.-mouth",
    "href": "week7.html#extended-example-f1-in-price-vs.-mouth",
    "title": "11  Generalized additive mixed-effects models (GAMMs)",
    "section": "11.3 Extended example: F1 in PRICE vs. MOUTH",
    "text": "11.3 Extended example: F1 in PRICE vs. MOUTH\nWe now look at a data set that has F1 measurements for the PRICE and MOUTH vowels for 18 younger New Zealand English speakers. Here’s what the data look like:\n\nggplot(pm_young, aes(x=measurement_no, y=f1, col=vowel)) +\n  facet_wrap(~speaker, nrow=3) +\n  geom_line(aes(group=id), alpha=0.05) +\n  geom_smooth() +\n  xlab(\"% vowel duration\") +\n  ylab(\"F1 (Hz)\") +\n  scale_colour_manual(values=c(\"orange\",\"purple\")) +\n  scale_x_continuous(breaks=seq(20,80,30),\n                     labels=paste0(seq(20,80,30), \"%\")) +\n  theme(axis.text.x=element_text(size=14,angle=60, hjust=1),\n        strip.text=element_blank(),\n        panel.grid.major.y=element_line(linewidth=0.1, colour=\"grey\"))\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n(Note again: these smoothers are GAMs!)\nOur goal is to find out whether the F1 trajectories for PRICE and MOUTH are reliably different. Traditional descriptions would have these vowels as /aI/ and /aU/ (I and U = lax i and u) – but that’s not what they look like due to changes that have occurred in NZE. Do those changes mean that the F1 trajectories are also now different (again, the traditional descriptions of /aI/ and /aU/ would imply essentially identical F1 trajectories).\n\nExercise 11.3  \n\nFit a GAMM to the F1 data that tests for differences between PRICE and MOUTH. The model should also have appropriate random effects and autocorrelation handling.\nOnce you have a model, use plotting and the model summaries to make sense of the results.\nExtra: Make your model also account for by-word variability (each wordform has a unique value of vowel) using appropriate random effect terms. This model will take significantly longer to fit.2\n\nNote: This is an open ended task, and you’ll have ~15 minutes to complete it (meaning, parts (a)–(b)). That’s not a lot of time, but should be enough to get you started. Don’t worry if you can’t finish the exercise: we will work through it together. This exercise sets the scene for the discussion that will follow.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nData processing:\n\npm_young$speaker_f &lt;- factor(pm_young$speaker)\npm_young$vowel_ord &lt;- as.ordered(pm_young$vowel)\ncontrasts(pm_young$vowel_ord) &lt;- \"contr.treatment\"\n\npm_young &lt;- pm_young %&gt;%\n  group_by(id) %&gt;%\n  mutate(traj_start = measurement_no == min(measurement_no)) %&gt;%\n  ungroup()\n\nFit model:\n\n# step 1: fit model to find autocorrelation at lag 1\npm_gam_noAR &lt;- bam(f1 ~ vowel_ord +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=vowel_ord) +\n                       s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11) +\n                       s(measurement_no, speaker_f, by=vowel_ord, bs=\"fs\", m=1, k=11),\n                     data=pm_young,\n                     discrete=T)\n## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated\n## 1-d smooths of same variable.\n\n# step 2: find autocorrelation\nrho_est &lt;- start_value_rho(pm_gam_noAR)\n\n# step 3: fit final model\npm_gam_AR &lt;- bam(f1 ~ vowel_ord +\n                       s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=vowel_ord) +\n                       s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11) +\n                       s(measurement_no, speaker_f, by=vowel_ord, bs=\"fs\", m=1, k=11),\n                 data=pm_young,\n                 AR.start=pm_young$traj_start, rho=rho_est,\n                 discrete=T)\n## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated\n## 1-d smooths of same variable.\n\nSummary and visualization:\n\nsummary(pm_gam_AR)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f1 ~ vowel_ord + s(measurement_no, bs = \"cr\", k = 11) + s(measurement_no, \n##     bs = \"cr\", k = 11, by = vowel_ord) + s(measurement_no, speaker_f, \n##     bs = \"fs\", m = 1, k = 11) + s(measurement_no, speaker_f, \n##     by = vowel_ord, bs = \"fs\", m = 1, k = 11)\n## \n## Parametric coefficients:\n##                Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)     694.634     18.543  37.460  &lt; 2e-16 ***\n## vowel_ordprice   25.687      6.199   4.144 3.43e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                                                edf  Ref.df      F p-value    \n## s(measurement_no)                            7.773   8.315 62.884  &lt;2e-16 ***\n## s(measurement_no):vowel_ordprice             6.010   6.926 23.241  &lt;2e-16 ***\n## s(measurement_no,speaker_f)                113.024 198.000 17.288  &lt;2e-16 ***\n## s(measurement_no,speaker_f):vowel_ordprice  83.581 197.000  1.232  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.491   Deviance explained = 49.5%\n## fREML = 1.5997e+05  Scale est. = 5785.7    n = 28890\nplot_smooth(pm_gam_AR, view=\"measurement_no\", plot_all=\"vowel_ord\", rm.ranef=T)\n## Summary:\n##  * vowel_ord : factor; set to the value(s): mouth, price. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor; set to the value(s): s-170. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f),s(measurement_no,speaker_f):vowel_ordprice\n##",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized additive mixed-effects models (GAMMs)</span>"
    ]
  },
  {
    "objectID": "week7.html#significance-testing-in-gamms",
    "href": "week7.html#significance-testing-in-gamms",
    "title": "11  Generalized additive mixed-effects models (GAMMs)",
    "section": "11.4 Significance testing in GAMMs",
    "text": "11.4 Significance testing in GAMMs\nSo how do we know if we have a significant effect of vowel in this model? There are a few things we can do:\n\nLook at the parametric difference term for vowel_ord; that will tell us about any height differences.\nLook at the smooth difference term for vowel_ord; that will tell us about any shape differences.\nFit the model with a so-called binary difference term. That actually corresponds to a simultaneous test of height and shape, which is nice if we don’t have separate predictions about those!\nPerform a model comparison between models with and without all terms involving vowel_ord; that also corresponds to a simultaneous effect of height and shape, and takes by-speaker differences into account.\nLook at the difference smooth & associated confidence interval.\n\nHere’s option (3):\n\npm_young$vowel_bin &lt;- as.numeric(pm_young$vowel == \"price\")\n\npm_gam_bin_AR &lt;- bam(f1 ~ s(measurement_no, bs=\"cr\", k=11) +\n                       s(measurement_no, bs=\"cr\", k=11, by=vowel_bin) +\n                       s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11) +\n                       s(measurement_no, speaker_f, by=vowel_bin, bs=\"fs\", m=1, k=11),\n                 data=pm_young,\n                 AR.start=pm_young$traj_start, rho=rho_est,\n                 discrete=T)\n## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated\n## 1-d smooths of same variable.\n\nsummary(pm_gam_bin_AR)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f1 ~ s(measurement_no, bs = \"cr\", k = 11) + s(measurement_no, \n##     bs = \"cr\", k = 11, by = vowel_bin) + s(measurement_no, speaker_f, \n##     bs = \"fs\", m = 1, k = 11) + s(measurement_no, speaker_f, \n##     by = vowel_bin, bs = \"fs\", m = 1, k = 11)\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   694.63      18.54   37.46   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                                           edf  Ref.df      F  p-value    \n## s(measurement_no)                       7.773   8.315 62.884  &lt; 2e-16 ***\n## s(measurement_no):vowel_bin             7.010   7.926 22.298  &lt; 2e-16 ***\n## s(measurement_no,speaker_f)           113.024 197.000 18.022  &lt; 2e-16 ***\n## s(measurement_no,speaker_f):vowel_bin  83.581 196.000  1.326 8.84e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.491   Deviance explained = 49.5%\n## fREML = 1.5997e+05  Scale est. = 5785.7    n = 28890\nplot_smooth(pm_gam_bin_AR, view=\"measurement_no\", plot_all=\"vowel_bin\", rm.ranef=T)\n## Warning in plot_smooth(pm_gam_bin_AR, view = \"measurement_no\", plot_all =\n## \"vowel_bin\", : Predictor vowel_bin is not a factor.\n## Summary:\n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * vowel_bin : numeric predictor; set to the value(s): 0, 1. \n##  * speaker_f : factor; set to the value(s): s-170. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f),s(measurement_no,speaker_f):vowel_bin\n## \n\n\n\n\n\n\n\n\nNote that the predictions from this model are identical to those from the one with a separate parametric / smooth difference term. It’s only the way that this information is presented that’s different.\nOption (4):\n\n## model without any vowel information\npm_gam_bin_AR_1 &lt;- bam(f1 ~ s(measurement_no, bs=\"cr\", k=11) +\n                         s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11),\n                     data=pm_young,\n                     AR.start=pm_young$traj_start, rho=rho_est,\n                     discrete=T)\n## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated\n## 1-d smooths of same variable.\n\ncompareML(pm_gam_bin_AR_1, pm_gam_bin_AR)\n## pm_gam_bin_AR_1: f1 ~ s(measurement_no, bs = \"cr\", k = 11) + s(measurement_no, \n##     speaker_f, bs = \"fs\", m = 1, k = 11)\n## \n## pm_gam_bin_AR: f1 ~ s(measurement_no, bs = \"cr\", k = 11) + s(measurement_no, \n##     bs = \"cr\", k = 11, by = vowel_bin) + s(measurement_no, speaker_f, \n##     bs = \"fs\", m = 1, k = 11) + s(measurement_no, speaker_f, \n##     by = vowel_bin, bs = \"fs\", m = 1, k = 11)\n## \n## Chi-square test of fREML scores\n## -----\n##             Model    Score Edf Difference    Df  p.value Sig.\n## 1 pm_gam_bin_AR_1 160334.9   5                               \n## 2   pm_gam_bin_AR 159969.4  10    365.540 5.000  &lt; 2e-16  ***\n## \n## AIC difference: 773.23, model pm_gam_bin_AR has lower AIC.\n\nOption (5) (looking at difference smooth):\n\nplot_smooth(pm_gam_AR, view=\"measurement_no\", plot_all=\"vowel_ord\", rm.ranef=T)\n## Summary:\n##  * vowel_ord : factor; set to the value(s): mouth, price. \n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor; set to the value(s): s-170. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f),s(measurement_no,speaker_f):vowel_ordprice\n## \nplot_diff(pm_gam_AR, view=\"measurement_no\", comp=list(vowel_ord=c(\"mouth\",\"price\")), rm.ranef=T)\n## Summary:\n##  * measurement_no : numeric predictor; with 100 values ranging from 0.000000 to 100.000000. \n##  * speaker_f : factor; set to the value(s): s-170. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f),s(measurement_no,speaker_f):vowel_ordprice\n## \n## \n## measurement_no window(s) of significant difference(s):\n##  0.000000 - 64.646465\n##  90.909091 - 100.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 11.4 Consider the model price_bin_gam_fv from Exercise 10.2.\n\nPerform two significance tests to assess the effects of age_o and foll_v, using one of options (1)-(4) from above.\nWhich predictor has a larger effect? (Or, if this isn’t possible to answer, explain why not.)\n\n\n\nprice_bin_gam_fv &lt;-\n  bam(\n    f2 ~ foll_v_o + age_o +\n      s(measurement_no, bs = \"cr\", k = 11) +\n      s(measurement_no, bs = \"cr\", k = 11, by = age_o) +\n      s(measurement_no, bs = \"cr\", k = 11, by = foll_v_o),\n    data = price_bin, method = \"ML\"\n  )\n\nprice_bin_gam_fv_no_age &lt;-\n  bam(\n    f2 ~ foll_v_o + \n      s(measurement_no, bs = \"cr\", k = 11) +\n      s(measurement_no, bs = \"cr\", k = 11, by = foll_v_o),\n    data = price_bin, method = \"ML\"\n  )\n\nprice_bin_gam_fv_no_fv &lt;-\n  bam(\n    f2 ~  age_o +\n      s(measurement_no, bs = \"cr\", k = 11) +\n      s(measurement_no, bs = \"cr\", k = 11, by = age_o) +\n      s(measurement_no, bs = \"cr\", k = 11, by = foll_v_o),\n    data = price_bin, method = \"ML\"\n  )\n\ncompareML(price_bin_gam_fv, price_bin_gam_fv_no_age)\n## price_bin_gam_fv: f2 ~ foll_v_o + age_o + s(measurement_no, bs = \"cr\", k = 11) + \n##     s(measurement_no, bs = \"cr\", k = 11, by = age_o) + s(measurement_no, \n##     bs = \"cr\", k = 11, by = foll_v_o)\n## \n## price_bin_gam_fv_no_age: f2 ~ foll_v_o + s(measurement_no, bs = \"cr\", k = 11) + s(measurement_no, \n##     bs = \"cr\", k = 11, by = foll_v_o)\n## \n## Chi-square test of ML scores\n## -----\n##                     Model    Score Edf Difference    Df  p.value Sig.\n## 1 price_bin_gam_fv_no_age 325164.1   6                               \n## 2        price_bin_gam_fv 323355.2   9   1808.923 3.000  &lt; 2e-16  ***\n## \n## AIC difference: -3622.30, model price_bin_gam_fv has lower AIC.\n\n\n11.4.1 Working with interactions\nIn this section, we return to the question of how age affects the F2 trajectory for PRICE. Previously, we tackled this issue by comparing an older and a younger group. We now turn to a continuous measure of age, i.e. year of birth (yob). But how can we test such a variable? The solution to this issue is the GAM(M) version of interactions: tensor product smooths. An interaction in a conventional regression model basically asks the following question: does variable A (e.g. age) change the effect of variable B (e.g. measurement time)? In a conventional regression model with an interaction between A and B, the slope associated with variable B can change as the value of variable A changes.\nA tensor product smooths implements the same idea for smooths: the non-linear effect of variable B changes as the value of variable A changes (also non-linearly). There are three basic ways to implement this in GAM(M) formulas:\n\ns(A, B): 2D smooth, suitable when A and B are on similar scales (e.g. latitude & longitude)\n\nThis is not commonly the case for (non-spatal) linguistic data, so tutorials for linguists tend to just consider the “tensor product” case.\nIn this case, you do not need to include independent smooths s(A) or s(B). This makes intuitive sense if you think of the spatial case—what is the meaning of “the smooth of latitude [alone]”?\n\nte(A, B): tensor product or tensor smooth, doesn’t assume that A and B are on similar scales.\n\nPro: simplicity, requires less data than tensor interaction\nDisadvantage: doesn’t separate the “interaction” of A and B from their “main effects”, which is sometimes useful.\n\ns(A) + s(B) + ti(A, B): tensor interaction or ANOVA decomposition\n\nPro: Decomposes “main effect” versus “interaction”\nCon: Requires more data, interpretation can get tricky when A and B participate in other terms.\n\n\nWhat a 2D smooth means will be easier to appreciate once we look at some plots, so let’s move on for now.\nHere’s a preliminary look at the data. The darker colours indicate speakers born in earlier decades; the range of years of birth spans 1900 to 1980. Note that this visualisation doesn’t tell us all that much – it’s hard to interpret it!\n\nggplot(price_cont, aes(x=measurement_no, y=f2, col=decade)) +\n  facet_wrap(~speaker_ord, nrow=4) +\n  geom_line(aes(group=id), alpha=0.1) +\n  geom_smooth() +\n  xlab(\"% vowel duration\") +\n  ylab(\"F2 (Hz)\") +\n  #scale_colour_manual(values=c(\"orange\",\"purple\"), guide=\"none\") +\n  scale_x_continuous(breaks=seq(20,80,30),\n                     labels=paste0(seq(20,80,30), \"%\")) +\n  theme(axis.text.x=element_text(size=14,angle=60, hjust=1),\n        strip.text=element_blank(),\n        panel.grid.major.y=element_line(size=0.1, colour=\"grey\"))\n## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n## ℹ Please use the `linewidth` argument instead.\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nHere’s the model that we fit to this data. Here we’ve used the “tensor interaction” form:\n\nSmooths of measurement_no and yob measure their simple effects (or “average”, “marginal” effects)\nThe ti() term is the tensor product smooth, i.e. the non-linear interaction term which measures the joint effect of measurement_no and yob beyond the simple effects.\n\n\nprice_cont$speaker_f &lt;- factor(price_cont$speaker)\nprice_cont &lt;- price_cont %&gt;%\n  group_by(id) %&gt;%\n  mutate(traj_start = measurement_no == min(measurement_no)) %&gt;%\n  ungroup()\n\nprice_cont_gam &lt;- \n  bam(f2 ~\n        s(measurement_no, bs=\"cr\", k=11) +\n        s(yob, bs=\"cr\", k=11) +\n        ti(measurement_no, yob, bs=\"cr\", k=c(11,11)) +\n        s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11),\n      data=price_cont,\n      AR.start=price_cont$traj_start, rho=0.7,\n      discrete=T)\n\nsummary(price_cont_gam)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## f2 ~ s(measurement_no, bs = \"cr\", k = 11) + s(yob, bs = \"cr\", \n##     k = 11) + ti(measurement_no, yob, bs = \"cr\", k = c(11, 11)) + \n##     s(measurement_no, speaker_f, bs = \"fs\", m = 1, k = 11)\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  1502.22      18.44   81.46   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                                 edf  Ref.df       F p-value    \n## s(measurement_no)             8.928   9.152 131.524  &lt;2e-16 ***\n## s(yob)                        1.499   1.504   1.850   0.291    \n## ti(measurement_no,yob)       26.735  29.654   3.181  &lt;2e-16 ***\n## s(measurement_no,speaker_f) 306.758 436.000 119.676  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.481   Deviance explained = 48.4%\n## fREML = 3.2595e+05  Scale est. = 33165     n = 51512\n\nThe model summary suggests that there is no significant effect of year of birth alone; this means that year of birth does not make the F2 trajectory as a whole significantly lower or higher (averaging over measurement_no). However, the interaction is significant, suggesting that the shape of the trajectory changes with the year of birth of the speaker.\nSo what does this interaction look like? Let’s create some plots that show us how the F2 trajectory changes as a function of decade of birth in our data. Here’s how to do this with plot_smooth() from itsadug (a bit clunky)\nfor (n in 0:4) {\nplot_smooth(price_cont_gam, view=\"measurement_no\",\n            cond=list(yob=1905 + n * 17.5), rm.ranef=T, se=FALSE, \n            lw=2, ylim=c(1300,1750), n=100, \n            main=paste0(\"year of birth: \", 1905 + n * 17.5))\n}\n## Summary:\n##  * measurement_no : numeric predictor; with 100 values ranging from 0.000000 to 100.000000. \n##  * yob : numeric predictor; set to the value(s): 1905. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \n## Summary:\n##  * measurement_no : numeric predictor; with 100 values ranging from 0.000000 to 100.000000. \n##  * yob : numeric predictor; set to the value(s): 1922.5. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \n## Summary:\n##  * measurement_no : numeric predictor; with 100 values ranging from 0.000000 to 100.000000. \n##  * yob : numeric predictor; set to the value(s): 1940. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \n## Summary:\n##  * measurement_no : numeric predictor; with 100 values ranging from 0.000000 to 100.000000. \n##  * yob : numeric predictor; set to the value(s): 1957.5. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \n## Summary:\n##  * measurement_no : numeric predictor; with 100 values ranging from 0.000000 to 100.000000. \n##  * yob : numeric predictor; set to the value(s): 1975. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is a simpler way to do this using ggpredict() (from ggeffects, see Section 10.4.2):\n\nggpredict(price_cont_gam, terms = c(\"measurement_no\", \"yob [1900, 1920, 1940, 1960, 1980]\")) %&gt;% plot(show_ci = FALSE)\n\nBasically, the trajectory lower and changes shape over time (as yob increases).\nA less intuitive, but often-used visualisation shows this as a heat map. Opinions differ on whether heat maps are:\n\nA natural way to visualize 2D smooths that you should learn to read (Morgan)\nA confusing way that should not be used in situations other than where the two axes have a straightforward spatial interpretation (e.g. for map data, where one axis is longitude and the other one latitude). (Márton)\n\nGenerate a heatmap using fvisgam() from itsadug, along with two plots of “slices” of the heatmap to aid interpretation:\n\nlayout(mat=matrix(c(1,1,2,3), ncol=2))\nfvisgam(price_cont_gam, view=c(\"measurement_no\",\"yob\"), rm.ranef=T)\n## Summary:\n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * yob : numeric predictor; with 30 values ranging from 1900.000000 to 1975.000000. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \n## Warning in gradientLegend(c(min.z, max.z), n.seg = 3, pos = 0.875, color = pal,\n## : Increase right margin to fit labels or decrease the number of decimals, see\n## help(gradientLegend).\nabline(h=1910, lty=2, lwd=2)\nabline(h=1965, lty=2, lwd=2)\npar(mar=c(2.1,4.1,3.1,2.1))\nplot_smooth(price_cont_gam, view=\"measurement_no\",\n            cond=list(yob=1965), rm.ranef=T, se=FALSE,\n            lw=2, ylim=c(1300,1750), n=100)\n## Summary:\n##  * measurement_no : numeric predictor; with 100 values ranging from 0.000000 to 100.000000. \n##  * yob : numeric predictor; set to the value(s): 1965. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \ntext(10, 1700, \"year of birth: 1965\", adj=0)\npar(mar=c(5.1,4.1,0.1,2.1))\nplot_smooth(price_cont_gam, view=\"measurement_no\",\n            cond=list(yob=1910), rm.ranef=T, se=FALSE,\n            lw=2, ylim=c(1300,1750), n=100)\n## Summary:\n##  * measurement_no : numeric predictor; with 100 values ranging from 0.000000 to 100.000000. \n##  * yob : numeric predictor; set to the value(s): 1910. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \ntext(10, 1700, \"year of birth: 1910\", adj=0)\n\n\n\n\n\n\n\n\nThis plot shows predicted f2 as yob and measurement_no are varied. This is not the same as showing just the ti(yob, measurement_no) effect, which would look like this (using draw() from gratia):\n\ndraw(price_cont_gam, select = 3, contour = FALSE)\n\n\n\n\n\n\n\n\nNote that the interaction looks a little “jittery” when looked at in its entirety. This is likely the same kind of artefact that we’ve seen before for the global warming data where a period of rapid change makes periods of stability look more wiggly than they should. An adaptive smoother could potentially help:\n\nprice_cont_gam &lt;- \n  bam(f2 ~\n        s(measurement_no, bs=\"cr\", k=11) +\n        s(yob, bs=\"ad\", k=11) +\n        ti(measurement_no, yob, bs=\"cr\") +\n        s(measurement_no, speaker_f, bs=\"fs\", m=1, k=11),\n      data=price_cont,\n      AR.start=price_cont$traj_start, rho=0.7,\n      discrete=T)\n\nlayout(mat=matrix(c(1,1,2,3), ncol=2))\nfvisgam(price_cont_gam, view=c(\"measurement_no\",\"yob\"), rm.ranef=T)\n## Summary:\n##  * measurement_no : numeric predictor; with 30 values ranging from 0.000000 to 100.000000. \n##  * yob : numeric predictor; with 30 values ranging from 1900.000000 to 1975.000000. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \n## Warning in gradientLegend(c(min.z, max.z), n.seg = 3, pos = 0.875, color = pal,\n## : Increase right margin to fit labels or decrease the number of decimals, see\n## help(gradientLegend).\nabline(h=1910, lty=2, lwd=2)\nabline(h=1965, lty=2, lwd=2)\npar(mar=c(2.1,4.1,3.1,2.1))\nplot_smooth(price_cont_gam, view=\"measurement_no\",\n            cond=list(yob=1965), rm.ranef=T, se=FALSE,\n            lw=2, ylim=c(1300,1750), n=100)\n## Summary:\n##  * measurement_no : numeric predictor; with 100 values ranging from 0.000000 to 100.000000. \n##  * yob : numeric predictor; set to the value(s): 1965. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \ntext(10, 1700, \"year of birth: 1965\", adj=0)\npar(mar=c(5.1,4.1,0.1,2.1))\nplot_smooth(price_cont_gam, view=\"measurement_no\",\n            cond=list(yob=1910), rm.ranef=T, se=FALSE,\n            lw=2, ylim=c(1300,1750), n=100)\n## Summary:\n##  * measurement_no : numeric predictor; with 100 values ranging from 0.000000 to 100.000000. \n##  * yob : numeric predictor; set to the value(s): 1910. \n##  * speaker_f : factor; set to the value(s): s-282. (Might be canceled as random effect, check below.) \n##  * NOTE : The following random effects columns are canceled: s(measurement_no,speaker_f)\n## \ntext(10, 1700, \"year of birth: 1910\", adj=0)\n\n\n\n\n\n\n\n\n\nExercise 11.5  \n\nFor the english data from languageR (introduced in Section 2.5.3): fit a GAM of RTlexdec including:\n\n\na 2D smooth coded using te (not ti) of WrittenFrequency and Familiarity\na parametric term for AgeSubject.\n\nRecall that WrittenFrequency and Familiarity are highly correlated.\n\nVisualize the 2D smooth as a heatmap using fvisgam()\nDo the same using draw() from gratia.\nHow do these visualizations differ? In particular, what does the blank space on the draw() heatmap mean?\nDoes one heatmap seem preferable, given what the data looks like?\n\nFor (c) and (d), you may need to read documentation or search online / ask a chatbot (and verify its answer).\n\n\n\n11.4.2 Extra: Custom plots & videos for GAMMs\nIt’s often useful to create visualizations showing how trajectory shapes change as a function of another variable. This example shows how, more generally, you can use animation to visualize effects from GAMMs.\n\nFirst, we will create a series of four plots showing how trajectory shapes change as a function of year of birth (i.e., four “frames” from an animation). Second, we will show the same change as an animation.\nFor both options, we need to generate model predictions. For this, we first design a data frame that has the same form as the original data, with all the predictors used in our model. Once we have this data frame (called newdat here), generating the actual predictions is really straightforward. For our data frame, we’ll use four levels of year of birth as set out above.\n\nnewdat &lt;- expand.grid(\n  measurement_no=0:100,\n  yob=c(1905, 1930, 1950, 1975),\n  speaker_f=levels(price_cont$speaker_f)[1]\n)\n\npreds &lt;- predict(price_cont_gam, newdat,\n                 exclude=c(\"s(measurement_no,speaker_f)\"),\n                 se.fit=T)\nnewdat$f2 &lt;- preds$fit\nnewdat$ll &lt;- preds$fit - 1.96*preds$se.fit\nnewdat$ul &lt;- preds$fit + 1.96*preds$se.fit\n\nThis data set now has everything we need for our plot! We’ll use ggplot() to create the plot.\n\nggplot(newdat, aes(x=measurement_no,y=f2)) +\n  facet_wrap(~yob, ncol=4)+\n  geom_ribbon(aes(ymin=ll,ymax=ul), col=NA, fill=\"grey\", alpha=0.5) +\n  geom_line() +\n  xlab(\"% vowel duration\") +\n  ylab(\"F2 (Hz)\") +\n  theme_minimal() +\n  theme(axis.line=element_line(),\n        panel.grid=element_blank())\n\n\n\n\n\n\n\n\nThe second option is to use gganimate and create an animation for the whole time period (1900–1980). The first step (creating newdat) is actually ludicrously simple! Instead of creating 4 time slices, we create 81, all the way from 1900 to 1980.\n\nnewdat &lt;- expand.grid(\n  measurement_no=0:100,\n  yob=1900:1980,\n  speaker_f=levels(price_cont$speaker_f)[1]\n)\npreds &lt;- predict(price_cont_gam, newdat,\n                 exclude=\"s(measurement_no,speaker_f)\",\n                 se.fit=T)\nnewdat$f2 &lt;- preds$fit\nnewdat$ll &lt;- preds$fit - 1.96*preds$se.fit\nnewdat$ul &lt;- preds$fit + 1.96*preds$se.fit\n\nThis data set now has everything we need for our plot! We’ll use ggplot() to create the plot. (This won’t render in HTML, but copy the code into your RStudio to see the animation.)\n\nlibrary(gganimate)\nvid &lt;- ggplot(newdat, aes(x=measurement_no,y=f2)) +\n  transition_time(yob) +\n  ease_aes('linear') +\n  labs(title = 'Year of birth: {frame_time}') +\n  geom_ribbon(aes(ymin=ll,ymax=ul), col=NA, fill=\"grey\", alpha=0.2) +\n  geom_line(lwd=2) +\n  xlab(\"% vowel duration\") +\n  ylab(\"F2 (Hz)\") +\n  theme_minimal() + #\n  theme(axis.line=element_line(),\n        panel.grid=element_blank(),\n        axis.text=element_text(size=18, colour=\"black\"),\n        axis.title=element_text(size=18, face=\"bold\"),\n        plot.title=element_text(size=18, face=\"bold\"))\nanimate(vid, nframes=81, fps=12, width=600, height=450)\n\n\n\n\n\nSimpson, Gavin L. 2024. gratia: Graceful ggplot-Based Graphics and Other Functions for GAMs Fitted Using mgcv. https://gavinsimpson.github.io/gratia/.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.\n\n\nSóskuthy, Márton. 2017. “Generalised Additive Mixed Models for Dynamic Analysis in Linguistics: A Practical Introduction.” arXiv Preprint arXiv:1703.05339.\n\n\n———. 2021. “Evaluating Generalised Additive Mixed Modelling Strategies for Dynamic Speech Analysis.” Journal of Phonetics 84: 101017.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized additive mixed-effects models (GAMMs)</span>"
    ]
  },
  {
    "objectID": "index.html#context",
    "href": "index.html#context",
    "title": "Advanced Quantitative Methods for Linguistic Data",
    "section": "Context",
    "text": "Context\nThe last update to these notes was on the “Published” date above.\n\nEach chapter includes:\n\nApplications to linguistic data of concepts from the reading\nPractical illustration of topics from the reading\nExercises  \n\nThe motivation for these notes is a lack of (1) and/or (2) and/or (3) in existing resources. To help linguists develop their quantitative toolbox, this study guide gives:\n\nPractical application to go with excellent existing readings on Bayesian regression models, primarily from McElreath (2020) (Chapters 1-8)\nAn applied introduction to methods which don’t currently have up-to-date published tutorials (Chapters 9, 12, 13, 14)\nExisting tutorial materials in a modern (Quarto) format (Chapters 10-11).\n\nThese materials have been used in:\n\nA graduate course (LING 683, Advanced Quantative Methods) taught in McGill Linguistics in Fall 2024. (Course schedule, including reading list)\nThe course “Bayesian Regression Modeling for Language Data: A Crash Course” at the 2025 LSA Institute (parts of Chapters 1-9)\nTutorials by Márton Sóskuthy (Chapters 10-12)\n\n\n\nThese notes were originally compiled with LING 683 in mind, but they are intended for use as a study guide for language scientists interested in expanding their quantitative toolbox. They can be seen as a follow-up to the material in Sonderegger (2023), but should be usable by readers who have learned similar material from a different source.\nHere is the introduction to the course syllabus, which should give a sense of whether these materials could be helpful for you:\n\n“This is a second course on quantitative methods for analyzing linguistic data. It follows LING 620, where we focused on regression modeling using R, up to linear and logistic mixed-effects models. Using this as a starting point, our goals are to broaden your conceptual knowledge and methodological toolkit of quantitative methods, in order to broaden the research questions you can ask and the types of data you can analyze. This term we will cover (a) Bayesian data analysis and (b) generalized additive (mixed) models, along the way introducing (c) model types beyond linear and logistic (e.g. multinomial, Poisson) and (d) possibly other current methods (e.g. functional data analysis). These methods are increasingly used to analyze linguistic data, but are relatively new to language scientists, and standard tools and best practices for practical applications are evolving. A theme of the course is practical application, and a primary goal is developing a sufficiently strong basis in (a)–(c) that you will be able to figure out the quantitative methods needed to analyze your data in the future.”",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Advanced Quantitative Methods for Linguistic Data",
    "section": "Citation",
    "text": "Citation\nSonderegger, Morgan, Sóskuthy, Márton, Lipari, Massimo, and Doucette, Amanda. (2025) Advanced Quantitative Methods for Linguistic Data. https://people.linguistics.mcgill.ca/~morgan/adv-quant-methods/. 7/2025 version.\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Chapman; Hall/CRC.\n\n\nSonderegger, Morgan. 2023. Regression Modeling for Linguistic Data. MIT Press, https://osf.io/pnumg/, preprint at https://github.com/msonderegger/rmld-v1.1/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "causal.html",
    "href": "causal.html",
    "title": "14  Causal Inference and Causal Discovery",
    "section": "",
    "text": "14.1 Introduction\nThis tutorial is a brief introduction to causal inference with a focus on how to choose “control variables” in a regression analysis, and a very brief introduction to using causal discovery algorithms to identify causal structure in data.\nFor more detailed coverage of causal inference, see Rohrer (2018), Pearl (2009), or Hernan and Robins (2024). Malinsky and Danks (2018) provides a good introduction to causal discovery methods.\nWe’ll be using the following R libraries:\nlibrary(lme4)\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(broom)\nlibrary(arm)\nlibrary(DT)\nlibrary(languageR)\nlibrary(ggdag)\nlibrary(dagitty)\nlibrary(pcalg)\nlibrary(igraph)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Inference and Causal Discovery</span>"
    ]
  },
  {
    "objectID": "causal.html#introduction",
    "href": "causal.html#introduction",
    "title": "14  Causal Inference and Causal Discovery",
    "section": "",
    "text": "14.1.1 Data\nIn this tutorial, we’ll be looking at the english dataset from languageR. To learn more about the dataset, run ?english in R, or see Baayen (2008). To keep things simple, we’ll only use a subset of the variables included in this data:\n\nRTlexdec: Reaction time in a lexical decision task\nWrittenFrequency: Log frequency from the CELEX lexical database\nFamiliarity: Subjective familiarity ratings\nAgeSubject: Factor with levels young and old\nFrequencyInitialDiphoneWord: Log frequency of the initial diphone\n\nLet’s load the dataset and center the predictors:\n\neng &lt;- english %&gt;%\n  mutate(\n    WrittenFrequency_c = rescale(WrittenFrequency),\n    Familiarity_c = rescale(Familiarity),\n    FrequencyInitialDiphoneWord_c = rescale(FrequencyInitialDiphoneWord),\n    AgeSubject_c = as.numeric(AgeSubject) - 1.5\n  ) %&gt;%\n  dplyr::select(all_of(c(\n    \"RTlexdec\", \"AgeSubject_c\", \"AgeSubject\",\n    \"WrittenFrequency_c\", \"Familiarity_c\", \"FrequencyInitialDiphoneWord_c\"\n  )))\n\n\n\n14.1.2 Causal Inference\nAs linguists, we often want to ask causal research questions: How does variable \\(X\\) affect outcome \\(Y\\)? In the english dataset, we may want to ask “How does word frequency affect reaction time in a lexical decision task?” We could fit a simple regression model to estimate the effect of WrittenFrequency on RTlexdec:\n\neng_m1 &lt;- lm(RTlexdec ~ 1 + WrittenFrequency_c, data = eng)\n\ntidy(eng_m1, conf.int = TRUE)\n## # A tibble: 2 × 7\n##   term               estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)           6.55    0.00209    3133.  0            6.55      6.55 \n## 2 WrittenFrequency_c   -0.136   0.00418     -32.6 4.46e-210   -0.145    -0.128\n\nThis model tells us that there is a relationship between frequency and reaction time: Reaction times are shorter for more frequent words. But is this effect the same as the causal effect we’re interested in?\n\n14.1.2.1 What is a causal effect?\nImagine we’re running an experiment to determine how helpful listening to music in a second language is for learning it. We recruit a group of participants taking classes to learn a new language, and divide them into two groups. One, we tell to listen to music in their second language for an hour a day (\\(M_i = 1\\)). The other, we tell to avoid music in their second language, and continue attending classes as normal (\\(M_i = 0\\)). Before the experiment, we record each participants scores on a language test (\\(X_i\\)).\nAfter some time, we administer another test, the outcome of the experiment \\(y_i\\). For the group who listened to music, we call this outcome \\(y_i^1\\), and for the group who did not listen to music, we call the outcome \\(y_i^0\\). For each participant, we can imagine a possible world where they were placed in the other experimental group: For participants in the music-listening group, we can imagine their potential outcome \\(y_i^0\\) had they been placed in the other group. The causal effect for each participant is the difference between these potential outcomes: \\(y_i^1 - y_i^0\\).\nOf course, we can’t observe these potential outcomes. This is the problem we aim to solve with causal inference.\nOur data will look something like this:\n\n\n\n\n\n\n\n\n\n\n\nParticipant \\(i\\)\nPre-test \\(X_i\\)\nGroup \\(M_i\\)\n\\(y_i^0\\)\n\\(y_i^1\\)\nCausal effect \\(y_i^1 - y_i^0\\)\n\n\n\n\n1\n53\n1\n???\n72\n???\n\n\n2\n81\n0\n90\n???\n???\n\n\n3\n67\n1\n???\n58\n???\n\n\n4\n40\n0\n50\n???\n???\n\n\n5\n39\n1\n???\n85\n???\n\n\n6\n77\n0\n78\n???\n???\n\n\n\nThere are several ways of getting around this problem. We’ll cover one here: regression.\nIf we can accurately predict the missing potential outcome for each participant, then we can get a good estimate of the causal effect. In this case, we can be fairly certain that the outcome for each participant was influenced by their prior knowledge of the language. There are many other variables that might predict post-experiment test scores, but for simplicity we’ll assume that prior test scores are the only one that matters. Under this assumption, our outcome variables \\(y_i\\) are directly caused by both prior test scores \\(X_i\\) and experimental group \\(M_i\\). Therefore, we can estimate the causal effect through regression:\n\\[\ny_i = \\beta_0 + \\beta_1 X_i + \\beta_2 M_i\n\\]\nThe parameter \\(\\beta_2\\) represents the causal effect listening to music on second-language test scores.\n\n\n14.1.2.2 What variables do I include in my regression?\n“Controlling for” variables in a regression can allow us to estimate causal effects. But in real data, determining which variables need to be controlled for can be challenging.\nConsider the english dataset from languageR. For simplicity, we’ll only consider a subset of the variables included in this dataset: RTlexdec, AgeSubject, Familiarity, WrittenFrequency and FrequencyInitialDiphoneWord.\n\n\nCode\ndatatable(sample_n(eng, 10))\n\n\n\n\n\n\nWe want to know the effect of WrittenFrequency on RTlexdec. There are several options for which variables to include in a regression:\n\n14.1.2.2.1 1. Only include the variable of interest.\nWe only care about the effect of WrittenFrequency, so let’s exclude everything else from our regression:\n\neng_m2 &lt;- lm(RTlexdec ~ 1 + WrittenFrequency_c, data = eng)\n\ntidy(eng_m2, conf.int = TRUE)\n## # A tibble: 2 × 7\n##   term               estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)           6.55    0.00209    3133.  0            6.55      6.55 \n## 2 WrittenFrequency_c   -0.136   0.00418     -32.6 4.46e-210   -0.145    -0.128\n\nWe can plot the partial effect of WrittenFrequency against the data. It looks reasonable.\n\nplot_model(eng_m2, type = \"pred\", terms = \"WrittenFrequency_c\") +\n  geom_point(aes(x = WrittenFrequency_c, y = RTlexdec), data = eng, size = 0.5)\n\n\n\n\n\n\n\n\n\n\n14.1.2.2.2 2. Include everything.\nOnly including WrittenFrequency in our regression might not be the best choice.\nRTlexdec is correlated with Familiarity:\n\n\nCode\neng %&gt;% ggplot(aes(x = Familiarity_c, y = RTlexdec)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nAnd with FrequencyInitialDiphoneWord:\n\n\nCode\neng %&gt;% ggplot(aes(x = FrequencyInitialDiphoneWord_c, y = RTlexdec)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nAnd also with AgeSubject:\n\n\nCode\neng %&gt;% ggplot(aes(x = AgeSubject, y = RTlexdec)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nAll of these variables could reasonably predict RTlexdec, so we could include them in a regression:\n\neng_m3 &lt;- lm(RTlexdec ~ 1 + WrittenFrequency_c + Familiarity_c + FrequencyInitialDiphoneWord_c + AgeSubject_c, data = eng)\n\ntidy(eng_m3, conf.int = TRUE)\n## # A tibble: 5 × 7\n##   term                  estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)            6.55      0.00124  5285.    0         6.55      6.55   \n## 2 WrittenFrequency_c    -0.0700    0.00406   -17.2   1.59e-64 -0.0780   -0.0620 \n## 3 Familiarity_c         -0.0843    0.00405   -20.8   9.77e-92 -0.0922   -0.0763 \n## 4 FrequencyInitialDiph…  0.00187   0.00249     0.750 4.54e- 1 -0.00302   0.00676\n## 5 AgeSubject_c          -0.222     0.00248   -89.4   0        -0.227    -0.217\n\nIn this model, we see a WrittenFrequency effect of -0.07 (95% CI: [-0.08, -0.06]). Like the previous model eng_m2, this is still negative, but much smaller.\n\n\n14.1.2.2.3 3. Incude the predictors that perform best under some model comparison procedure.\nWe could also fit multiple models, compare them, and choose the “best” model in our comparison. Using AIC, we would choose eng_m3, the model with all variables included.\n\nAIC(eng_m2, eng_m3)\n##        df       AIC\n## eng_m2  3 -4908.994\n## eng_m3  6 -9684.018\n\n\n\n14.1.2.2.4 4. Include only the variables needed to make a causal inference.\nNone of these procedures for choosing a model make any guarantees that the model output will be an estimate of the causal effect. To do this, we need to consider a causal model of the data. If we know that age directly influences reaction times, perhaps it should be included in the model. Written frequency and familiarity essentially represent the same information, so maybe only one should be included. To make this decision, we need to introduce a formal representation of a causal model: the DAG.\n\n\n\n14.1.2.3 Representing causal models with DAGs\nWe can use a Directed Acyclic Graph (DAG) to represent a causal graph. Vertices in the graph represent random variables, and directed edges (arrows connecting the vertices) represent direct causal relationships. Directed means that edges are arrows in one direction, while acyclic means that there are no cycles in the graph: There is no path from a variable back to itself.\nA causal DAG \\(X \\rightarrow Y\\) implies that \\(X\\) directly causes \\(Y\\). In other words, if we could experimentally intervene and change the value of \\(X\\), \\(Y\\) would change as well. However, if we changed the value of \\(Y\\), \\(X\\) would not change.\nAs linguists, we have some background knowledge about the english dataset, and can represent our intuitions about its causal structure as edges in a DAG. These assumptions might be wrong, but we’ll go with them for now:\n\nAgeSubject is a direct cause of RTlexdec: We know that older subjects are generally slightly slower to respond. This adds an arrow AgeSubjext \\(\\rightarrow\\) RTlexdex to our DAG.\nWrittenFrequency is a direct cause of Familiarity: A person’s subjective familiarity with a word must be strongly influenced by its frequency. This adds an arrow WrittenFrequency \\(\\rightarrow\\) Familiarity.\nWrittenFrequency is not a direct cause of RTlexdec: Participants in the experiment don’t know the actual frequencies of words in the corpus these frequencies were estimated with, they only know their own Familiarity with the words. There is no arrow from WrittenFrequency to RTlexdec.\nWrittenFrequency is a direct cause of FrequencyInitialDiphoneWord: The frequency of whole words is likely to have some influence on the frequency of their initial diphones. This adds an arrow WrittenFrequency \\(\\rightarrow\\) FrequencyInitialDiphoneWord.\nFrequencyInitialDiphoneWord is not a direct cause of anything: AgeSubject certainly isn’t caused by initial diphone frequency. Familiarity depends on knowledge of the whole word, not on any diphone frequency – familiar words can have low-frequency initial diphones. WrittenFrequency can’t be caused by diphone frequency, because that would add a cycle to our DAG.\n\nThis gives us the following causal DAG, which we can plot with dagitty and ggdag:\n\n\nCode\ndag1 &lt;- dagitty('dag{\n    \"DiphoneFreq\" [pos=\"4,2\"]\n    \"WrittenFrequency\" [pos=\"3,3\"]\n    \"Familiarity\" [pos=\"3,2\"]\n    \"RTlexdec\" [pos=\"3,1\"]\n    \"AgeSubject\" [pos=\"2,2\"]\n    \"DiphoneFreq\" &lt;- \"WrittenFrequency\"\n    \"WrittenFrequency\" -&gt; \"Familiarity\"\n    \"Familiarity\" -&gt; \"RTlexdec\"\n    \"RTlexdec\" &lt;- \"AgeSubject\"\n    }')\n\nggdag(dag1) +\n  theme_dag() +\n  geom_dag_point(col = \"gray\") +\n  geom_dag_text(col = \"black\")\n\n\n\n\n\n\n\n\n\nCausal DAGs can get quite large and complex, but we can break them down into smaller pieces to understand how variables are related. Three common structures have names that are worth knowing:\n\n14.1.2.3.1 1. Chain: \\(A \\rightarrow B \\rightarrow C\\)\nA chain implies a causal association between \\(A\\) and \\(C\\). This association is mediated by \\(B\\). Any directed path between two variables in a DAG transmits a causal association. Intervening on \\(A\\) causes a change in \\(B\\), which causes a change in \\(C\\). We can generate a dataset to look at this relationship:\n\nchain &lt;- tibble(\n  A = rnorm(1000, mean = 0),\n  B = map_dbl(A, function(i) rnorm(1, mean = i)),\n  C = map_dbl(B, function(i) rnorm(1, mean = i))\n)\n\nIf we “control for \\(B\\)” in a regression on this data, the causal effect of \\(A\\) on \\(C\\) is masked, even though we know there is a causal relationship. Controlling for \\(B\\) effectively blocks the association between \\(A\\) and \\(C\\):\n\nchain_m1 &lt;- lm(C ~ A + B, data = chain)\n\ntidy(chain_m1, conf.int = TRUE)\n## # A tibble: 3 × 7\n##   term        estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept) -0.00840    0.0315    -0.267 7.90e-  1  -0.0702    0.0534\n## 2 A           -0.0133     0.0426    -0.313 7.54e-  1  -0.0970    0.0703\n## 3 B            1.03       0.0306    33.7   5.90e-167   0.971     1.09\n\nIn a chain, we shouldn’t control for the middle variable \\(B\\) if we’re intersted in the causal effect of \\(A\\) on \\(C\\). Removing \\(B\\) from our model, we get a correct estimate of the causal effect:\n\nchain_m2 &lt;- lm(C ~ A, data = chain)\n\ntidy(chain_m2, conf.int = TRUE)\n## # A tibble: 2 × 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)  0.00822    0.0460     0.178 8.58e- 1  -0.0821    0.0986\n## 2 A            0.955      0.0461    20.7   1.32e-79   0.864     1.05\n\n\n\n14.1.2.3.2 2. Fork: \\(A \\leftarrow B \\rightarrow C\\)\nA fork does not imply a causal association between \\(A\\) and \\(C\\). Instead, \\(A\\) and \\(C\\) share a common cause, \\(B\\). Because \\(A\\) and \\(C\\) share a common cause, there is an association between them, but it is not causal. The causal effect of \\(A\\) on \\(C\\) is zero. Here, correlation very much does not imply causation. We can generate a dataset:\n\nfork &lt;- tibble(\n  B = rnorm(1000, mean = 0),\n  A = map_dbl(B, function(i) rnorm(1, mean = i)),\n  C = map_dbl(B, function(i) rnorm(1, mean = i))\n)\n\nAnd we can see that \\(A\\) and \\(C\\) are correlated, although they are not causally related:\n\n\nCode\nfork %&gt;% ggplot(aes(x = A, y = C)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nBut if we fit a model controlling for \\(B\\), we can see that \\(A\\) in fact has no causal effect on \\(C\\). Again, controlling for \\(B\\) blocks the association between \\(A\\) and \\(C\\). Unlike the previous example of a chain, this is desirable – there is no causal relationship between \\(A\\) and \\(C\\), so we want to block the non-causal association induced by \\(B\\):\n\nfork_m1 &lt;- lm(C ~ A + B, data = fork)\n\ntidy(fork_m1, conf.int = TRUE)\n## # A tibble: 3 × 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)  0.0470     0.0326     1.44  1.49e- 1  -0.0169    0.111 \n## 2 A           -0.00669    0.0332    -0.202 8.40e- 1  -0.0718    0.0584\n## 3 B            1.02       0.0469    21.8   2.09e-86   0.929     1.11\n\nIn a fork, the common cause variable is known as a confounder: it can induce spurious correlations and bias measurement of the true causal effect.\n\n\n14.1.2.3.3 3. Collider: \\(A \\rightarrow B \\leftarrow C\\)\nAlso known as an inverted fork, a collider does not transmit an association between \\(A\\) and \\(C\\). Manipulating \\(A\\) has no effect on \\(C\\).\n\ncollider &lt;- tibble(\n  A = rnorm(1000, mean = 0),\n  C = rnorm(1000, mean = 3),\n  B = map2_dbl(A, C, function(i, j) rnorm(1, mean = i + j))\n)\n\nHere, \\(A\\) and \\(C\\) are not correlated:\n\n\nCode\ncollider %&gt;% ggplot(aes(x = A, y = C)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nIf we include \\(B\\) in our model, we incorrectly conclude that \\(A\\) has an effect on \\(C\\):\n\ncollider_m1 &lt;- lm(C ~ A + B, data = collider)\n\ntidy(collider_m1, conf.int = TRUE)\n## # A tibble: 3 × 7\n##   term        estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    1.55     0.0515      30.0 2.17e-141    1.44      1.65 \n## 2 A             -0.470    0.0275     -17.1 1.80e- 57   -0.524    -0.416\n## 3 B              0.483    0.0156      31.0 5.25e-148    0.453     0.514\n\nInstead, \\(B\\) should be excluded from the model:\n\ncollider_m2 &lt;- lm(C ~ A, data = collider)\n\ntidy(collider_m2, conf.int = TRUE)\n## # A tibble: 2 × 7\n##   term        estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   2.98      0.0313    95.4     0       2.92      3.04  \n## 2 A             0.0122    0.0318     0.385   0.701  -0.0502    0.0746\n\n\n\n14.1.2.3.4 Confounding and back-door paths\nTo summarize the above examples in terms of “what to control for”:\n\nIn a chain \\(A \\rightarrow B \\rightarrow C\\), controlling for \\(B\\) blocks the causal association between \\(A\\) and \\(C\\), giving us biased causal effect estimate.\nIn a fork \\(A \\leftarrow B \\rightarrow C\\), controlling for \\(B\\) blocks the non-causal association, giving us an unbiased causal effect estimate.\nIn a collider \\(A \\rightarrow B \\leftarrow C\\), controlling for \\(B\\) unblocks the non-causal association, giving us a biased causal effect estimate.\n\nTo estimate a causal effect between two variables, we want to ensure that all causal paths between them are unblocked, while all non-causal paths are blocked. Because forks transmit non-causal associations, any path \\(X \\leftarrow \\dots \\rightarrow Y\\) needs to be blocked. These paths are also called back-door paths.\nA path between two variables \\(X\\) and \\(Y\\) is blocked by a set of variables \\(\\mathbf{Z}\\) if and only if:\n\nIt contains a fork \\(X \\leftarrow W \\rightarrow Y\\) or a chain \\(X \\rightarrow W \\rightarrow Y\\) where the middle variable \\(W\\) is in the set \\(\\mathbf{Z}\\).\nIt contains a collider \\(X \\rightarrow W \\leftarrow Y\\) where the middle variable \\(W\\) or any descendent of it is not in the set \\(\\mathbf{Z}\\).\n\nThis is also known as d-separation. A set \\(\\mathbf{Z}\\) d-separates \\(X\\) and \\(Y\\) if it blocks every path from \\(X\\) to \\(Y\\).\nA causal effect is considered identifiable when it is possible to block all back-door paths in the DAG (under the assumption that the DAG is correct).\n\n\n\n14.1.2.4 Causal inference in the english dataset\nGoing back to our original example of the english data, we can now identify the causal effect of WrittenFrequency on RTlexdec. It turns out to be fairly straightforward.\nIn our DAG, there is exactly one path between these variables: WrittenFrequency \\(\\rightarrow\\) Familiarity \\(\\rightarrow\\) RTlexdec. This path contains no forks, so it is not a back-door path, and does not need to be blocked to estimate the causal effect.\nThere are no other paths from WrittenFrequency to RTlexdec, so including any other variables in our regression would only bias our estimate of the effect.\nThis means we should use our first model, eng_m2, to estimate the causal effect of WrittenFrequency on RTlexdec, although it performed worse in our model comparison. The best causal model is not always the same as the best predictive model: Including Familiarity as a predictor improved the model’s performance under AIC, an estimator of prediction error, but confounded our causal effect estimate1.\n\n14.1.2.4.1 The causal effect of DiphoneFreq on RTlexdec\nLet’s look at a more complicated example. Suppose we want to estimate the causal effect of DiphoneFreq on RTlexdec. While we can manually figure out the backdoor paths in this DAG, you might be dealing with a much larger, interconnected set of variables in the future. Luckily, the dagitty package provides some convenient functions for figuring out the set of variables to control for in estimating a causal effect. To do this, first we input our DAG:2\n\neng_dag &lt;- dagitty('dag{\n    \"DiphoneFreq\" [pos=\"4,2\"]\n    \"WrittenFrequency\" [pos=\"3,3\"]\n    \"Familiarity\" [pos=\"3,2\"]\n    \"RTlexdec\" [pos=\"3,1\"]\n    \"AgeSubject\" [pos=\"2,2\"]\n    \"DiphoneFreq\" &lt;- \"WrittenFrequency\"\n    \"WrittenFrequency\" -&gt; \"Familiarity\"\n    \"Familiarity\" -&gt; \"RTlexdec\"\n    \"RTlexdec\" &lt;- \"AgeSubject\"\n    }')\n\nIn dagitty (and in much of the causal inference literature), independent variables are referred to as exposure variables, and dependent effects referred to as outcome variables. We can add these to our DAG:\n\nexposures(eng_dag) &lt;- \"DiphoneFreq\"\noutcomes(eng_dag) &lt;- \"RTlexdec\"\n\nWhile we have been referring to the additional variables in our analysis as control variables, dagitty refers to them as adjustment variables. An adjustment set is a minimal set of variables to control for to determine a causal effect. To calculate this set for our DAG:3\n\nadjustmentSets(eng_dag)\n## { Familiarity }\n## { WrittenFrequency }\n\nThis gives us two possible adjustment sets. We can control for either WrittenFrequency or Familiarity to estimate the causal effect of DiphoneFreq on RTlexdec. We can fit both models with lm:\n\neng_m4 &lt;- lm(RTlexdec ~ 1 + WrittenFrequency_c + FrequencyInitialDiphoneWord_c, data = eng)\n\ntidy(eng_m4, conf.int = TRUE)\n## # A tibble: 3 × 7\n##   term                 estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)           6.55      0.00209  3132.    0          6.55      6.55   \n## 2 WrittenFrequency_c   -0.137     0.00421   -32.5   3.00e-208 -0.145    -0.128  \n## 3 FrequencyInitialDip…  0.00141   0.00421     0.335 7.38e-  1 -0.00684   0.00966\n\n\neng_m5 &lt;- lm(RTlexdec ~ 1 + Familiarity_c + FrequencyInitialDiphoneWord_c, data = eng)\n\ntidy(eng_m5, conf.int = TRUE)\n## # A tibble: 3 × 7\n##   term                 estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)           6.55e+0   0.00208  3149.    0          6.55      6.55   \n## 2 Familiarity_c        -1.39e-1   0.00418   -33.4   1.32e-218 -0.148    -0.131  \n## 3 FrequencyInitialDip… -6.87e-4   0.00418    -0.164 8.69e-  1 -0.00888   0.00750\n\nWhether we control for WrittenFrequency or Familiarity, we get the same result: Initial diphone frequency has no effect on reaction time. This is expected, given our DAG: There is only one path from DiphoneFreq to RTlexdec, and it contains a fork, so we block it to estimate the causal effects. There are no paths left, so we get zero causal effect.\n\n\n14.1.2.4.2 The causal effect of AgeSubject on Familiarity\nWe can estimate the causal effect of age on familiarity in the same way:\n\nexposures(eng_dag) &lt;- \"AgeSubject\"\noutcomes(eng_dag) &lt;- \"Familiarity\"\n\nadjustmentSets(eng_dag)\n##  {}\n\nIn this case, the adjustment set is empty, because RTlexdec is a collider and already blocks the non-causal path between AgeSubject and Familiarity. Next we fit a model with no control variables:\n\neng_m6 &lt;- lm(Familiarity_c ~ 1 + AgeSubject_c, data = eng)\n\ntidy(eng_m6, conf.int = TRUE)\n## # A tibble: 2 × 7\n##   term          estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)  -2.06e-16   0.00740 -2.79e-14   1.000  -0.0145    0.0145\n## 2 AgeSubject_c  1.49e-16   0.0148   1.01e-14   1.000  -0.0290    0.0290\n\nAs we would expect, age has no causal effect on familiarty.\n\n\n\n14.1.2.5 Direct effects and mediation\nIn a chain such as WrittenFrequency \\(\\rightarrow\\) Familiarity \\(\\rightarrow\\) RTlexdec, the middle variable is known as a mediator. Before, we were interested in how changing WrittenFrequency impacts RTlexdec. This is also called the total effect of WrittenFrequency.\nBut, knowing that the relationship between WrittenFrequency and RTlexdec is mediated by Familiarity, we may also want to ask a different question: What is the remaining effect of frequency after accounting for familiarity? This is called a direct effect.\ndagitty can also tell us what to control for to estimate a direct effect:\n\nadjustmentSets(\n  eng_dag,\n  exposure = \"WrittenFrequency\",\n  outcome = \"RTlexdec\",\n  effect = \"direct\"\n)\n## { Familiarity }\n\nTo answer this question, we can fit a model controlling for familiarity. However, if we look at our DAG, we can see that controlling for Familiarity blocks the only path between WrittenFrequency and RTlexdec. We should see zero direct causal effect.\nLet’s fit the model:\n\neng_m7 &lt;- lm(RTlexdec ~ 1 + WrittenFrequency_c + Familiarity_c, data = eng)\n\ntidy(eng_m7, conf.int = TRUE)\n## # A tibble: 3 × 7\n##   term               estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)          6.55     0.00206    3186.  0          6.55      6.55  \n## 2 WrittenFrequency_c  -0.0698   0.00673     -10.4 5.84e-25  -0.0830   -0.0566\n## 3 Familiarity_c       -0.0842   0.00673     -12.5 2.10e-35  -0.0974   -0.0710\n\nWrittenFrequency still has a negative effect, even after controlling for Familiarity. This result is inconsistent with our DAG. Why? There are multiple possibilities.\n\n14.1.2.5.1 1. Measurement error\nThis effect, which was not predicted by our DAG, could be a result of measurement error in the dataset. This seems unlikely here, but is a possibility that should be considered in other datasets.\n\n\n14.1.2.5.2 2. An unmeasured confounder\nPerhaps our DAG is missing a variable. It is generally a strong assumption that a dataset includes all causally relevant variables. An unmeasured confounder will lead to an unblocked back-door path.\nFor example, if we add an unmeasured variable to our DAG, it might look like this:\n\n\nCode\neng_dag2 &lt;- dagitty('dag{\n    \"DiphoneFreq\" [pos=\"4,2\"]\n    \"WrittenFrequency\" [pos=\"3,3\"]\n    \"Familiarity\" [pos=\"3,2\"]\n    \"RTlexdec\" [pos=\"3,1\"]\n    \"AgeSubject\" [pos=\"2,2\"]\n    \"U\" [pos=\"2.5,2\"]\n    \"DiphoneFreq\" &lt;- \"WrittenFrequency\"\n    \"WrittenFrequency\" -&gt; \"Familiarity\"\n    \"Familiarity\" -&gt; \"RTlexdec\"\n    \"RTlexdec\" &lt;- \"AgeSubject\"\n    \"U\" -&gt; \"WrittenFrequency\"\n    \"U\" -&gt; \"RTlexdec\"\n    }')\n\nggdag(eng_dag2) +\n  theme_dag() +\n  geom_dag_point(col = \"gray\") +\n  geom_dag_text(col = \"black\")\n\n\n\n\n\n\n\n\n\nWe can get an adjustment set for the direct effect in this new DAG:\n\nadjustmentSets(\n  eng_dag2,\n  exposure = \"WrittenFrequency\",\n  outcome = \"RTlexdec\",\n  effect = \"direct\"\n)\n## { Familiarity, U }\n\nWe need to control for both Familiarity and the unmeasured variable U. We don’t have U in our dataset, so we cannot get an unbiased estimate of the effect if this is the correct DAG.\nUnmeasured confounders in general are a problem for causal inference. All of the methods described here assume a causally sufficient set: A set of variables that includes all common causes of all variables in the set. These methods also assume that the DAG being used for your analysis is accurate. If in reality there are missing arrows or incorrect arrows, your causal inference could be wrong.\nSubject matter knowledge and experimental evidence can help to verify the correctness of a DAG, but in general it is difficult to know if a causal DAG is 100% correct.\nIn some cases, you might not even have an idea of how the variables are causally related. To solve this problem, a different set of methods exist to infer causal structure from data.\n\n\n\n\n14.1.3 Causal Discovery\nCausal discovery or causal search is the problem of identifying causal structure from observational data. We won’t go into detail about the algorithms here, but we can use the implementations in the pcalg package on the english data.\nWe’ll use the PC algorithm4, which is able to identify causal structure up to a Markov equivalence class. A single dataset can correspond to multiple causal DAGs – with the data alone, it is impossible to tell which is correct within an equivalence class. This uncertainty is represented with bidirected edges in the output of the algorithm. A bidirected edge \\(X \\leftrightarrow Y\\) implies that there is no way to distinguish between two possibilities: \\(X \\rightarrow Y\\) and \\(X \\leftarrow Y\\).\nThe PC algorithm depends on conditional independence tests to determine the existence of edges in the DAG. We’ll use pcalg’s gaussCItest, which assumes our data is normally distributed. We also need to provide the pc function with “sufficient statistics” to conduct the conditional independence tests. In this case, that’s a list including a correlation matrix and the number of variables in our data. We also need to provide a significance level alpha and a list of labels for the variables:\n\n## drop the AgeSubject column that was used for plotting earlier\neng_disc &lt;- eng %&gt;% dplyr::select(-c(AgeSubject))\n\npc_dag_out &lt;- pc(\n  suffStat = list(C = cor(eng_disc), n = nrow(eng_disc)),\n  indepTest = gaussCItest,\n  labels = colnames(eng_disc),\n  alpha = 0.01\n)\n\nsummary(pc_dag_out)\n## Object of class 'pcAlgo', from Call:\n## pc(suffStat = list(C = cor(eng_disc), n = nrow(eng_disc)), indepTest = gaussCItest, \n##     alpha = 0.01, labels = colnames(eng_disc))\n## \n## Nmb. edgetests during skeleton estimation:\n## ===========================================\n## Max. order of algorithm:  2 \n## Number of edgetests from m = 0 up to m = 2 :  17 25 6\n## \n## Graphical properties of skeleton:\n## =================================\n## Max. number of neighbours:  3 at node(s) 3 \n## Avg. number of neighbours:  1.4 \n## \n## Adjacency Matrix G:\n##                               RTlexdec AgeSubject_c WrittenFrequency_c\n## RTlexdec                             .            .                  .\n## AgeSubject_c                         1            .                  .\n## WrittenFrequency_c                   1            .                  .\n## Familiarity_c                        1            .                  1\n## FrequencyInitialDiphoneWord_c        .            .                  1\n##                               Familiarity_c FrequencyInitialDiphoneWord_c\n## RTlexdec                                  .                             .\n## AgeSubject_c                              .                             .\n## WrittenFrequency_c                        1                             1\n## Familiarity_c                             .                             .\n## FrequencyInitialDiphoneWord_c             .                             .\n\nThe output includes an adjacency matrix, a representation of the edges in the discovered graph. We can plot the graph using the plot function from the igraph package:\n\n## convert the adjacency matrix into an igraph object\npc_dag &lt;- graph_from_adjacency_matrix(as(pc_dag_out, \"amat\"))\n\nplot(\n  pc_dag,\n  layout = layout_nicely,\n  vertex.label.dist = 3.5,\n  edge.color = \"black\"\n)\n\n\n\n\n\n\n\n\nThe DAG discovered by the PC algorithm looks similar to the one shown above, with an additional edge between WrittenFrequency and RTlexdec. The PC algorithm cannot identify unmeasured confounders, so it is possible that there is an unmeasured variable between these variables. We also see uncertainty in edge direction: There are bidirected edges, and edges implying that RTlexdec causes age, familiarity, and frequency. We were able to find a reasonable skeleton for the DAG, but not edge directions. Other causal discovery algorithms exist, some of which can identify unmeasured confounders.\n\n\n\n\nArif, Suchinta, and M Aaron MacNeil. 2022. “Predictive Models Aren’t for Causal Inference.” Ecology Letters 25 (8): 1741–45. https://doi.org/10.1111/ele.14033.\n\n\nBaayen, R. H. 2008. Analyzing linguistic data: A practical introduction to statistics using R. Cambridge: Cambridge University Press.\n\n\nHernan, M. A., and J. M. Robins. 2024. Causal Inference: What If. CRC Press.\n\n\nMalinsky, Daniel, and David Danks. 2018. “Causal Discovery Algorithms: A Practical Guide.” Philosophy Compass 13 (1): e12470. https://doi.org/10.1111/phc3.12470.\n\n\nPearl, Judea. 2009. “Causal inference in statistics: An overview.” Statistics Surveys 3: 96–146. https://doi.org/10.1214/09-SS057.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Inference and Causal Discovery</span>"
    ]
  },
  {
    "objectID": "causal.html#footnotes",
    "href": "causal.html#footnotes",
    "title": "14  Causal Inference and Causal Discovery",
    "section": "",
    "text": "See Arif and MacNeil (2022) for more discussion of predictive and causal models, written for ecologists.↩︎\nThe [pos=x,y] indicators are completely optional, and are only used to provide a better layout for graphing functions.↩︎\nAlso possible: adjustmentSets(eng_dag, exposure = \"DiphoneFreq\", outcome = \"RTlexdec\")↩︎\nnamed for its creators, Peter Spirtes and Clark Glymour↩︎",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Inference and Causal Discovery</span>"
    ]
  },
  {
    "objectID": "causal.html#causal-inference",
    "href": "causal.html#causal-inference",
    "title": "14  Causal Inference and Causal Discovery",
    "section": "14.3 Causal Inference",
    "text": "14.3 Causal Inference\nAs linguists, we often want to ask causal research questions: How does variable \\(X\\) affect outcome \\(Y\\)? In the english dataset, we may want to ask “How does word frequency affect reaction time in a lexical decision task?” We could fit a simple regression model to estimate the effect of WrittenFrequency on RTlexdec:\n\neng_m1 &lt;- lm(RTlexdec ~ 1 + WrittenFrequency_c, data = eng)\n\ntidy(eng_m1, conf.int = TRUE)\n## # A tibble: 2 × 7\n##   term               estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)           6.55    0.00209    3133.  0            6.55      6.55 \n## 2 WrittenFrequency_c   -0.136   0.00418     -32.6 4.46e-210   -0.145    -0.128\n\nThis model tells us that there is a relationship between frequency and reaction time: Reaction times are shorter for more frequent words. But is this effect the same as the causal effect we’re interested in?\n\n14.3.1 What is a causal effect?\nImagine we’re running an experiment to determine how helpful listening to music in a second language is for learning it. We recruit a group of participants taking classes to learn a new language, and divide them into two groups. One, we tell to listen to music in their second language for an hour a day (\\(M_i = 1\\)). The other, we tell to avoid music in their second language, and continue attending classes as normal (\\(M_i = 0\\)). Before the experiment, we record each participants scores on a language test (\\(X_i\\)).\nAfter some time, we administer another test, the outcome of the experiment \\(y_i\\). For the group who listened to music, we call this outcome \\(y_i^1\\), and for the group who did not listen to music, we call the outcome \\(y_i^0\\). For each participant, we can imagine a possible world where they were placed in the other experimental group: For participants in the music-listening group, we can imagine their potential outcome \\(y_i^0\\) had they been placed in the other group. The causal effect for each participant is the difference between these potential outcomes: \\(y_i^1 - y_i^0\\).\nOf course, we can’t observe these potential outcomes. This is the problem we aim to solve with causal inference.\nOur data will look something like this:\n\n\n\n\n\n\n\n\n\n\n\nParticipant \\(i\\)\nPre-test \\(X_i\\)\nGroup \\(M_i\\)\n\\(y_i^0\\)\n\\(y_i^1\\)\nCausal effect \\(y_i^1 - y_i^0\\)\n\n\n\n\n1\n53\n1\n???\n72\n???\n\n\n2\n81\n0\n90\n???\n???\n\n\n3\n67\n1\n???\n58\n???\n\n\n4\n40\n0\n50\n???\n???\n\n\n5\n39\n1\n???\n85\n???\n\n\n6\n77\n0\n78\n???\n???\n\n\n\nThere are several ways of getting around this problem. We’ll cover one here: regression.\nIf we can accurately predict the missing potential outcome for each participant, then we can get a good estimate of the causal effect. In this case, we can be fairly certain that the outcome for each participant was influenced by their prior knowledge of the language. There are many other variables that might predict post-experiment test scores, but for simplicity we’ll assume that prior test scores are the only one that matters. Under this assumption, our outcome variables \\(y_i\\) are directly caused by both prior test scores \\(X_i\\) and experimental group \\(M_i\\). Therefore, we can estimate the causal effect through regression:\n\\[\ny_i = \\beta_0 + \\beta_1 X_i + \\beta_2 M_i\n\\]\nThe parameter \\(\\beta_2\\) represents the causal effect listening to music on second-language test scores.\n\n\n14.3.2 What variables do I include in my regression?\n“Controlling for” variables in a regression can allow us to estimate causal effects. But in real data, determining which variables need to be controlled for can be challenging.\nConsider the english dataset from languageR. For simplicity, we’ll only consider a subset of the variables included in this dataset: RTlexdec, AgeSubject, Familiarity, WrittenFrequency and FrequencyInitialDiphoneWord.\n\n\nCode\ndatatable(sample_n(eng, 10))\n\n\n\n\n\n\nWe want to know the effect of WrittenFrequency on RTlexdec. There are several options for which variables to include in a regression:\n\n14.3.2.1 1. Only include the variable of interest.\nWe only care about the effect of WrittenFrequency, so let’s exclude everything else from our regression:\n\neng_m2 &lt;- lm(RTlexdec ~ 1 + WrittenFrequency_c, data = eng)\n\ntidy(eng_m2, conf.int = TRUE)\n## # A tibble: 2 × 7\n##   term               estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)           6.55    0.00209    3133.  0            6.55      6.55 \n## 2 WrittenFrequency_c   -0.136   0.00418     -32.6 4.46e-210   -0.145    -0.128\n\nWe can plot the partial effect of WrittenFrequency against the data. It looks reasonable.\n\nplot_model(eng_m2, type = \"pred\", terms = \"WrittenFrequency_c\") +\n  geom_point(aes(x = WrittenFrequency_c, y = RTlexdec), data = eng, size = 0.5)\n\n\n\n\n\n\n\n\n\n\n14.3.2.2 2. Include everything.\nOnly including WrittenFrequency in our regression might not be the best choice.\nRTlexdec is correlated with Familiarity:\n\n\nCode\neng %&gt;% ggplot(aes(x = Familiarity_c, y = RTlexdec)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nAnd with FrequencyInitialDiphoneWord:\n\n\nCode\neng %&gt;% ggplot(aes(x = FrequencyInitialDiphoneWord_c, y = RTlexdec)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nAnd also with AgeSubject:\n\n\nCode\neng %&gt;% ggplot(aes(x = AgeSubject, y = RTlexdec)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nAll of these variables could reasonably predict RTlexdec, so we could include them in a regression:\n\neng_m3 &lt;- lm(RTlexdec ~ 1 + WrittenFrequency_c + Familiarity_c + FrequencyInitialDiphoneWord_c + AgeSubject_c, data = eng)\n\ntidy(eng_m3, conf.int = TRUE)\n## # A tibble: 5 × 7\n##   term                  estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)            6.55      0.00124  5285.    0         6.55      6.55   \n## 2 WrittenFrequency_c    -0.0700    0.00406   -17.2   1.59e-64 -0.0780   -0.0620 \n## 3 Familiarity_c         -0.0843    0.00405   -20.8   9.77e-92 -0.0922   -0.0763 \n## 4 FrequencyInitialDiph…  0.00187   0.00249     0.750 4.54e- 1 -0.00302   0.00676\n## 5 AgeSubject_c          -0.222     0.00248   -89.4   0        -0.227    -0.217\n\nIn this model, we see a WrittenFrequency effect of -0.07 (95% CI: [-0.08, -0.06]). Like the previous model eng_m2, this is still negative, but much smaller.\n\n\n14.3.2.3 3. Incude the predictors that perform best under some model comparison procedure.\nWe could also fit multiple models, compare them, and choose the “best” model in our comparison. Using AIC, we would choose eng_m3, the model with all variables included.\n\nAIC(eng_m2, eng_m3)\n##        df       AIC\n## eng_m2  3 -4908.994\n## eng_m3  6 -9684.018\n\n\n\n14.3.2.4 4. Include only the variables needed to make a causal inference.\nNone of these procedures for choosing a model make any guarantees that the model output will be an estimate of the causal effect. To do this, we need to consider a causal model of the data. If we know that age directly influences reaction times, perhaps it should be included in the model. Written frequency and familiarity essentially represent the same information, so maybe only one should be included. To make this decision, we need to introduce a formal representation of a causal model: the DAG.\n\n\n\n14.3.3 Representing causal models with DAGs\nWe can use a Directed Acyclic Graph (DAG) to represent a causal graph. Vertices in the graph represent random variables, and directed edges (arrows connecting the vertices) represent direct causal relationships. Directed means that edges are arrows in one direction, while acyclic means that there are no cycles in the graph: There is no path from a variable back to itself.\nA causal DAG \\(X \\rightarrow Y\\) implies that \\(X\\) directly causes \\(Y\\). In other words, if we could experimentally intervene and change the value of \\(X\\), \\(Y\\) would change as well. However, if we changed the value of \\(Y\\), \\(X\\) would not change.\nAs linguists, we have some background knowledge about the english dataset, and can represent our intuitions about its causal structure as edges in a DAG. These assumptions might be wrong, but we’ll go with them for now:\n\nAgeSubject is a direct cause of RTlexdec: We know that older subjects are generally slightly slower to respond. This adds an arrow AgeSubjext \\(\\rightarrow\\) RTlexdex to our DAG.\nWrittenFrequency is a direct cause of Familiarity: A person’s subjective familiarity with a word must be strongly influenced by its frequency. This adds an arrow WrittenFrequency \\(\\rightarrow\\) Familiarity.\nWrittenFrequency is not a direct cause of RTlexdec: Participants in the experiment don’t know the actual frequencies of words in the corpus these frequencies were estimated with, they only know their own Familiarity with the words. There is no arrow from WrittenFrequency to RTlexdec.\nWrittenFrequency is a direct cause of FrequencyInitialDiphoneWord: The frequency of whole words is likely to have some influence on the frequency of their initial diphones. This adds an arrow WrittenFrequency \\(\\rightarrow\\) FrequencyInitialDiphoneWord.\nFrequencyInitialDiphoneWord is not a direct cause of anything: AgeSubject certainly isn’t caused by initial diphone frequency. Familiarity depends on knowledge of the whole word, not on any diphone frequency – familiar words can have low-frequency initial diphones. WrittenFrequency can’t be caused by diphone frequency, because that would add a cycle to our DAG.\n\nThis gives us the following causal DAG, which we can plot with dagitty and ggdag:\n\n\nCode\ndag1 &lt;- dagitty('dag{\n    \"DiphoneFreq\" [pos=\"4,2\"]\n    \"WrittenFrequency\" [pos=\"3,3\"]\n    \"Familiarity\" [pos=\"3,2\"]\n    \"RTlexdec\" [pos=\"3,1\"]\n    \"AgeSubject\" [pos=\"2,2\"]\n    \"DiphoneFreq\" &lt;- \"WrittenFrequency\"\n    \"WrittenFrequency\" -&gt; \"Familiarity\"\n    \"Familiarity\" -&gt; \"RTlexdec\"\n    \"RTlexdec\" &lt;- \"AgeSubject\"\n    }')\n\nggdag(dag1) +\n  theme_dag() +\n  geom_dag_point(col = \"gray\") +\n  geom_dag_text(col = \"black\")\n\n\n\n\n\n\n\n\n\nCausal DAGs can get quite large and complex, but we can break them down into smaller pieces to understand how variables are related. Three common structures have names that are worth knowing:\n\n14.3.3.1 1. Chain: \\(A \\rightarrow B \\rightarrow C\\)\nA chain implies a causal association between \\(A\\) and \\(C\\). This association is mediated by \\(B\\). Any directed path between two variables in a DAG transmits a causal association. Intervening on \\(A\\) causes a change in \\(B\\), which causes a change in \\(C\\). We can generate a dataset to look at this relationship:\n\nchain &lt;- tibble(\n  A = rnorm(1000, mean = 0),\n  B = map_dbl(A, function(i) rnorm(1, mean = i)),\n  C = map_dbl(B, function(i) rnorm(1, mean = i))\n)\n\nIf we “control for \\(B\\)” in a regression on this data, the causal effect of \\(A\\) on \\(C\\) is masked, even though we know there is a causal relationship. Controlling for \\(B\\) effectively blocks the association between \\(A\\) and \\(C\\):\n\nchain_m1 &lt;- lm(C ~ A + B, data = chain)\n\ntidy(chain_m1, conf.int = TRUE)\n## # A tibble: 3 × 7\n##   term        estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept) -0.00840    0.0315    -0.267 7.90e-  1  -0.0702    0.0534\n## 2 A           -0.0133     0.0426    -0.313 7.54e-  1  -0.0970    0.0703\n## 3 B            1.03       0.0306    33.7   5.90e-167   0.971     1.09\n\nIn a chain, we shouldn’t control for the middle variable \\(B\\) if we’re intersted in the causal effect of \\(A\\) on \\(C\\). Removing \\(B\\) from our model, we get a correct estimate of the causal effect:\n\nchain_m2 &lt;- lm(C ~ A, data = chain)\n\ntidy(chain_m2, conf.int = TRUE)\n## # A tibble: 2 × 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)  0.00822    0.0460     0.178 8.58e- 1  -0.0821    0.0986\n## 2 A            0.955      0.0461    20.7   1.32e-79   0.864     1.05\n\n\n\n14.3.3.2 2. Fork: \\(A \\leftarrow B \\rightarrow C\\)\nA fork does not imply a causal association between \\(A\\) and \\(C\\). Instead, \\(A\\) and \\(C\\) share a common cause, \\(B\\). Because \\(A\\) and \\(C\\) share a common cause, there is an association between them, but it is not causal. The causal effect of \\(A\\) on \\(C\\) is zero. Here, correlation very much does not imply causation. We can generate a dataset:\n\nfork &lt;- tibble(\n  B = rnorm(1000, mean = 0),\n  A = map_dbl(B, function(i) rnorm(1, mean = i)),\n  C = map_dbl(B, function(i) rnorm(1, mean = i))\n)\n\nAnd we can see that \\(A\\) and \\(C\\) are correlated, although they are not causally related:\n\n\nCode\nfork %&gt;% ggplot(aes(x = A, y = C)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nBut if we fit a model controlling for \\(B\\), we can see that \\(A\\) in fact has no causal effect on \\(C\\). Again, controlling for \\(B\\) blocks the association between \\(A\\) and \\(C\\). Unlike the previous example of a chain, this is desirable – there is no causal relationship between \\(A\\) and \\(C\\), so we want to block the non-causal association induced by \\(B\\):\n\nfork_m1 &lt;- lm(C ~ A + B, data = fork)\n\ntidy(fork_m1, conf.int = TRUE)\n## # A tibble: 3 × 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)  0.0470     0.0326     1.44  1.49e- 1  -0.0169    0.111 \n## 2 A           -0.00669    0.0332    -0.202 8.40e- 1  -0.0718    0.0584\n## 3 B            1.02       0.0469    21.8   2.09e-86   0.929     1.11\n\nIn a fork, the common cause variable is known as a confounder: it can induce spurious correlations and bias measurement of the true causal effect.\n\n\n14.3.3.3 3. Collider: \\(A \\rightarrow B \\leftarrow C\\)\nAlso known as an inverted fork, a collider does not transmit an association between \\(A\\) and \\(C\\). Manipulating \\(A\\) has no effect on \\(C\\).\n\ncollider &lt;- tibble(\n  A = rnorm(1000, mean = 0),\n  C = rnorm(1000, mean = 3),\n  B = map2_dbl(A, C, function(i, j) rnorm(1, mean = i + j))\n)\n\nHere, \\(A\\) and \\(C\\) are not correlated:\n\n\nCode\ncollider %&gt;% ggplot(aes(x = A, y = C)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nIf we include \\(B\\) in our model, we incorrectly conclude that \\(A\\) has an effect on \\(C\\):\n\ncollider_m1 &lt;- lm(C ~ A + B, data = collider)\n\ntidy(collider_m1, conf.int = TRUE)\n## # A tibble: 3 × 7\n##   term        estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    1.55     0.0515      30.0 2.17e-141    1.44      1.65 \n## 2 A             -0.470    0.0275     -17.1 1.80e- 57   -0.524    -0.416\n## 3 B              0.483    0.0156      31.0 5.25e-148    0.453     0.514\n\nInstead, \\(B\\) should be excluded from the model:\n\ncollider_m2 &lt;- lm(C ~ A, data = collider)\n\ntidy(collider_m2, conf.int = TRUE)\n## # A tibble: 2 × 7\n##   term        estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   2.98      0.0313    95.4     0       2.92      3.04  \n## 2 A             0.0122    0.0318     0.385   0.701  -0.0502    0.0746\n\n\n\n14.3.3.4 Confounding and back-door paths\nTo summarize the above examples in terms of “what to control for”:\n\nIn a chain \\(A \\rightarrow B \\rightarrow C\\), controlling for \\(B\\) blocks the causal association between \\(A\\) and \\(C\\), giving us biased causal effect estimate.\nIn a fork \\(A \\leftarrow B \\rightarrow C\\), controlling for \\(B\\) blocks the non-causal association, giving us an unbiased causal effect estimate.\nIn a collider \\(A \\rightarrow B \\leftarrow C\\), controlling for \\(B\\) unblocks the non-causal association, giving us a biased causal effect estimate.\n\nTo estimate a causal effect between two variables, we want to ensure that all causal paths between them are unblocked, while all non-causal paths are blocked. Because forks transmit non-causal associations, any path \\(X \\leftarrow \\dots \\rightarrow Y\\) needs to be blocked. These paths are also called back-door paths.\nA path between two variables \\(X\\) and \\(Y\\) is blocked by a set of variables \\(\\mathbf{Z}\\) if and only if:\n\nIt contains a fork \\(X \\leftarrow W \\rightarrow Y\\) or a chain \\(X \\rightarrow W \\rightarrow Y\\) where the middle variable \\(W\\) is in the set \\(\\mathbf{Z}\\).\nIt contains a collider \\(X \\rightarrow W \\leftarrow Y\\) where the middle variable \\(W\\) or any descendent of it is not in the set \\(\\mathbf{Z}\\).\n\nThis is also known as d-separation. A set \\(\\mathbf{Z}\\) d-separates \\(X\\) and \\(Y\\) if it blocks every path from \\(X\\) to \\(Y\\).\nA causal effect is considered identifiable when it is possible to block all back-door paths in the DAG (under the assumption that the DAG is correct).\n\n\n\n14.3.4 Causal inference in the english dataset\nGoing back to our original example of the english data, we can now identify the causal effect of WrittenFrequency on RTlexdec. It turns out to be fairly straightforward.\nIn our DAG, there is exactly one path between these variables: WrittenFrequency \\(\\rightarrow\\) Familiarity \\(\\rightarrow\\) RTlexdec. This path contains no forks, so it is not a back-door path, and does not need to be blocked to estimate the causal effect.\nThere are no other paths from WrittenFrequency to RTlexdec, so including any other variables in our regression would only bias our estimate of the effect.\nThis means we should use our first model, eng_m2, to estimate the causal effect of WrittenFrequency on RTlexdec, although it performed worse in our model comparison. The best causal model is not always the same as the best predictive model: Including Familiarity as a predictor improved the model’s performance under AIC, an estimator of prediction error, but confounded our causal effect estimate1.\n\n14.3.4.1 The causal effect of DiphoneFreq on RTlexdec\nLet’s look at a more complicated example. Suppose we want to estimate the causal effect of DiphoneFreq on RTlexdec. While we can manually figure out the backdoor paths in this DAG, you might be dealing with a much larger, interconnected set of variables in the future. Luckily, the dagitty package provides some convenient functions for figuring out the set of variables to control for in estimating a causal effect. To do this, first we input our DAG:2\n\neng_dag &lt;- dagitty('dag{\n    \"DiphoneFreq\" [pos=\"4,2\"]\n    \"WrittenFrequency\" [pos=\"3,3\"]\n    \"Familiarity\" [pos=\"3,2\"]\n    \"RTlexdec\" [pos=\"3,1\"]\n    \"AgeSubject\" [pos=\"2,2\"]\n    \"DiphoneFreq\" &lt;- \"WrittenFrequency\"\n    \"WrittenFrequency\" -&gt; \"Familiarity\"\n    \"Familiarity\" -&gt; \"RTlexdec\"\n    \"RTlexdec\" &lt;- \"AgeSubject\"\n    }')\n\nIn dagitty (and in much of the causal inference literature), independent variables are referred to as exposure variables, and dependent effects referred to as outcome variables. We can add these to our DAG:\n\nexposures(eng_dag) &lt;- \"DiphoneFreq\"\noutcomes(eng_dag) &lt;- \"RTlexdec\"\n\nWhile we have been referring to the additional variables in our analysis as control variables, dagitty refers to them as adjustment variables. An adjustment set is a minimal set of variables to control for to determine a causal effect. To calculate this set for our DAG:3\n\nadjustmentSets(eng_dag)\n## { Familiarity }\n## { WrittenFrequency }\n\nThis gives us two possible adjustment sets. We can control for either WrittenFrequency or Familiarity to estimate the causal effect of DiphoneFreq on RTlexdec. We can fit both models with lm:\n\neng_m4 &lt;- lm(RTlexdec ~ 1 + WrittenFrequency_c + FrequencyInitialDiphoneWord_c, data = eng)\n\ntidy(eng_m4, conf.int = TRUE)\n## # A tibble: 3 × 7\n##   term                 estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)           6.55      0.00209  3132.    0          6.55      6.55   \n## 2 WrittenFrequency_c   -0.137     0.00421   -32.5   3.00e-208 -0.145    -0.128  \n## 3 FrequencyInitialDip…  0.00141   0.00421     0.335 7.38e-  1 -0.00684   0.00966\n\n\neng_m5 &lt;- lm(RTlexdec ~ 1 + Familiarity_c + FrequencyInitialDiphoneWord_c, data = eng)\n\ntidy(eng_m5, conf.int = TRUE)\n## # A tibble: 3 × 7\n##   term                 estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)           6.55e+0   0.00208  3149.    0          6.55      6.55   \n## 2 Familiarity_c        -1.39e-1   0.00418   -33.4   1.32e-218 -0.148    -0.131  \n## 3 FrequencyInitialDip… -6.87e-4   0.00418    -0.164 8.69e-  1 -0.00888   0.00750\n\nWhether we control for WrittenFrequency or Familiarity, we get the same result: Initial diphone frequency has no effect on reaction time. This is expected, given our DAG: There is only one path from DiphoneFreq to RTlexdec, and it contains a fork, so we block it to estimate the causal effects. There are no paths left, so we get zero causal effect.\n\n\n14.3.4.2 The causal effect of AgeSubject on Familiarity\nWe can estimate the causal effect of age on familiarity in the same way:\n\nexposures(eng_dag) &lt;- \"AgeSubject\"\noutcomes(eng_dag) &lt;- \"Familiarity\"\n\nadjustmentSets(eng_dag)\n##  {}\n\nIn this case, the adjustment set is empty, because RTlexdec is a collider and already blocks the non-causal path between AgeSubject and Familiarity. Next we fit a model with no control variables:\n\neng_m6 &lt;- lm(Familiarity_c ~ 1 + AgeSubject_c, data = eng)\n\ntidy(eng_m6, conf.int = TRUE)\n## # A tibble: 2 × 7\n##   term          estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)  -2.06e-16   0.00740 -2.79e-14   1.000  -0.0145    0.0145\n## 2 AgeSubject_c  1.49e-16   0.0148   1.01e-14   1.000  -0.0290    0.0290\n\nAs we would expect, age has no causal effect on familiarty.\n\n\n\n14.3.5 Direct effects and mediation\nIn a chain such as WrittenFrequency \\(\\rightarrow\\) Familiarity \\(\\rightarrow\\) RTlexdec, the middle variable is known as a mediator. Before, we were interested in how changing WrittenFrequency impacts RTlexdec. This is also called the total effect of WrittenFrequency.\nBut, knowing that the relationship between WrittenFrequency and RTlexdec is mediated by Familiarity, we may also want to ask a different question: What is the remaining effect of frequency after accounting for familiarity? This is called a direct effect.\ndagitty can also tell us what to control for to estimate a direct effect:\n\nadjustmentSets(\n  eng_dag,\n  exposure = \"WrittenFrequency\",\n  outcome = \"RTlexdec\",\n  effect = \"direct\"\n)\n## { Familiarity }\n\nTo answer this question, we can fit a model controlling for familiarity. However, if we look at our DAG, we can see that controlling for Familiarity blocks the only path between WrittenFrequency and RTlexdec. We should see zero direct causal effect.\nLet’s fit the model:\n\neng_m7 &lt;- lm(RTlexdec ~ 1 + WrittenFrequency_c + Familiarity_c, data = eng)\n\ntidy(eng_m7, conf.int = TRUE)\n## # A tibble: 3 × 7\n##   term               estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)          6.55     0.00206    3186.  0          6.55      6.55  \n## 2 WrittenFrequency_c  -0.0698   0.00673     -10.4 5.84e-25  -0.0830   -0.0566\n## 3 Familiarity_c       -0.0842   0.00673     -12.5 2.10e-35  -0.0974   -0.0710\n\nWrittenFrequency still has a negative effect, even after controlling for Familiarity. This result is inconsistent with our DAG. Why? There are multiple possibilities.\n\n14.3.5.1 1. Measurement error\nThis effect, which was not predicted by our DAG, could be a result of measurement error in the dataset. This seems unlikely here, but is a possibility that should be considered in other datasets.\n\n\n14.3.5.2 2. An unmeasured confounder\nPerhaps our DAG is missing a variable. It is generally a strong assumption that a dataset includes all causally relevant variables. An unmeasured confounder will lead to an unblocked back-door path.\nFor example, if we add an unmeasured variable to our DAG, it might look like this:\n\n\nCode\neng_dag2 &lt;- dagitty('dag{\n    \"DiphoneFreq\" [pos=\"4,2\"]\n    \"WrittenFrequency\" [pos=\"3,3\"]\n    \"Familiarity\" [pos=\"3,2\"]\n    \"RTlexdec\" [pos=\"3,1\"]\n    \"AgeSubject\" [pos=\"2,2\"]\n    \"U\" [pos=\"2.5,2\"]\n    \"DiphoneFreq\" &lt;- \"WrittenFrequency\"\n    \"WrittenFrequency\" -&gt; \"Familiarity\"\n    \"Familiarity\" -&gt; \"RTlexdec\"\n    \"RTlexdec\" &lt;- \"AgeSubject\"\n    \"U\" -&gt; \"WrittenFrequency\"\n    \"U\" -&gt; \"RTlexdec\"\n    }')\n\nggdag(eng_dag2) +\n  theme_dag() +\n  geom_dag_point(col = \"gray\") +\n  geom_dag_text(col = \"black\")\n\n\n\n\n\n\n\n\n\nWe can get an adjustment set for the direct effect in this new DAG:\n\nadjustmentSets(\n  eng_dag2,\n  exposure = \"WrittenFrequency\",\n  outcome = \"RTlexdec\",\n  effect = \"direct\"\n)\n## { Familiarity, U }\n\nWe need to control for both Familiarity and the unmeasured variable U. We don’t have U in our dataset, so we cannot get an unbiased estimate of the effect if this is the correct DAG.\nUnmeasured confounders in general are a problem for causal inference. All of the methods described here assume a causally sufficient set: A set of variables that includes all common causes of all variables in the set. These methods also assume that the DAG being used for your analysis is accurate. If in reality there are missing arrows or incorrect arrows, your causal inference could be wrong.\nSubject matter knowledge and experimental evidence can help to verify the correctness of a DAG, but in general it is difficult to know if a causal DAG is 100% correct.\nIn some cases, you might not even have an idea of how the variables are causally related. To solve this problem, a different set of methods exist to infer causal structure from data.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Inference and Causal Discovery</span>"
    ]
  },
  {
    "objectID": "causal.html#causal-discovery",
    "href": "causal.html#causal-discovery",
    "title": "14  Causal Inference and Causal Discovery",
    "section": "14.4 Causal Discovery",
    "text": "14.4 Causal Discovery\nCausal discovery or causal search is the problem of identifying causal structure from observational data. We won’t go into detail about the algorithms here, but we can use the implementations in the pcalg package on the english data.\nWe’ll use the PC algorithm4, which is able to identify causal structure up to a Markov equivalence class. A single dataset can correspond to multiple causal DAGs – with the data alone, it is impossible to tell which is correct within an equivalence class. This uncertainty is represented with bidirected edges in the output of the algorithm. A bidirected edge \\(X \\leftrightarrow Y\\) implies that there is no way to distinguish between two possibilities: \\(X \\rightarrow Y\\) and \\(X \\leftarrow Y\\).\nThe PC algorithm depends on conditional independence tests to determine the existence of edges in the DAG. We’ll use pcalg’s gaussCItest, which assumes our data is normally distributed. We also need to provide the pc function with “sufficient statistics” to conduct the conditional independence tests. In this case, that’s a list including a correlation matrix and the number of variables in our data. We also need to provide a significance level alpha and a list of labels for the variables:\n\n# drop the AgeSubject column that was used for plotting earlier\neng_disc &lt;- eng %&gt;% dplyr::select(-c(AgeSubject))\n\npc_dag_out &lt;- pc(\n  suffStat = list(C = cor(eng_disc), n = nrow(eng_disc)),\n  indepTest = gaussCItest,\n  labels = colnames(eng_disc),\n  alpha = 0.01\n)\n\nsummary(pc_dag_out)\n## Object of class 'pcAlgo', from Call:\n## pc(suffStat = list(C = cor(eng_disc), n = nrow(eng_disc)), indepTest = gaussCItest, \n##     alpha = 0.01, labels = colnames(eng_disc))\n## \n## Nmb. edgetests during skeleton estimation:\n## ===========================================\n## Max. order of algorithm:  2 \n## Number of edgetests from m = 0 up to m = 2 :  17 25 6\n## \n## Graphical properties of skeleton:\n## =================================\n## Max. number of neighbours:  3 at node(s) 3 \n## Avg. number of neighbours:  1.4 \n## \n## Adjacency Matrix G:\n##                               RTlexdec AgeSubject_c WrittenFrequency_c\n## RTlexdec                             .            .                  .\n## AgeSubject_c                         1            .                  .\n## WrittenFrequency_c                   1            .                  .\n## Familiarity_c                        1            .                  1\n## FrequencyInitialDiphoneWord_c        .            .                  1\n##                               Familiarity_c FrequencyInitialDiphoneWord_c\n## RTlexdec                                  .                             .\n## AgeSubject_c                              .                             .\n## WrittenFrequency_c                        1                             1\n## Familiarity_c                             .                             .\n## FrequencyInitialDiphoneWord_c             .                             .\n\nThe output includes an adjacency matrix, a representation of the edges in the discovered graph. We can plot the graph using the plot function from the igraph package:\n\n# convert the adjacency matrix into an igraph object\npc_dag &lt;- graph_from_adjacency_matrix(as(pc_dag_out, \"amat\"))\n\nplot(\n  pc_dag,\n  layout = layout_nicely,\n  vertex.label.dist = 3.5,\n  edge.color = \"black\"\n)\n\n\n\n\n\n\n\n\nThe DAG discovered by the PC algorithm looks similar to the one shown above, with an additional edge between WrittenFrequency and RTlexdec. The PC algorithm cannot identify unmeasured confounders, so it is possible that there is an unmeasured variable between these variables. We also see uncertainty in edge direction: There are bidirected edges, and edges implying that RTlexdec causes age, familiarity, and frequency. We were able to find a reasonable skeleton for the DAG, but not edge directions. Other causal discovery algorithms exist, some of which can identify unmeasured confounders.\n\n\n\n\nArif, Suchinta, and M Aaron MacNeil. 2022. “Predictive Models Aren’t for Causal Inference.” Ecology Letters 25 (8): 1741–45. https://doi.org/10.1111/ele.14033.\n\n\nBaayen, R. H. 2008. Analyzing linguistic data: A practical introduction to statistics using R. Cambridge: Cambridge University Press.\n\n\nHernan, M. A., and J. M. Robins. 2024. Causal Inference: What If. CRC Press.\n\n\nMalinsky, Daniel, and David Danks. 2018. “Causal Discovery Algorithms: A Practical Guide.” Philosophy Compass 13 (1): e12470. https://doi.org/10.1111/phc3.12470.\n\n\nPearl, Judea. 2009. “Causal inference in statistics: An overview.” Statistics Surveys 3: 96–146. https://doi.org/10.1214/09-SS057.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Inference and Causal Discovery</span>"
    ]
  },
  {
    "objectID": "causal.html#data",
    "href": "causal.html#data",
    "title": "14  Causal Inference and Causal Discovery",
    "section": "14.2 Data",
    "text": "14.2 Data\nIn this tutorial, we’ll be looking at the english dataset from languageR. To learn more about the dataset, run ?english in R, or see Baayen (2008). To keep things simple, we’ll only use a subset of the variables included in this data:\n\nRTlexdec: Reaction time in a lexical decision task\nWrittenFrequency: Log frequency from the CELEX lexical database\nFamiliarity: Subjective familiarity ratings\nAgeSubject: Factor with levels young and old\nFrequencyInitialDiphoneWord: Log frequency of the initial diphone\n\nLet’s load the dataset and center the predictors:\n\neng &lt;- english %&gt;%\n  mutate(\n    WrittenFrequency_c = rescale(WrittenFrequency),\n    Familiarity_c = rescale(Familiarity),\n    FrequencyInitialDiphoneWord_c = rescale(FrequencyInitialDiphoneWord),\n    AgeSubject_c = as.numeric(AgeSubject) - 1.5\n  ) %&gt;%\n  dplyr::select(all_of(c(\n    \"RTlexdec\", \"AgeSubject_c\", \"AgeSubject\",\n    \"WrittenFrequency_c\", \"Familiarity_c\", \"FrequencyInitialDiphoneWord_c\"\n  )))",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Inference and Causal Discovery</span>"
    ]
  }
]